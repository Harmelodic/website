{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Matt Smith / Harmelodic</p> <p>If you're looking to hire me, or communicate professionally, please use LinkedIn.</p> <p>If you're looking for some of my software engineering work, you can view my GitHub.</p>"},{"location":"creations/","title":"Creations","text":""},{"location":"creations/#foofie-the-dog","title":"Foofie The Dog","text":"<p>Description: \"Weird Al\" Yankovic Homage Link: https://www.foofiethedog.com</p>"},{"location":"creations/#minimal-grey-chrome","title":"Minimal Grey Chrome","text":"<p>Description: Chrome Theme Link:  https://chromewebstore.google.com/detail/minimal-grey/eibdijcbgalojjjeheligknccencdpng</p>"},{"location":"creations/#minimal-grey-firefox","title":"Minimal Grey Firefox","text":"<p>Description: Firefox Theme Link: https://addons.mozilla.org/en-GB/firefox/addon/minimal-grey-theme/</p>"},{"location":"creations/#professor-zorgs-guide-to-alien-etiquette","title":"Professor Zorg's Guide to Alien Etiquette","text":"<p>Description: Retro Game Tool Link: https://harmelodic.github.io/zorg</p>"},{"location":"creations/#stackchat","title":"Stackchat","text":"<p>Description: Podcast Link: https://stackchat.github.io/</p>"},{"location":"library/","title":"Library","text":"<p>Previously stored in my browser's bookmarks, here's a collection of (somewhat) useful things on the internet.</p> <p>Most of them are software related.</p> <p>A link is not an endorsement or indication that I recommend the thing.</p>"},{"location":"library/#artifact-repositories","title":"Artifact Repositories","text":"<ul> <li>Artifact Registry - Google Cloud</li> <li>ArtifactHub - Kubernetes Packages</li> <li>Docker Hub</li> <li>GitHub Releases</li> <li>GitLab Releases</li> <li>JFrog Artifactory</li> <li>npm - JavaScript packages</li> <li>Maven Repository</li> <li>Maven \"Central\" Repository</li> <li>Sonatype Nexus OSS</li> </ul>"},{"location":"library/#authentication-authorization-iam","title":"Authentication / Authorization / IAM","text":"<ul> <li>Auth0</li> <li>Crowd - Atlassian</li> <li>GitHub's OpenID Configuration</li> <li>JWT - JSON Web Tokens</li> <li>Keycloak</li> <li>LDAP - Lightweight Directory Access Protocol</li> <li>Workload Identity (GKE)</li> </ul>"},{"location":"library/#business-management","title":"Business &amp; Management","text":"<ul> <li>Tom Peters (Extreme Humanism)</li> <li>DevOps</li> <li>Site Reliability Engineering - Google</li> <li>The One Minute Manager</li> </ul>"},{"location":"library/#browser-extension-development","title":"Browser Extension Development","text":"<ul> <li>Firefox Extension Compatibility Tester</li> </ul>"},{"location":"library/#c-and-c","title":"C and C++","text":"<ul> <li>CMake</li> <li>Conan - Package Manager</li> <li>GNU make</li> <li>GTK Docs</li> <li>learn-cpp.org</li> <li>learncpp.com</li> <li>Qt Framework Reference</li> </ul>"},{"location":"library/#certificate-management","title":"Certificate Management","text":"<ul> <li>cert-manager</li> <li>digicert - CA</li> <li>GlobalSign - CA</li> <li>GoDaddy - SSL Certificates</li> <li>Let's Encrypt - CA (nonprofit)</li> <li>The SSL Store</li> </ul>"},{"location":"library/#certification","title":"Certification","text":"<ul> <li>AWS Certification</li> <li>CNCF - Kubernetes Administrator</li> <li>CNCF - Kubernetes Application Developer</li> <li>Google Cloud Certification</li> <li>Microsoft Azure Certification</li> <li>Oracle - Java</li> </ul>"},{"location":"library/#ci-cd-systems","title":"CI / CD Systems","text":"<ul> <li>Argo CD</li> <li>Bamboo - Atlassian</li> <li>CircleCI</li> <li>GitHub Actions</li> <li>GitLab CI</li> <li>Helm - Kubernetes Deployments</li> <li>Jenkins</li> <li>Jenkins X</li> <li>Kustomize - Kubernetes Deployments</li> <li>TeamCity - Jetbrains</li> <li>Travis CI</li> </ul>"},{"location":"library/#cloud-providers","title":"Cloud Providers","text":"<ul> <li>AWS - Amazon Web Services</li> <li>Cloudflare</li> <li>DigitalOcean</li> <li>GCP - Google Cloud Platform</li> <li>Google API Client Libraries</li> <li>Heroku</li> <li>Microsoft Azure</li> <li>Nextcloud</li> <li>Openstack</li> <li>tsoHost (UK only)</li> </ul>"},{"location":"library/#conferences","title":"Conferences","text":"<ul> <li>AWS re:Invent</li> <li>devopsdays</li> <li>Devoxx</li> <li>GitHub Universe</li> <li>GitLab Events</li> <li>GitOps Days</li> <li>Google I/O</li> <li>JFokus</li> <li>JSConf</li> <li>Linux Foundation Events</li> <li>Spring I/O</li> <li>VoxxedDays</li> <li>WikiCFP - Call For Papers</li> </ul>"},{"location":"library/#dart","title":"Dart","text":"<ul> <li>Dart programming language</li> <li>Flutter - Framework for Dart</li> <li>Flutter Course - App Brewery</li> <li>Flutter LiveCoding</li> </ul>"},{"location":"library/#data-processing","title":"Data Processing","text":"<ul> <li>Apache Camel</li> <li>Apache Kafka</li> <li>Apache NiFi - User Guide</li> <li>Apache Tika - Metadata Extraction</li> <li>Cloudera - Managed Apache Software</li> <li>GCP Dataflow</li> <li>GCP Pub/Sub</li> <li>Spring Integration</li> </ul>"},{"location":"library/#data-science","title":"Data Science","text":"<ul> <li>Data lake</li> <li>Data mesh</li> <li>Data warehouse</li> <li>Jupyter (Notebooks)</li> <li>Looker (Google Cloud)</li> <li>Pandas</li> <li>Python</li> <li>PyTorch</li> <li>R</li> </ul>"},{"location":"library/#databases","title":"Databases","text":"<ul> <li>H2 - in-memory for Java</li> <li>MongoDB</li> <li>MySQL</li> <li>PostgreSQL</li> <li>SQL Server by Microsoft</li> <li>SQLite</li> </ul>"},{"location":"library/#desktop-linux-development","title":"Desktop Linux Development","text":"<ul> <li>AppImage</li> <li>Wayland Window System</li> <li>X Window System</li> </ul>"},{"location":"library/#developer-networks","title":"Developer Networks","text":"<ul> <li>Amazon Developer</li> <li>Android Developers</li> <li>Apple Developer</li> <li>Chrome for Developers</li> <li>Discord Developer Portal</li> <li>Facebook/Meta for Developers</li> <li>Firefox Add-ons Developer Hub</li> <li>Google Developers</li> <li>Google Play Developer Console</li> <li>IBM Developer</li> <li>LinkedIn Developer Solutions</li> <li>Microsoft Developer</li> <li>Microsoft Technical Docs</li> <li>Snap for Developers</li> <li>Spotify for Developers</li> <li>Twitter/X Developer Docs</li> <li>Ubuntu Desktop for Developers</li> </ul>"},{"location":"library/#documentation","title":"Documentation","text":"<ul> <li>AsyncAPI</li> <li>AsyncAPI Studio</li> <li>Confluence - Atlassian</li> <li>Gitbook</li> <li>Kroki - Diagrams from Code</li> <li>Markdown Spec (GitHub Flavored)</li> <li>Markdown Spec (Original)</li> <li>MkDocs</li> <li>Material for MkDocs</li> <li>OpenAPI - Swagger Editor</li> <li>OpenAPI - Swagger UI</li> <li>OpenAPI Specification</li> <li>Read the Docs</li> <li>swimlanes.io - Sequence Diagrams as Code</li> </ul>"},{"location":"library/#email","title":"Email","text":"<ul> <li>Apache James</li> <li>Mailchimp</li> <li>Postfix</li> <li>roundcube</li> <li>Sendgrid</li> </ul>"},{"location":"library/#frontend-web-development","title":"Frontend Web Development","text":"<ul> <li>Angular</li> <li>Babel</li> <li>BrowserStack - Browser Testing</li> <li>Can I Use... - Browser Compatibility</li> <li>Chrome Platform Feature Status</li> <li>Cypress</li> <li>D3 - Data Visualisation</li> <li>Favicon Generator</li> <li>Geeman's A11y guide</li> <li>HTML Entities</li> <li>Jasmine - BDD Testing</li> <li>Loqate - Post Address Verification</li> <li>Meteor</li> <li>Mocha</li> <li>Modern JS Cheatsheet</li> <li>Next.js - Framework</li> <li>Pa11y</li> <li>Polyfill.io</li> <li>React</li> <li>Redux</li> <li>Selenium - Browser Automation</li> <li>SinonJS - Test Mocks &amp; Spies</li> <li>Turf.js - Map Graphing</li> <li>Web APIs - MDN</li> <li>Web Components</li> <li>WebAssembly</li> <li>WebGL API</li> <li>WebRTC API</li> <li>Webpack</li> <li>axe - Accessibility Testing</li> <li>npm</li> <li>web.dev</li> </ul>"},{"location":"library/#frontend-web-styling","title":"Frontend Web Styling","text":"<ul> <li>Atlaskit - Atlassian</li> <li>CSS in JS - Technique Comparison</li> <li>Flexbox - A Complete Guide</li> <li>Flexbox Overview - MDN</li> <li>Flexbox learning - Froggy</li> <li>Material UI</li> <li>Material UI - Icons</li> <li>Using CSS animations</li> <li>cssreference.io - Visual CSS Reference</li> <li>prefers-color-scheme - MDN</li> <li>styled-components</li> </ul>"},{"location":"library/#game-development","title":"Game Development","text":"<ul> <li>AR Core - Google</li> <li>C++ Tutorials</li> <li>Intro to C++ in Unreal Engine</li> <li>Materialize by Bounding Box Software</li> <li>Phaser - 2D Framework for HTML5</li> <li>Stop Killing Games</li> <li>Unity</li> <li>Unreal Engine</li> <li>speedtree - Vegetation Modeling</li> </ul>"},{"location":"library/#go","title":"Go","text":"<ul> <li>Go Styleguide</li> <li>Standard Library - Go Packages</li> </ul>"},{"location":"library/#google-tools","title":"Google Tools","text":"<ul> <li>Google Admin SDK API</li> <li>Google Workspace APIs Explorer</li> <li>OAuth 2.0 - Google Playground</li> <li>Understanding Roles - GCP</li> <li>gcloud - CLI Reference</li> <li>gsutil - CLI Reference</li> </ul>"},{"location":"library/#hardware","title":"Hardware","text":"<ul> <li>IC Graphite Thermal Pad</li> </ul>"},{"location":"library/#java","title":"Java","text":"<ul> <li>Adoptium Temurin JDK</li> <li>Apache POI - Processing Microsoft Office Docs</li> <li>Arquillian - Integration Testing Framework</li> <li>Gradle</li> <li>JUnit</li> <li>Java 20 - Javadoc</li> <li>JavaFX Architecture</li> <li>Learn Java - dev.java</li> <li>Maven</li> <li>Maven Repository - Raw</li> <li>Multithreading in Java</li> <li>OpenJDK JEPs</li> <li>Phil's Data Structure Zoo</li> <li>SDKMAN!</li> <li>Spring - Framework</li> <li>javadoc.io - Free Javadoc Hosting</li> </ul>"},{"location":"library/#learning-software-engineering","title":"Learning Software Engineering","text":"<ul> <li>\"Awesome\" List</li> <li>Codecademy - Learning programming</li> <li>Udacity - General Learning</li> <li>Udemy - General Learning</li> <li>egghead.io - Learning modern web development</li> <li>exercism - Learning programming languages</li> </ul>"},{"location":"library/#learning-languages","title":"Learning Languages","text":"<ul> <li>Duolingo</li> <li>svenska.se</li> <li>synonymer.se</li> </ul>"},{"location":"library/#life","title":"Life","text":"<ul> <li>Ian's Shoelace Site</li> <li>The Man in Seat Sixty-One</li> </ul>"},{"location":"library/#mobile-development","title":"Mobile Development","text":"<ul> <li>Android Dev Guides</li> <li>Firebase - Platform</li> <li>React Native</li> <li>Swift Docs</li> <li>adb - Android Debug Bridge</li> </ul>"},{"location":"library/#networking","title":"Networking","text":"<ul> <li>Consul - Service Discovery</li> <li>Fastly</li> <li>HTTP/1</li> <li>HTTP/2</li> <li>HTTP/3</li> <li>Istio - Service Mesh</li> <li>Kubernetes Ingress-NGINX Controller</li> <li>NGINX Docs</li> <li>SMP - Service Mesh Performance</li> <li>Varnish HTTP Cache</li> </ul>"},{"location":"library/#nodejs","title":"Node.js","text":"<ul> <li>Electron - Desktop Framework</li> <li>Express - Framework</li> <li>The Event Loop - Philip Roberts' JSConf Talk</li> <li>Useful Node.js Modules</li> </ul>"},{"location":"library/#observability","title":"Observability","text":"<ul> <li>Alertmanager - Prometheus</li> <li>Elasticsearch</li> <li>Google Analytics</li> <li>Grafana</li> <li>Jaeger Tracing</li> <li>Kibana</li> <li>Kubeapps Dashboard</li> <li>Logstash</li> <li>Micrometer</li> <li>OpenTelemetry</li> <li>Opsgenie</li> <li>Prometheus</li> <li>Splunk</li> <li>UptimeRobot</li> <li>fluentd</li> </ul>"},{"location":"library/#password-management","title":"Password Management","text":"<ul> <li>1Password</li> <li>Bitwarden</li> <li>Dashlane</li> <li>KeePass</li> <li>LastPass</li> <li>Passkeys - Apple Developer</li> <li>Passkeys - FIDO Alliance</li> <li>Passkeys - Google Developers</li> </ul>"},{"location":"library/#payments","title":"Payments","text":"<ul> <li>PayPal Braintree</li> <li>Square API</li> <li>Stax Payments API</li> <li>String Payments API</li> <li>Trustly Payments API</li> </ul>"},{"location":"library/#performance-testing","title":"Performance Testing","text":"<ul> <li>Artillery</li> <li>JMeter</li> <li>k6</li> <li>Locustj</li> <li>Testkube</li> <li>UL Benchmarks</li> </ul>"},{"location":"library/#platform-engineering","title":"Platform Engineering","text":"<ul> <li>Ansible</li> <li>Docker</li> <li>Kubernetes</li> <li>OpenShift</li> <li>Puppet</li> <li>Terraform</li> <li>kOps</li> <li>Vagrant</li> </ul>"},{"location":"library/#project-management","title":"Project Management","text":"<ul> <li>Asana</li> <li>Jira - Atlassian</li> <li>Trello</li> <li>YouTrack - Jetbrains</li> </ul>"},{"location":"library/#random-useful-dev-tools","title":"Random Useful Dev Tools","text":"<ul> <li>Crontab Guru</li> <li>CyberChef</li> <li>Devhints</li> <li>IETF Author Resources</li> <li>osquery - SQL-like OS querying</li> </ul>"},{"location":"library/#regulations","title":"Regulations","text":"<ul> <li>GDPR</li> <li>PCI Payment Security Standards</li> </ul>"},{"location":"library/#rust","title":"Rust","text":"<ul> <li>Cargo - Reference</li> <li>Rust Playground</li> <li>The Rust Book</li> <li>crates.io</li> <li>gtk - crate</li> <li>imgui - crate</li> <li>qt_core - crate</li> </ul>"},{"location":"library/#seo-offsite-ux","title":"SEO &amp; Offsite UX","text":"<ul> <li>Dark Visitors - Robots.txt Agents</li> <li>Meta Description Tag</li> <li>OGP - Open Graph Protocol</li> <li>Summary Card - Twitter/X</li> </ul>"},{"location":"library/#scheduling","title":"Scheduling","text":"<ul> <li>Doodle</li> <li>YouCanBookMe</li> </ul>"},{"location":"library/#security","title":"Security","text":"<ul> <li>Darktrace</li> <li>GNU Privacy Handbook</li> <li>GPG (GnuPG)</li> <li>GPG Cheat Sheet</li> <li>OWASP Cheat Sheet Series</li> <li>Open Source Insights</li> <li>Phil's PGP Docs</li> <li>eicar Anti-Malware Testfile</li> <li>security.txt</li> </ul>"},{"location":"library/#security-analysis-hacking","title":"Security Analysis (Hacking)","text":"<ul> <li>AbuseIPDB</li> <li>Binary Ninja</li> <li>BuiltWith</li> <li>CPU.fail</li> <li>CVE - Common Vulnerabilities &amp; Exposures</li> <li>Hex Rays IDA - Disassembler</li> <li>Hopper - Disassembler</li> <li>IDA - Interactive Disassembler</li> <li>Kali Linux - Penetration Testing Linux Distro</li> <li>Keysweeper - Wireless Keystroke Logger</li> <li>Play by Play - Internet of Vulnerabilties</li> <li>Protect against Cryptojacking</li> <li>Public GitHub User information (harmelodic)</li> <li>Radare - Reverse Engineering Framework</li> <li>Shodan - Network Fisher</li> <li>Wappalyzer</li> <li>Xen - Security Advisories</li> <li>ZMap - Scanners &amp; Tools</li> <li>factordb - Find Factors of Big Numbers</li> </ul>"},{"location":"library/#serverless","title":"Serverless","text":"<ul> <li>Apache Openwhisk - Serverless Cloud Platform</li> <li>Serverless Framework</li> </ul>"},{"location":"library/#software-blogs","title":"Software Blogs","text":"<ul> <li>Andrew Harmel-Law</li> <li>Contempt Culture - Aurynn Shaw</li> <li>Geoff Does Stuff - Geoff Lawrence</li> <li>GitHub Blog</li> <li>Jetbrains Blog</li> <li>Julia Evans</li> <li>Mozilla Hacks</li> <li>Red Route - Malcolm Young</li> <li>Software disenchantment - Tonsky</li> <li>The Google Chrome Book</li> </ul>"},{"location":"library/#software-design","title":"Software Design","text":"<ul> <li>Design Patterns for Humans</li> <li>Domain-driven Design</li> <li>Eight fallacies of distributed computing</li> <li>Event Sourcing</li> <li>GraphQL in 30 minutes</li> <li>Interface control document</li> <li>Protocol Buffers Docs</li> <li>Seven Layers of a System - OSI model</li> <li>Twelve-Factor App methodology</li> </ul>"},{"location":"library/#star-trek","title":"Star Trek","text":"<ul> <li>Ex Astris Scientia</li> <li>The Star Trek List - Harmelodic</li> <li>The Star Trek List - OG</li> </ul>"},{"location":"library/#static-analysis","title":"Static Analysis","text":"<ul> <li>GitHub Code Security</li> <li>GitLab Security Configuration</li> <li>Renovate by Mend</li> <li>Snyk</li> <li>SonarQube by SonarSource</li> <li>Sourcegraph</li> <li>Synopsys (BlackDuck) Application Security Testing</li> </ul>"},{"location":"library/#testing","title":"Testing","text":"<ul> <li>Integrated Tests are a Scam</li> </ul>"},{"location":"library/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>command-not-found.com</li> <li>scrcpy - Screen Copy (Android)</li> </ul>"},{"location":"library/#ui-design","title":"UI Design","text":"<ul> <li>InVision</li> <li>Laws of UX</li> <li>Lightning Design System - Salesforce</li> <li>Storybook</li> <li>The A11y Project</li> </ul>"},{"location":"library/#version-control","title":"Version Control","text":"<ul> <li>Bitbucket - Atlassian</li> <li>Git</li> <li>GitHub</li> <li>GitLab</li> <li>Mercurial</li> <li>SVN - Apache Subversion</li> </ul>"},{"location":"open-source/","title":"Open Source","text":"<p>My personal projects are found on my GitHub.</p> <p>There are a variety of different projects that have - some of which are highlighted and explained below.</p>"},{"location":"open-source/#init-projects","title":"Init projects","text":"<p>It's helpful to have a starting point to get going. It's also useful to have that starting point be based on industry standards. It's also useful to have those starting points showcase one's software engineering abilities.</p> <p>To solve all that, I made my init projects.</p> <ul> <li><code>init-web-frontend</code>: For guidance on building web frontends.</li> <li><code>init-microservice</code>: For a template on building microservices in Java (my go-to \"backend\" language).</li> <li><code>init-microservice-go</code>: For a template on building microservices in Go.</li> </ul>"},{"location":"open-source/#config-projects","title":"Config projects","text":"<p>When switching between machines (work, home, laptops, etc.) it's helpful to carry over configuration of things between machines. It's also nice to share that with other people, if they're interested in it.</p> <p>My config projects cover this:</p> <ul> <li><code>.code</code>: For configuration for Visual Studio Code.</li> <li><code>dots</code>: For common scripts and POSIX config.</li> <li><code>renovate-config</code>: For a common configuration for keeping other repositories up to date.</li> </ul>"},{"location":"open-source/#personal-platform","title":"Personal platform","text":"<p>Developing platforms involves multiple different components that are (hopefully) well-structured and fit together. I have showcased how I tend to think about and structure platforms in the various repositories that form my \"personal platform\".</p> <p>The platform is built on Google Cloud Platform (GCP). This is because I'm most comfortable with it from my professional experience, but also I like the way that Google Cloud structures projects, IAM and resources (as it is structured in the way that I think of how infrastructure should fit together).</p> <p>This infrastructure / platform is written in Terraform. Declarative language which maps a <code>resource</code> in code to an infrastructure resource created which inherently discourages programmatical &amp; conditional logic (which would otherwise tend to lead to unnecessary complexity and differences between environments). Environments are easy to implement with Terraform workspaces. Terraform \"state\" tracks what infrastructure \"exists\" and what doesn't, and the lack of continuous reconciliation means stateful infrastructure issues (which inevitably happen) can be manually fixed easily (or at least more easily than fighting against reconciliation system).</p> <p>The infrastructure repositories are ordered in a way that allows recreating the platform easily, without requiring re-running old infrastructure, and provides clear structure and scoping of different platform components. As a rule, infrastructure in any one repository should not depend on infrastructure created in a \"later\" (or equivalent) repository - abiding by this rule completely avoids circular dependency issues.</p> <p>The repositories that make up my personal platform are:</p> <ul> <li><code>automation-infrastructure</code>: The first bits of infrastructure that enable creating further infrastructure using some   kind of CD system (GitHub Actions, Atlantis, etc.). This infrastructure repo only has a single \"environment\".</li> <li><code>personal-initial-infrastructure</code>: The base infrastructure of the personal platform. This is limited to infrastructure   that is required for the specific platform, but is shared across all environments (e.g. artifact storage). This   infrastructure repo only has a single \"environment\".</li> <li><code>personal-env-infrastructure</code>: This is the base infrastructure of an environment in the personal platform. Typically,   it contains some kind of compute runtime system, databases, networking, storage, and more. Ideally, only one   environment is needed (production/prod), but sometimes other environments are needed for development, testing,   research &amp; development, or just for playing around in. In simple platforms, this can be contained to a single repo. In   more complex platforms, I recommend splitting this up by \"platform component\" with a single underlying \"base\"   infrastructure repository.</li> <li><code>personal-apps</code>: Once a runtime platform exists, you need to deploy some apps (typically). This repository (or   repositories) are environment-aware and provide deployment configuration for deploying applications the   compute/runtime platform.</li> </ul> <p>Need more platforms? Just add them! I like to prefix repositories with their \"platform\" name. Ultimately platforms \"host\" things, so it's useful to organise your platforms on the purpose of what your hosting: Business services, CI/CD, Data Analysis, Development Tools, Corporate systems.</p>"},{"location":"open-source/#et-cetera","title":"Et Cetera","text":"<p>There's other stuff that I work on or try to build (and sometimes fail at building). Take a look at my GitHub for more of that.</p>"},{"location":"work-history/","title":"Work History","text":"<p>I believe in Pay Transparency, which is why below I have listed the highest salary I achieved for each role.</p> <p>If you believe you are being underpaid for your work, then please raise this with your employer or union.</p> <ul> <li> <p>Software Engineer @ Nordnet   May 2021 - Current   79,500 kr / month</p> </li> <li> <p>Director @ Coding for Immigrants (CFI)   September 2022 - May 2024   Volunteer</p> </li> <li> <p>Engineering Manager @ Klarna   August 2020 - May 2021   68,967 kr / month</p> </li> <li> <p>Software Engineer Lead @ Capgemini   September 2014 - August 2020   \u00a349,425 / year</p> </li> </ul>"},{"location":"work-history/#certifications-recognitions","title":"Certifications / Recognitions","text":"<ul> <li>Google Cloud Certified Professional - Cloud Architect</li> <li>CNCF Contributor</li> <li>GitLab Hero</li> <li>B.Sc. Digital &amp; Technology Solutions (Aston University)</li> </ul>"},{"location":"blog/","title":"Blog","text":"<p>A collection of articles, reviews, guides, and general blog posts - most of them related to software engineering.</p>"},{"location":"blog/bike-tire-sizing/","title":"Bike Tire Sizing","text":"<p>Need to replace a tire? This is how tire sizing works.</p> <p></p> <p>Bike Sizing is defined in 3 different forms:</p> <ul> <li>The ISO 5775 standard.</li> <li>The Fractional / USA / UK / Imperial system.</li> <li>The French system.</li> </ul>"},{"location":"blog/bike-tire-sizing/#iso-5775","title":"ISO 5775","text":"<p>Formatted as two numbers with a dash, e.g. <code>37-622</code> (as pictured above)</p> <ul> <li>The first number the width of the tire in millimetres.</li> <li>The second number is the inner diameter of the tire in millimetres.</li> </ul> <p>I recommend using this standard.</p>"},{"location":"blog/bike-tire-sizing/#french","title":"French","text":"<p>Formatted as a number, an 'x', a number and a letter, e.g. <code>700 x 35C</code> (as pictured above).</p> <ul> <li>The first number is the outer diameter of the tire in millimetres.</li> <li>The second number is the width of the tire in millimetres.</li> <li>The letter signifies the designation for bicycle rims, see the table below.</li> </ul> Letter Designation type rims SS Straight-side TSS Tubeless straight-side C Crochet HB Hooked-bead (none) Tubeless crochet <p>The 'x' is just an 'x'.</p>"},{"location":"blog/bike-tire-sizing/#fractional-usa-uk-imperial","title":"Fractional / USA / UK / Imperial","text":"<p>Formatted as a number, an 'x', a number, an 'x', and a number, e.g. <code>28 x 1 5\u20448 x 1 3\u20448</code> (as pictured above).</p> <ul> <li>The first number is the outer diameter of the tire in inches.</li> <li>The second number is the depth of the tire, in inches (and fractions of an inch)</li> <li>The third number is the width of the tire, in inches (and fractions of an inch)</li> </ul> <p>The 'x' are just 'x's.</p>"},{"location":"blog/bike-tire-sizing/#sources","title":"Sources","text":"<ul> <li>https://en.wikipedia.org/wiki/ISO_5775</li> <li>https://ilovebicycling.com/a-guide-to-bike-tire-sizes/</li> <li>https://www.sheldonbrown.com/tire-sizing.html</li> </ul>"},{"location":"blog/bike-tire-sizing/#why-post-this","title":"Why post this?","text":"<p>I needed to replace a tire and it took me a little while to find the information I needed.</p> <p>Hope this helps someone else!</p> <p>~ Matt Smith / Harmelodic</p>"},{"location":"blog/cast-iron-cookware/","title":"Cast-iron cookware","text":"<p>Cast-iron pots and pans are great for cooking. It's durable, retains heat for a long time, and versatile (pots and pans can be used on grills, in the oven or on a campfire - provided that any handles or other features can withstand the temperatures).</p> <p>Cast-iron cookware is subject to rust and sticking if not cleaned and maintained properly.</p> <p>Store your cookware in a dry place, such as a cupboard or in your oven. Humidity increases the rate of rusting / corrosion.</p>"},{"location":"blog/cast-iron-cookware/#bare-cast-iron","title":"Bare cast iron","text":"<p>Bare cast iron is cast iron that does not have a coating.</p> <p>Note: Iron will likely leach from cast iron cookware into the food you cook. This can be beneficial for people with iron deficiencies, but dangerous for people who are subject to \"iron overload\".</p>"},{"location":"blog/cast-iron-cookware/#seasoning-bare-cast-iron","title":"Seasoning bare cast iron","text":"<p>Bare cast iron needs special \"seasoning\" maintenance in order to build up and maintain a stick-resistant surface that aids cooking, helps prevent rusting and limits interaction with the iron of the cookware. This seasoning also affects how you should clean the cookware.</p> <p>Seasoning means applying fat (oil or animal fat) be coated and cooked into the cookware. A neutral-tasting oil with a high smoke point (like vegetable oil) is recommended to avoid your food acquiring the taste of the oil too much and avoids the oil from burning during seasoning.</p> <p>To season:</p> <ul> <li>Pre-heat an oven to 180\u00b0C.</li> <li>Apply a thin layer of your chosen fat into all surfaces of the cast-iron cookware - including bottom, sides and   handles to also prevent rusting on these parts. Use a clean kitchen cloth or towel to apply the fat.</li> <li>Place into the oven upside down and leave for about 1 hour for the fat to cook into the iron. Placing it upside down   ensures excess fat does not cling to the cookware and helps the fat coating remain even.</li> <li>The cookware should come out with a slightly glossy black finish. Applying a little extra oil after can aid   protection and stick-resistance.</li> <li>Re-season your cookware whenever you notice it lose any of that black finish. This can be after each time you use it,   or after a few times. More often you season it and keep it protected, the longer it will last.</li> </ul>"},{"location":"blog/cast-iron-cookware/#cooking-with-bare-cast-iron","title":"Cooking with bare cast iron","text":"<p>Cooking with acidic foods (citruses, tomatoes, etc.) can damage the seasoning, so either be prepared to re-season your cookware after cooking with acidic foods, or avoid cooking with them with cast-iron cookware.</p> <p>Use wood utensils with cast-iron cookware to protect the cast iron and seasoning from being damaged. Using metal utensils is OK provided you don't scrape on the cast-iron metal.</p>"},{"location":"blog/cast-iron-cookware/#cleaning-bare-cast-iron","title":"Cleaning bare cast iron","text":"<p>To clean bare cast iron after use:</p> <ul> <li>Use small amounts of mild soap in warm water and gently remove all food debris ensure proper cleaning whilst not   removing the seasoned coating.</li> <li>Use a wooden utensil to scrub larger pieces of food debris off.</li> <li>Use a little coarse salt to scrub any pieces of food debris that may still stick on.</li> <li>After, rinse and thoroughly dry with a kitchen towel or in a hot oven (180\u00b0C for 5 to 10 minutes).</li> <li>Do not wash seasoned cast iron in a dishwasher as this will remove the seasoned coating.</li> </ul> <p>To remove rust (and seasoning):</p> <ul> <li>Use coarse salt as an abrasive and a kitchen cloth/towel to gently scrub off rust. Alternatively, gently scrub   using a chain mail scrubber.</li> <li>Rinse thoroughly and dry completely. Consider placing the cast-iron cookware in a hot oven to ensure thorough drying.</li> <li>Remember to season your cookware after.</li> </ul>"},{"location":"blog/cast-iron-cookware/#enameled-cast-iron","title":"Enameled cast iron","text":"<p>Enameled cast iron is cast iron with a \"vitreous\" or \"porcelain\" enamel glaze. This means the manufacturer has taken powdered glass and fused it to the cast iron by firing.</p> <p>Cooking with enameled cast iron requires more \"fat\" to be used during cooking than bare cast iron because enameled cast iron is not as stick-resistant.</p> <p>Enameled cast iron does not require as much maintenance as bare cast iron:</p> <ul> <li>Seasoning is not required.</li> <li>Cleaning is easier, but:<ul> <li>The cookware should be completely cool before cleaning, to prevent cracking the enamel.</li> <li>Cleaning should be done with soft brushes or sponges with mild detergent to prevent damaging the   enamel (metal sponges/brushes should be avoided, as they will scratch and damage the enamel).</li> <li>Stuck-on food can be loosened by soaking in hot, soapy water or by boiling water with a little baking soda.</li> </ul> </li> <li>Parts of cookware that are not enameled (usually on edges) should be dried thoroughly after cleaning to prevent   rusting.</li> </ul> <p>The downsides of enameled cast iron are:</p> <ul> <li>Lessened ability to withstand searing heat.</li> <li>Lessened ability to resist sticking.</li> <li>It tends to be more expensive (since there's an extra process involved in manufacturing the cookware).</li> </ul>"},{"location":"blog/management/are-you-in-governance-answer-this/","title":"Are you in governance/management? Answer this.","text":"<p>Originally published: 12 September 2020</p> <p>When a person speaks out about a problem, do you:</p> <ol> <li>Listen, and then address the problem</li> <li>Listen, but do nothing</li> <li>Don't listen, and do nothing</li> <li>Attempt to silence the person</li> </ol> <p>Hint: Only point #1 is correct.</p> <p>(Addressing usually involves fixing the problem, but this might not always be the best course of action for the organisation / community. In these cases, it is still unacceptable to do nothing. Communication, transparency, empathy and creating alignment are all vital to resolve the matter and prevent resentment or other social issues from developing.)</p>"},{"location":"blog/management/organising-software-teams-2019/","title":"Organising Software Engineering Teams (2019)","text":"<p>Originally published: 27 November 2019</p> <p>Not interested in the Why but the What? - Read The Structure.</p>"},{"location":"blog/management/organising-software-teams-2019/#the-challenge","title":"The Challenge","text":"<p>Our manual, auditable, securable, tangible, form-filling processes have been modernised, been \"virtualised\", been made \" digital\"  - in other words, they've turned into software.</p> <p>Some organisations figure out quite quickly that the world has changed and that instead of being An Organisation that does X, Y &amp; Z, they need to become A Software Engineering Organisation that also does X, Y &amp; Z, so they can pay the bills.</p> <p>Which means that these organisations needs to write software, and to do that they need Sofware Engineers. However, once you have engineers, you introduce a different problem: How to build, grow and manage your Software Engineering teams.</p> <p>This is one of the key aspects to growing a modern organisation and has been one of the core elements behind companies like Netflix, Spotify, LinkedIn, Facebook and Twitter sprinting ahead of the competition and having massive success. Not only have they got a killer idea, but their Software Engineering structure and culture has led them to grow and adapt.</p> <p>Every big organisation that I've ever dealt with has struggled with how to organise and run their Software Engineering departments. Software Engineers are not like normal employees. The world of work has changed from being factory labourers doing repeatable tasks and tired office workers filling in forms. Those tasks have been automated through robotics and software.</p> <p>Software Engineers aren't Code Monkeys that you give a task to and then they go and do it (though a lot of Software Engineers I know ARE Code Monkeys and enjoy doing just that).</p> <p>Software Engineers are diverse. They're creatives, designers, problem-solvers, academics, musicians, artists, foodies, cyclists, petrol-heads, travellers, carers, fashionistas, and more.</p> <p>They're human beings - with ideas to contribute and questions to ask.</p> <p>Creating a structure for such a diverse group of people to work within to keep them focused, productive, supported and happy is really difficult.</p> <p>I've been helping organisations do this for a while now. So here's the structure I would establish, in 2019.<sup>[1]</sup></p>"},{"location":"blog/management/organising-software-teams-2019/#existing-structures","title":"Existing Structures","text":"<p>SAFe, LeSS, DaD, Lean, Agile, Lean-Agile. Wherever you look, there are articles and books on structure that people have devised that invents roles, puts questionable limits on people and makes decisions for you.</p> <p>Don't get me wrong, some of these structures have potential and provide benefits to organisations - Hell, even this post is adding to that cacophony! - but the point of these modern ideas and ways of working is to be flexible and responsive to change. Going 'by-the-book' ain't that.</p>"},{"location":"blog/management/organising-software-teams-2019/#too-many-cooks","title":"Too Many Cooks","text":"<p>Why do we need ALL of the following?:</p> <ul> <li>A Programme Lead</li> <li>Release Train Engineers</li> <li>Solution Train Engineers</li> <li>Engagement Manager(s)</li> <li>A Principal Technical Architect</li> <li>An Architecture Team</li> <li>Project Managers</li> <li>Product Owners</li> <li>SCRUM Masters</li> <li>Business Analysts</li> <li>Dev Team Leads</li> <li>Dev Teams</li> <li>A Platform Team Lead</li> <li>A Platform Team</li> <li>A UAT Test Team Lead</li> <li>A UAT Test Team</li> <li>A Performance Test Team Lead</li> <li>A Performance Test Team</li> <li>A SIT Test Team Lead</li> <li>A SIT Test Team</li> <li>A Penetration Test Team Lead</li> <li>A Penetration Test Team</li> <li>and so on...</li> </ul> <p>I'm exaggerating a bit with this list, but my fundamental point behind this list is: We have too many Roles in organisations to facillitate the job of \"making software\".</p> <p>A lot of these roles can be combined, some can be automated, and some can be completely removed.</p>"},{"location":"blog/management/organising-software-teams-2019/#the-structure","title":"The Structure","text":"<p>Working with an example is best, I'll try to be as abstract as I can. Let's assume you're trying to architect, develop, deploy and maintain a very large software SOLUTION, involving * many SYSTEMS*.</p>"},{"location":"blog/management/organising-software-teams-2019/#system-level-development","title":"System-level Development","text":"<p>A Team manages a single system. With the team made up of:</p> <ul> <li>A Team Lead</li> <li>5-9 Team Members</li> </ul> <p>A Team is responsible for all aspects of their System and their Team, including:</p> <ul> <li>Requirements Gathering &amp; Analysis</li> <li>Architecture for their System</li> <li>UX/UI Design</li> <li>Application Development</li> <li>Platform Engineering</li> <li>Testing</li> <li>Automation &amp; Delivery</li> <li>Support &amp; Maintenance</li> <li>Self-management of the Team</li> <li>Organising fun activities for the Team</li> </ul> <p>I use the term Team Members purposefully here. As the responsibilities of Team are varied, it is naive to expect that all skills and capability is distributed evenly across the Team. Some Team Members will be better at the more BA-esque roles involving Requirements Gathering &amp; Analysis, other's will be experts in Application Development, other's in Platform Engieering and Architecture. The core goal of any Team is to share knowledge and empower each other, so that if one Team Member is off due to sickness or leave, the rest of the Team could collectively fill in for the missing Team Member until their return.</p> <p>This provides:</p> <ul> <li>Team Ownership of their System, thus removing the need for a Product Owner.</li> <li>A DevOps culture/implementation where the Team is responsible for all aspects of Application Development (Dev) and   Platform Engineering, Automation &amp; Delivery (Ops) for their System.</li> </ul> <p>The number of Team Members depends on the size and complexity of the System, but overly complex Systems should be broken down into less complicated or even simple Systems and then developed by multiple Teams. 5-9 Team Members has been a good fit in the past that allowed creative discussions to flow ( avoiding HiPPO and Groupthink) whilst still having enough people in the team to get the work done quickly. If in doubt, use the 2-pizza rule.</p> <p>The Team Lead is the role of an experienced member of the team who has the responsibility of making final decisions, ending arguments, providing an experienced expert opinion and generally enables the team to do their job. They can also be the SCRUM Master if using SCRUM. The Team Lead also functions partly as a Project Manager by engaging the Solution Administration Team (detailed below) to facillitate recruitment into the Team and any other business-related matters regarding their Team. Overall, a Team Lead should not only have a large technical background, covering everything from Development to Architecture, but also have all the soft skills and leadership ability to run a successful Team. I would recommend, to prevent a Team Lead from becoming too content in their role, that 20% of their work-time be spent on researching the best practices and newest innovations, so the Team can stay ahead of the curve.</p> <p>For most cases, when I've been a Team Lead in the past, I've found running the team using SCRUM (i.e. sprints, standups, retrospectives, etc.) has been the most appropriate. However, it can depend on the context and systems like KANBAN can work better; For example, if the team does more ad-hoc consultancy than developing a specific System.</p>"},{"location":"blog/management/organising-software-teams-2019/#solution-level-development","title":"Solution-level Development","text":"<p>The only Solution-level development that needs to be thought about is:</p> <ul> <li>Architecture of the high-level Solution design.</li> <li>How each Team contributes and fits together as part of that high-level Solution design.</li> </ul> <p>This is usually done by: An Architectural Team, and in this Structure: An Architectural Team is made up of all Team Leads from each Team.</p> <p>Having a centralised Architectural Team, away from normal Teams, that make decisions about how the Solution is built and what technologies should be used, is a terrible idea. Not only does it mean technical decisions are made by the people who aren't dealing with the techincalities, but it means that new Architectural ideas being innovated in Teams are not recognised and shared amongst the rest Teams working on the Solution. Put simply, centralised Architectural Teams have nearly always turned into a circle-jerk of supposedly experienced engineers, who haven't touched code in years (or who never have) making grand, \"big picture\" decisions and pushing them down onto Teams.</p> <p>Contrary to that, an Architectural Team made up of all Team Leads from each Team, provides all the benefits of a centralised Architectural Team (i.e. high-level overview and design of the entire Solution) without any of the \" circle-jerk\"-ing cons, due to the fact that Team Leads recognise new innovations from Teams, are highly experienced, and will have to deal with the consequences of bad Solution-level Architectural decisions and so will have an active mandate to ensure the correct decisions are made in the right way.</p>"},{"location":"blog/management/organising-software-teams-2019/#dedicated-testing","title":"Dedicated Testing","text":"<p>ALL testing should be automated. However, there is sometimes a need for a Dedicated Testing Team with a particular use case: Solution-level Penetration Testing.</p> <p>All other use cases, including unit tests, component tests, integration tests, UAT tests and performance tests can have those tests written by a Team for the System they are developing and then automated as part of CI/CD pipelines.</p> <p>The amount of testing required here may also require a few extra Testers dedicated to writing these tests (such as Solution-level Integration, Performance and UAT Testing), however once written they can be developed on and maintained by the Teams, and run via regular automation jobs.</p>"},{"location":"blog/management/organising-software-teams-2019/#inter-team-communication-alignment","title":"Inter-Team Communication &amp; Alignment","text":"<p>Given that Team Leads make up the Architectural Team, this team responsibilities can be expanded to communicating Blockers to one other in regular Solution-level architectural meetings. This way, Blockers can be brought up shortly after they appear (or beforehand if there is a known upcoming Blocker that needs to be prioritised) and the Team that is responsible for resolving that Blocker can re-prioritise their backlog to ensure that the Blocker is resolved promptly. This keeps Teams in alignment and not continually blocked by one another.</p> <p>To ensure knowledge and experience is shared amongst the Teams, regular Demo Days should be organised to provide Teams or specific Team Members the platform to talk about innovations they've developed, problems they've come across and then resolved, new tools they've found useful, and so on. Generally, it's an opportunity to share knowledge and be inspired to make improvements and changes in your own Team. Demo Days also give the Team Leads the opportunity to provide an update around the Solution-level architecture and any changes, updates they have come up with and if anyone attending the Demo Day have any questions, queries, worries or woes about the future of the Solution.</p> <p>Innersource is one of the most game-changing things to implement in any Software Engineering organisation. If you look at internet as it is at the moment, on of the big reasons why so many organisations have been successful is because of the HUGE range of open-source tools that are available to them to use on their own solutions. A strong Innersource community culture effectively replicates the same impact that open-source has had on society, but within your organisation. With more tools and products available to choose from and contribute to, a strong Innersource community has been one of biggest catalysts for evolution in Software Engineering practices and development.</p> <p>Finally, active communication between Teams is also beneficial. Nowadays, this has taken the form of everyone working on the Solution being on an Instant Messaging System (IMS) such as Slack, MS Teams, RocketChat, or Keybase, and then communicating with one another on that IMS as they see fit. One restriction I have found good to implement, is to ensure that Blockers are not reported and dealt with immediately via Team Members talking on an IMS. Minor issues being reported and dealt with are fine, but as a general rule: Anything that takes 30+ minutes or more should be reported via the proper channels to ensure that the Blocker is tracked and dealt with appropriately. All this ensures that Team Members are focused on the work that has been planned and are not constantly fixing problems reported via an IMS.</p>"},{"location":"blog/management/organising-software-teams-2019/#solution-administration","title":"Solution Administration","text":"<p>Depending on the size of the Solution, a certain number of administrative staff need to be in place to manage:</p> <ul> <li>Recruitment for Teams</li> <li>System &amp; Solution estimations (pulled for ticket-tracking software)</li> <li>Structure Coaches</li> </ul> <p>Recruitment is important, of course. Engineers leave organisations for a variety of reasons and it's important that there are people in place to sort out hiring the right people for the right role in the right team.</p> <p>System &amp; Solution estimates have always traditionally been a responsibility of the Project Manager. However, with the rise in ticket-tracking software, like JIRA, and the expansion of tools &amp; plugins that generate estimations and reports from JIRA Projects, has meant that the hardwork that Project Managers used to do around estimations, project trajectories and reporting, can now be generated by the click of a button on JIRA. That being said, the software isn't perfect and upper-management that require these sorts of reports and estimations for financial and general business reasons aren't completed catered for by the reports that the software generates. As such, there is still a need for human element to generate the specific reports that upper-management require and to provide a human interface to upper management if they want to engage further down to the Solution-level.</p> <p>All of this structural, Agile mess needs people who know how this works and know what makes Teams successful. That's the role of the Structure Coaches. These are people who will facillitate the implementation of The Structure as well as engage with Teams to ensure they are reflecting, improving and evolving their Ways of Working - where they aren't the Structure Coach will help the Team make that happen. They also would organise things like Demo Days and engage with the rest of the business to ensure that the external tools and systems that Teams need are made available to the Teams. Structure coaches need to be energetic, knowledgable in how to manage people, have enough technical knowledge to understand the needs of Teams and outspoken enough to challenge people.</p> <p>Enjoy changing the world!</p> <p>Harm x</p> <p><sup>[1]</sup> \"The only constant in life is change\" ~ Heraclitus. Post-2019, there will be new organisational problems to solve and this structure will need to evolve and change.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/","title":"2nd-Gen Google Pixel Buds Review","text":"<p>Originally published: 15 June 2020</p> <p>Before I bought the Pixel Buds, I was using the OnePlus Bullets Wireless - a pair of wireless (though not \" True-Wireless\") earphones.</p> <p>Honestly, I would have been happy to use the Bullets Wireless for the rest of my life. They had great battery life ( around 7-8 hours), fantastic comfort due to the little wings that come with them (of which they come in 3 different sizes to cover different ear sizes) and had great audio quality.</p> <p>Unfortunately, the build quality of the little button control on the wire didn't last forever. The rubber cover that protected the volume &amp; control switches warped after a few months and eventually fell off, resulting in the circuit board underneath being exposed to the outside elements - which in rainy Britain, quickly kills earphones.</p> <p>As a result, I was back using my wired earphones until I found a replacement - a pair of Smile Jamaica In-Ear Headphones from House of Marley, which are a great pair of reasonably-priced, high-quality wired earphones with a nylon-braided cable.</p> <p>But I was still looking for a pair of wireless earphones with the following qualities:</p> <ul> <li>True-Wireless earbuds (I wanted to cut the cable entirely)</li> <li>Good Build Quality</li> <li>Sensible button/touch controls</li> <li>Small size and didn't protrude out of my ears a lot (I'm going to be on meetings a lot, plus if earphones protrude a   lot then I find they fall out more)</li> <li>Wings for comfort and stability</li> <li>Silicone/rubber inserts for comfort, stability and passive audio isolation.</li> <li>No Active Noise cancellation - for no compromises on battery life and not blocking out outside noise (I like to be   aware of the world around me)</li> <li>Decent battery life (5+ hours listening time)</li> <li>A small charging case that can fit in my pocket comfortably</li> <li>Bluetooth-connectivity so I can connect to my Macbook and to my Android phone</li> <li>Minimal audio connectivitiy glitches (cheap true-wireless are renowned for this)</li> <li>Not too bass-heavy</li> <li>Decent reviews from well-reputed Tech reviews (LinusTechTips, Marquess Brownlee, JimsReviewRoom, etc.)</li> <li>Not built by Apple (unless there is no other viable option)</li> </ul> <p>Price was not really a consideration in this purchase, given I'd tested some cheap ass Veatool J29s for \u00a318 off Amazon and they were AWFUL, plus I have a decently-paid job that provides me with enough disposable income that I could afford any of the expensive options - though I wasn't going to go for something ridiculously expensive.</p> <p>As you will find, in the below points, the Pixel Buds cover all my requirements - some better than others.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#review","title":"Review","text":""},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#purchasing-experience","title":"Purchasing experience","text":"<p>A few months back, I'd subscribed for stock notifications for the 2<sup>nd</sup>-Gen Google Pixel Buds. On Monday ( 13th), I received an email from Google telling me they were available and after a healthy audio-related debate with my SO, I decided to buy them yesterday (14th) afternoon with expected delivery for Thursday (16th) but yet today (15th), my Pixel Buds arrived. Delivered by DPD. Pretty quick postal service, especially considering the current COVID-19 situation - props to Google for sorting that out.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#unboxing","title":"Unboxing","text":"<p>Upon ripping off the DPD packaging, the box is rapped in brown, recyclable cardboard (nice) with an easy pull tab to open (nice).</p> <p>The box itself is wrapped in plastic (ew) with a plastic tab with an OSHA hang hole. However, everything else about the box is made of bleached white, high-quality cardboard (okay).</p> <p>Pull off the top and you find the charging case (with Pixel Buds inside) wrapped in a plastic (ew) wrapper, with pull tab &amp; adhesive (ew) to remove.</p> <p>Underneath the cardboard (nice) display holded, you'll find a little cardboard (nice) and plastic (ew) case containing the large &amp; small-sized gel tips (the mediums are already on the Pixel Buds).</p> <p>Underneath this is another piece of cardboard (nice), and underneath this is the rubber-coated (eh) charging cable held in place with a cardboard (nice) and adhesive (ew) frame.</p> <p>At the very bottom of the box is the paper (nice) manual and Terms &amp; Conditions, wrapped in a thin, plastic (ew) tab.</p> <p>The charging case comes fully-charged with the Pixel Buds inside, also fully-charged.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#build-quality","title":"Build Quality","text":"<p>The charging case feels sooooooo good in the hand, with a soft matt, plastic white finish and a comfortable weight. The magnetic lid snaps open and close with a satisfying feeling that I've already started to fidget with and the Pixel Buds are held in place inside with strong magnetics.</p> <p>The Pixel Buds have the same soft matt, plastic finish that feels good in the hand. The gel tips are held firmly in place and the rubber wing-nub is soft, yet still firm. They are IPX4 rated, which will protect against \"Splashing of water\" - so not good to go swimming in, but a bit of sweat or rain won't harm them.</p> <p>The 1m charging-cable is rubber-coated, which I'm not a fan of (I prefer more durable nylon-braided cables), and Google have a reputation with not shielding their cables properly, so we'll have to see whether this holds up any better than previous Google cables I've used in the past.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#comfort","title":"Comfort","text":"<p>Taking the Pixel Buds out, you realise how small and thin they are, compared to competitors that end up having these chunky, protruding buds near-falling out of your ears - I'm looking at you, Samsung Galaxy Buds, and you, Anker Soundcore Libery 2 Pros.</p> <p>In the ear, I'd say the medium gel tips are a little on the small size compared to previous medium-sized gel tips I've had on other earphones - that being said, the large gel-tips fit perfect for my goofy, big ears. I would say that if you have more normal medium-sized ears, you might have to switch between the medium-sized gel-tips or the large-sized gel tips to find which one provides the best comfort option.</p> <p>Once in my ears, they feel really comfy. The wing-nubs ensure that they don't fall out, whilst not providing pressure on the ear that would become uncomfortable during long-periods of wearing them (though only once you've positioned them correctly - if you don't have it quite right, the wing-nub can feel like it's rubbing on your inner-ear a little). The gel-tip inserts comfortably into my ear canal without feeling like they're so far in that there is an air-tight seal in place, or so far out that they don't feel \"in\".</p> <p>I will say that the OnePlus Bullets Wireless are still slightly more comfortable though. The Bullets Wireless came with replacable wings in 3 sizes, so you could get the comfort of the buds just right, whereas the Pixel Buds' permanently fixed wing-nub doesn't provide that same sort of absolutely, perfect comfort.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#speaker-sound-quality","title":"Speaker Sound Quality","text":"<p>Honestly, perfectly fine, if not personally preferable.</p> <p>As I mention above, I don't like bass-heavy speakers of any kind (unless it's a dedicated bass-related speaker, like a subwoofer), and I prefer my headphones/earphones tuned pretty flat, as I prefer to hear music and audio as it was intended by the artist/studio, as opposed to having headphones/earphones that \"colour\" the sound a lot, usually in favour of enriching the bass.</p> <p>The Pixel Buds have an already great sound where I can hear the bass pumping through on tracks like Old 45's, whilst hearing clear trebles on solo instruments like in The Monsters Inc. Theme. I listened to a couple of podcasts during the day too, and the audio sounds good too, with voices and jingles sound clear.</p> <p>I would say that they're on the quiet side, and that you'll find you'll be turning the volume up higher than on other earphones you might have used, but that's not really a problem, given they still sound loud when on full volume.</p> <p>If Google was going for a great all-round sound for the average consumer, then I think they've nailed it.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#microphone-sound-quality","title":"Microphone Sound Quality","text":"<p>I had an initial hiccup on my first microphone audio test (a phone call to my SO), where the audio was apparently very juttery and robotic (probably a glitch).</p> <p>However, after I popped the buds back in their case and removed them again, the microphone audio has not glitched all day and I've had reports from my colleagues and SO that I sound clear on calls and video meetings.</p> <p>If you're interested in a sample audio clip, soundguys.com have a good clip here.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#battery-life","title":"Battery Life","text":"<p>According to Google the battery life on the Pixel Buds is around 5 hours listening-time or 2.5 hours of talk-time, and I'd say that's about right, if not a bit of an understatement given I went the entire afternoon with them in my ears before they died (~5 hours) and I was intermintently changing between music and being in meetings.</p> <p>The charging case will reportedly help you last a total of 24 hours listening-time, or 12 hours talk-time - can't verify as I've only had them for a day.</p> <p>The charging case also provides a quick-charge feature when you can pop dead earbuds in for 10 minutes and you get 2 more hours of listening-time, which I can confirm, as I did this today.</p> <p>Finally, the charging case can be recharged using the USB-C (nice) port on the bottom, or via placing them on a Qi charging pad or phone capable of providing power via Qi (also nice).</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#user-experience","title":"User Experience","text":"<p>Generally, the User Experience of the Pixel Buds are fantastic.</p> <p>Here's a couple of positive shout-outs:</p> <p>The Touch controls on the buds themselves are smooth and intuitive. No clicky/tactile buttons that tend to push the buds further into the ear causing discomfort is #good. So on either bud, once in your ear:</p> <ul> <li>Swipe forwards, towards your face, to put volume up.</li> <li>Swipe back, towards your ear, to put volume down.</li> <li>Tap once to play, pause or accept a call.</li> <li>Tap twice to skip forward a track or reject a call.</li> <li>Tap thrice to skip back a track.</li> <li>Hold down for an assistant update on the time and on current notifications.</li> <li>Say \"Hey, Google\" or \"OK, Google\" to activate the assistant.</li> </ul> <p>The Pixel Buds App is decent Not only does it provide you guides &amp; animations on the touch controls, and general support, but it also gives you features to handle Pixel Bud related settings like notifications, finding your Pixel Buds, enabling \"Adaptive Sound\" and \"In-ear detection\", and handling Firmware updates. You also get access to more in-depth battery information, down to the percentage of each earbud.</p> <p>The little audio bloops that confirm Bluetooth connection and disconnection They are lovely and it's like I have little cute squidgy creatures in my ears that make cute bloopy noises. No weird audio glitches, just bloopy-goodness.</p> <p>and here's a few annoyances/negative shout-outs:</p> <p>The Pairing process is not intuitive When pairing, I'd just want to open the case, take out the buds out and pop them in my ears and then open bluetooth settings and pair them. It doesn't work like that. Instead, you have to leave them in the case, and just open the case to activate the pairing sequence, only THEN can you pair them with a device. HOWEVER, once paired, if your device (be it Macbook or Android, etc.) remembers the Pixel Buds in its Bluetooth settings, then reconnecting sequence is as simple as ensuring the Pixel Buds aren't connected to anything and just...connecting to them from your chosen device.</p> <p>Occasional audio glitches on calls I've found occasionally audio in my call cuts out slightly from one ear and then comes back a fraction of a second later. This has a weird panning/stereo sound effect that isn't unpleasant like some other audio glitches I've had on previous wireless audio devices, but it is a little weird to experience.</p> <p>Talking notifications on by default annoying When I first setup the Pixel Buds, my phone auto setup having all WhatsApp messages not only providing the normal audio notification, but a follow up message from the Google Assistant saying that if I \"hold down on the Pixel Buds then I can have the Google Assistant read out my notifications\", which is helpful the first time, but VERY annoying when it happens on EVERY WhatsApp notification. Thankfully this feature can be switched off in the Pixel Buds app.</p>"},{"location":"blog/reviews/2nd-gen-google-pixel-buds/#conclusion","title":"Conclusion","text":"<p>And that's it!</p> <p>Solid all-round, good, true-wireless earbuds.</p> <p>Further support for Google Pixel Buds can be found here: https://support.google.com/googlepixelbuds</p> <p>Love, Harm x</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/","title":"Fairphone 5 Review - after 2 months","text":"<p>Originally published: 27 June 2024</p> <p>I bought a Fairphone 5 on the 6th of May 2024. That's about 2 months before I wrote this... ish.</p> <p>Note: On July 15th 2024, Fairphone released a new version of Fairphone OS, based on Android 14. Some of the minor issues that I mention in this post have now been fixed.</p> <p>Read more in my post on the update.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#why-a-new-phone","title":"Why a new phone?","text":"<p>I used to have a Google Pixel 5a. In early May, I was scrolling through some photos and the phone just froze for a few seconds, then the screen went black, and it wouldn't turn back on. It's a known issue with some Google Pixel phones, nicknamed \"The Black Screen of Death\".</p> <p>I'd resuscitated my partner's Pixel 5a from the same issue a few months prior by replacing the main board (resulting in a complete data loss - thankfully she had backups for most things).</p> <p>I decided instead of repairing my Pixel 5a for the umpteenth time (I've done a couple battery and screen repairs by this point), it was time for a new phone.</p> <p>I bought a Fairphone 5.</p> <p>Note: Throughout this review I'll make reference to my Pixel 5a in comparison. Keep in mind:</p> <ul> <li>The Google Pixel 5a was released in August 2021.</li> <li>The Fairphone 5 was released in September 2023.</li> <li>The Pixel 5a cost me SEK4995 in   2021 (adjusting for inflation,   this is SEK6075 in 2024 (about \u20ac541))</li> <li>The Fairphone 5 cost me SEK8490 in 2024 (about \u20ac757)</li> </ul>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#purchasing-decision","title":"Purchasing Decision","text":"<p>What did I want from a phone? Here was my thinking:</p> <ul> <li>Be a decent, mid-range Android phone. I don't need fancy features, but I don't want something cheap and will not last.</li> <li>Want a phone that will last a long time.<ul> <li>Decent specs that will not be be dated / slow within 5-10 years (I used GSMArena to do   comparisons to other phones)</li> <li>Gets as many Android updates as I can (I use Android Update Tracker to   discover this)</li> <li>Be reasonably repairable, ideally very repairable - so I can fix it and keep it alive and running if it breaks.</li> </ul> </li> <li>Strong preference for a phone made sustainably, because I want everyone to be paid fairly and the environment to not   be destroyed in the process.</li> <li>Use a USB-C connector, because that's modern connector we use.</li> <li>Fingerprint sensor, ideally on the back, because it's ergonomic to use and I don't like using face-scanning systems.</li> <li>Ideally have a 3.5mm Headphone/Microphone jack, because I like to plug in my phone into speaker systems or   occasionally use wired headphones.</li> <li>Dual SIM support (eSIM + physical SIM, ideally) as I'm a British citizen living in Sweden and have both a Swedish   number (eSIM) and British number (physical SIM) and would like to keep both SIMs in the same phone.</li> </ul> <p>The decision came down to:</p> <ul> <li>A Fairphone 5<ul> <li>Ticks most of my boxes</li> <li>No headphone jack, sadly.</li> <li>Fingerprint sensor is on the power button, rather than on the back, but that's not a major problem, right? (   Foreshadowing)</li> </ul> </li> <li>Samsung A15<ul> <li>Ticks some of the boxes.</li> <li>Fingerprint sensor is on the power button, rather than on the back.</li> <li>Repairability, in comparison to the Fairphone 5, sucks, otherwise it'd be like most other phones I've had where I   need some heat, opening-pics and spudgers.</li> <li>Sustainability of Samsung devices is... mixed. They'll use recycled materials and recycleable packaging but...   they're a fucking huge for-profit corporation that is contributing to the climate crisis, and their sustainability   statements on their website look mostly like green-washing, and I've seen reports of awful working conditions   being discovered at Samsung factories.</li> </ul> </li> <li>Samsung A35<ul> <li>Basically the same as the A15, except slightly more powerful, an underscreen fingerprint sensor (meh) and no   headphone jack.</li> </ul> </li> </ul> <p>I believe the Fairphone was the most expensive of the 3, and honestly... as much as it ticked most of my boxes, it's price was what made me genuinely consider the Samsung devices. I ultimately bit the bullet and coughed up the money for the Fairphone 5, telling myself \"This is what you have to pay to get a phone from a good, sustainable source\".</p> <p>The Google Pixel 8a had not been announced/released at this point.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#findings","title":"Findings","text":"<p>I discovered after purchasing it that Fairphone has an app pre-installed on the phone where you can request for support and find out info about Fairphone news &amp; things... which is mostly useless to me because I could just do all that through their site.</p> <p>HOWEVER, I found in the Fairphone app, that Fairphone offers a free Extended Warranty. Fairphone already offer a 2-year warranty as standard, but they offer an extra 3 years on top of that, zif you register your phone. I did, so I now have an Extended Warranty until 2029. Neat!... Although, applying for it through the Fairphone App didn't actually work, and I had to do through the site - so room for improvement.</p> <p>Anyway, it's a decent Android phone!</p> <p>It does what I need it to do, in a snappy fashion, supports all the apps I need, and most of the functionality that I want, and no major quirks. If you want a phone, and have similar expectations to me for what a phone should be, how it should function, and want to support a supposedly more sustainable phone-manufacturing company: Get a Fairphone 5.</p> <p>I have no major complaints... except for a few minor issues, some of which I've been able to mitigate:</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#minor-issues","title":"Minor issues","text":"<p>\u26a0\ufe0f NOTE BEFORE READING ISSUES:</p> <p>Fairphone ships with Fairphone OS, which is basically vanilla (or \"stock\") Android, with some extra apps pre-installed (Google apps and the My Fairphone app).</p> <p>Many of the issues I detail below are likely issues with vanilla Android. However, it is my opinion that Fairphone could fix these issues by doing any/all of the following: - Contribute to Android and fix these issues directly, since Android is open-source, and then update their Fairphone     OS to use the newer, fixed Android. - Update their Fairphone OS more regularly, since I believe some of these issues have already been fixed in newer     versions of Android. - Fix/Patch the issues as part of their Fairphone OS version of Android.</p> <p>Since Fairphone is doing none of the above (as far as I can tell), I still treat these issues as \"Fairphone issues\", even if the root technical issue may be with vanilla Android.</p> <p>Theoretically, I could resolve some/all of my vanilla Android issues by rooting my phone and installing a custom ROM. I do not want to do this due to security, warranty, stability, and maintenance concerns.</p> <p>I've marked each issue below with a little comment as to where I think the root of the issue lies, in italics.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#60hz-default","title":"60Hz default","text":"<p>I'm pretty sure this is a Fairphone-specific issue.</p> <p>The phone comes with a screen capability of 90Hz, but it's set by default to be 60Hz. I believe the reason for this is to save the battery life, since a lot of people won't notice, but I wanted the snappiness and don't mind the extra bit of battery loss.</p> <p>Mitigated: Go into settings and set screen refresh rate to 90Hz. My battery life has been pretty fine, reaching ~10-20% after a day of heavy use (YouTube, TV-streaming, filming) and ~40-50% after a normal day (Messaging, bit of music, bit of YouTube).</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#nfc-icon-persists","title":"NFC icon persists","text":"<p>I think this is likely a vanilla Android issue (see note above).</p> <p>Amongst the System Icons in the top right of the phone is a big square NFC icon. On the Fairphone 5, there is no option/setting/anything that makes it go away.</p> <p>I know I have NFC, I always have it on (for using Contactless payments), and I don't need to know that with the icon, and the icon takes up a bunch of space and is a noticeably \"chunkier\" icon compared to the others. It'd be much tidier if wasn't there.</p> <p>However, after 2 months of usage, my brain/eyes have begun to notice it a little less. I still want the option though.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#data-transfer-icons-appear-disappear","title":"Data transfer icons appear &amp; disappear","text":"<p>I think this is likely a vanilla Android issue (see note above).</p> <p>Just next to the NFC icon is the Network icons, things like Wi-Fi icons or the mobile signal icons.</p> <p>I like those, but what I don't like is the fact that ANY time there is a data transfer going on (download or upload) there are a couple of little triangles that appear next to those Network icons. Once the transfer is done, they disappear.</p> <p>This is already an eye-catching, distracting irritation, but it is exacerbated by the fact that when these Data-transfer icons appear, the icons next to them (like the big NFC icon) shifts to the side to make room for the Data-transfer icons. Once the Data-transfer icons disappear... they shift back. This is VERY eye-catching and really, really annoying.</p> <p>After 2 months... my brain/eyes hav begun to tune it out a bit, and whilst that sounds OK, I might remind you that the icons exist to keep you aware of things, ideally important things, and if I'm tuning it out then that's also removing THE POINT OF THE ICONS.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#default-launcher-sucks","title":"Default launcher sucks","text":"<p>I think this is likely a vanilla Android issue (see note above).</p> <p>The default launcher for the Fairphone kinda sucks. It lacks customizability options, for example: it forces you to use the Google search bar, and not be able to move it or change it out for a different search engine.</p> <p>This is understandable since the default launcher is Quickstep - the default launcher for vanilla Android.</p> <p>Nova Launcher to the rescue! It's a free launcher, though a paid version is available that is reasonably priced and unlocks a bunch of useful customisability features.</p> <p>After tweaking Nova Launcher every once in a while to get it how I like it, it's solved all issues I had.</p> <p>So... mitigated, by not using the launcher given to me by Fairphone. Not a good look for Fairphone.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#the-speaker-is-not-very-good","title":"The speaker is not very good","text":"<p>This is a Fairphone-specific hardware issue.</p> <p>At least compared to the Google Pixel 5a, the sound from the speaker sounds small and tinny.</p> <p>Thankfully I don't use it often, but my partner has commented several times \"oh no, can we not listen on your phone - your phone sounds poopy\".</p> <p>That's not a good look.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#its-a-bit-chonky-heavy","title":"It's a bit chonky &amp; heavy","text":"<p>This is a Fairphone-specific design issue.</p> <p>The Fairphone 5 is not a sleek, thin phone like the modern iPhones, Samsungs and Googles. It's not a super thick phone, but it's noticeably thick and heavy, and then it ALSO has a camera bump as well as the normal thickness.</p> <p>This makes holding it feel a little uncomfortable at times as the weight distribution feels a little off. It also means that if I'm wearing some light trousers or shorts (as I tend to, in the summer) the weight of the phone is very noticeable when cycling, though not really when walking.</p> <p>When it's not in my pocket though, I definitely notice - though that might just be because I've nearly always had a phone in my pocket for the last 14 years.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#no-35mm-jack-has-already-stung","title":"No 3.5mm jack has already stung","text":"<p>This is a Fairphone-specific design issue.</p> <p>I do have wireless headphones (Sony WH-1000XM4's) this isn't too bad when I'm around the house, but I leave those at home and use wired headphones in the office.</p> <p>Meaning every day when I get up to go get lunch and I want to listen to music, I take my wired headphones and... realise once again that I don't have a headphone jack.</p> <p>I tried getting a USB-C to 3.5mm jack adapter but it was flimsy and broke, and I feel I'd lose it.</p> <p>I could get wireless earbuds, but I've gone through 3 pairs of Google Pixel Buds (my favourite wireless ear buds to date) and they are NOT repairable and... well, I've gone through 3 pairs!</p> <p>Could get some Fairbuds... but they look a bit chonky. They do look a bit more repairable though, and I like the replaceable battery...</p> <p>Hm.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#no-android-14-yet","title":"No Android 14 yet","text":"<p>This is a Fairphone-specific software issue.</p> <p>The Fairphone 5 doesn't have Android 14 yet. I went from Google Pixel 5a with Android 14, to Fairphone 5 with Android 13.</p> <p>It's not a big change, but there's a couple quality of life &amp; privacy features that I would like to get back.</p> <p>I saw somewhere that it Android 14 would be coming \"this summer\" so... any day now! \ud83e\udd1e</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#side-fingerprint-sensor","title":"Side fingerprint sensor","text":"<p>This is a Fairphone-specific design issue.</p> <p>A fingerprint sensor on the side isn't as ergonomic, or maybe it's too ergonomic. Either way, I still would have preferred it on the back.</p> <p>I often find myself with my hands in my pockets, and accidentally touching the fingerprint sensor and either (a) causing the phone to unlock or (b) causing the phone to vibrate a little, as it reacts the touch... meaning I think I've got a message or notification and so take out my phone to find... nothing.</p> <p>Also, on my Google Pixel 5a, I'd sometimes press the power button to wake the phone and see the lock screen for notifications or quick info. If I try doing that with the Fairphone 5, it usually results in unlocking the phone, which I didn't want to do and so have to either relock the phone and try to not unlock it, or swipe to open the notification tray.</p> <p>Bit annoying. Rear fingerprint sensor would have been better.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#camera-exposure-issue","title":"Camera Exposure issue","text":"<p>This is either a Fairphone-specific hardware issue, or a vanilla Android issue (see note above).</p> <p>When I use the camera app, especially as it's getting more into the summer, I've noticed the camera struggles with exposure - as in, if the scene I'm taking a picture of is brightly lit, then the camera just \"blows out\" the phone and doesn't compensate for the amount of light. If you tap on the thing that is blown out, then the phone does auto-focus and adjusts for the light and THEN you can take the picture, but I'd really like the camera app to be a bit smarter than how it is right now.</p> <p>This exposure weirdness is also noticeable in other apps, such as BeReal. When a take BeReal or send a reaction to someone, I've noticed (again) that the photos are often a little \"blown out\". This makes me think it's more of a camera-sensor/hardware issue rather than a software one.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#quick-open-camera-glitch","title":"Quick-open Camera glitch","text":"<p>This is likely a vanilla Android issue (see note above).</p> <p>I've also noticed that when I double-press the power button to open the camera (as I have that turned on in Settings and I like that feature), the camera app often glitches a bit when it opens and I'm nearly always forced, in these cases, to either retry double-pressing the power button, or unlocking and opening the camera app.</p> <p>This sounds like a minor issue, but the point of this feature is to allow for getting your camera out when you need to quickly and when the feature glitches, it means it's not achieving it's purpose.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#double-tap-to-wake","title":"Double Tap to Wake","text":"<p>This is likely a vanilla Android (ssee note above) or hardware-compatibility issue.</p> <p>I've also enabled the \"Double tap phone screen to wake phone\" option in Settings, as I like to do that when my phone is laid down and I want to quickly check the lock screen.</p> <p>Sometimes, it works first time. However it is sadly quite temperamental and it's not clear on whether I'm pressing too quickly, or too slowly, or it just doesn't work properly. This hasn't been something I've seen before with more \" vanilla\" Android experiences, so could be a hardware-compatibility issue where the Fairphone hardware isn't working well with how Android expects it to work to wake the device? I don't really know though, I feel like I'm just guessing on this one.</p> <p>This is a very annoying software bug since the feature is designed to be quick, and it's sluggishness and temperamental nature make the feature basically useless.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#default-notification-sound-is-loud","title":"Default notification sound is LOUD","text":"<p>This is a Fairphone-specific software design issue.</p> <p>Pixel Dust is way to loud and long for a default notification sound.</p> <p>I chose \"Zirconium\" instead, as a quiet, little, low bloop.</p> <p>Please pick a better default, Fairphone.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#it-cost-a-lot-of-money","title":"It cost a lot of money","text":"<p>This is a Fairphone-specific pricing issue.</p> <p>I covered this earlier, but it is a thought that occasionally pops back into my head.</p> <p>SEK8490 is a lot of money, for what I view as a mid-tier phone. Even when comparing against the Google Pixel 5a, a mid-tier phone from 2021, and adjusting for inflation, it is over SEK2000 more expensive (~\u20ac178).</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#pinned-contacts-in-sharing-doesnt-work","title":"Pinned contacts in sharing doesn't work","text":"<p>This is likely a vanilla Android issue (see note above).</p> <p>Sometimes I like to share things with other people, as one does. For example, I complete the Wordle puzzle and like to share it with:</p> <ul> <li>My partner on Signal.</li> <li>My aunt on WhatsApp.</li> <li>My colleagues on Slack.</li> </ul> <p>Since I do this daily, I configured my sharing settings to \"pin\" these 3 places to share to.</p> <p>Every day, when I try to share the Wordle, the \"pinned\" sharing locations do not show up as the first options. Sometimes one of them does, but it's not consistent and so makes the \"pinning\" feature completely useless, and makes sharing much more of a hassle.</p> <p>This might be an Android bug, as I think I've seen it before and I don't remember having it on Android 14... but it's annoying all the same and Fairphone could do something about it.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#i-miss-pixels-good-at-a-glance","title":"I miss Pixel's good \"At A Glance\"","text":"<p>This is less of an issue, and more of a desire for a specific feature.</p> <p>Google make a widget called \"At a Glance\" where it shows info about the weather, your calendar, flight info, travel info, etc. and it's quite useful, and it has some Pixel-only bits that made it SUPER useful.</p> <p>Moving to the Fairphone, I started using the non-Pixel version of the widget and it looks a bit different and doesn't function as well.</p> <p>Looking on the internet, it might actually just be Google's new version of the widget, and they made it shitter (ah, good ol' enshittification), but I miss my old useful At A Glance widget on my Pixel 5a.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#i-miss-pixels-now-playing","title":"I miss Pixel's \"Now Playing\"","text":"<p>This is less of an issue, and more of a desire for a specific feature.</p> <p>Again, another Pixel-only feature! On the Pixel 5a, the phone has a feature where it basically is always listening for music and if it hears music, it will tell you the song and artist that is playing. Basically like Shazam... except always on and most usefully: Visible on the lock screen.</p> <p>Google claimed it happened all locally, and would only record a few seconds and delete it straight after, so... not the privacy nightmare that many think it might be.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#friend-complains-of-a-screen-glitch-when-charging","title":"Friend complains of a screen glitch when charging","text":"<p>This is a Fairphone-specific hardware issue.</p> <p>Not happened to me, but a friend of mine got a Fairphone 5 and they found that when they plugged in their phone to charge, the screen glitched a bit and a line appeared, going up through the center of the screen.</p> <p>I recommended they send it in to Fairphone for a fix/replacement.</p>"},{"location":"blog/reviews/fairphone-5-review-after-2-months/#conclusion","title":"Conclusion","text":"<p>The Fairphone 5 is a decent phone, if a little pricey, that has a few issues - many of them software-related.</p> <p>I hope Fairphone improve the quality of their software - I see it as their major flaw.</p> <p>Would I recommend it?</p> <p>If you're buying an Android phone and paying for a sustainable phone? Yes, absolutely.</p> <p>If you're buying an Android phone and you can't spend the money? Maybe get a Samsung A35, like a few people I know did.</p> <p>~ Harmelodic</p>"},{"location":"blog/reviews/fairphone-5-review-update-android-14/","title":"Fairphone 5 Review Update - Android 14","text":"<p>Originally published: 16 June 2024</p> <p>A short while ago, I made a review of the Fairphone 5, after using it for ~2 months.</p> <p>In this review, I recommend the Fairphone 5, though I had a bunch of minor issues with the Fairphone 5. Many of them were issues that I figured were likely issues with Stock Android (though as I made clear in the previous post, I believe that Fairphone is at fault for me experiencing these issues).</p> <p>On the 15th July, Fairphone released a new build version of Fairphone OS (Fairphone's Android-based operating system): <code>FP5.UT20.B.041</code>.</p> <p>This build is their Android 14 release!</p> <p>So... have my issues been resolved? Any new issues appeared? Let's find out:</p> <p>60Hz default</p> <p>\ud83d\udd27 This wasn't really much of an issue, since I can still use 90Hz provided I set it.</p> <p>NFC icon persists</p> <p>\u2705 It's gone! I can't re-enable it, which seems odd and not good for those who maybe liked it, but I'm not in that crowd.</p> <p>Data transfer icons appear &amp; disappear</p> <p>\u2705 No longer happening! Similarly to the NFC icon, can't seem re-enable them, but I'm not wanting to.</p> <p>Default launcher sucks</p> <p>\ud83d\udd27 I switched to Nova Launcher, which works much better. I quickly tried the default Android launcher (Quickstep) and it still kinda sucks.</p> <p>The speaker is not very good</p> <p>\ud83d\udcf1 It's a hardware issue, not going to be fixed.</p> <p>It's a bit chonky &amp; heavy</p> <p>\ud83d\udcf1 It's a hardware design issue, not going to be fixed.</p> <p>No 3.5mm jack has already stung</p> <p>\ud83d\udcf1 It's a hardware design issue, not going to be fixed.</p> <p>No Android 14 yet</p> <p>\u2705 It is here!</p> <p>Side fingerprint sensor</p> <p>\ud83d\udcf1 It's a hardware design issue, not going to be fixed.</p> <p>Camera Exposure issue</p> <p>\ud83d\udcf1 Doesn't seem to have massively improved, which makes me think it's a hardware issue.</p> <p>Quick-open Camera glitch</p> <p>\u2705 Used it a couple of times since I got the update and it seems to work pretty fine!</p> <p>Double Tap to Wake</p> <p>\ud83e\udd37 So... not really improved, but at least more consistent (or maybe I've found how to make it consistent... not sure).</p> <p>The double tap does work, but works differently under 2 kinda of conditions:</p> <ol> <li>If the phone has been inactive for more than about 2 seconds, then it is about 5 seconds between double tapping and    the screen turning on. 5 seconds might not sound long, but just count to 5 right now: 1. 2. 3. 4. 5 - that was quite    long, ey? Still needs improvement.</li> <li>If the phone has been inactive for only a second or so, then it is less than a second between double tapping and the    screen turning on - i.e. the way it should be.</li> </ol> <p>Default notification sound is LOUD</p> <p>\u2705 The notification sound is now a little quieter! However, the default notification sound \"Pixie Dust\" is still quite a long notification sound, I prefer the quiet, low bloop of \"Zirconium\"</p> <p>It cost a lot of money</p> <p>\ud83d\udcb0 Can't change what I paid for it.</p> <p>Pinned contacts in sharing doesn't work</p> <p>\u2705 Seems to work now!</p> <p>I miss Pixel's good \"At A Glance\"</p> <p>\ud83d\udd27 The normal Google \"At a Glance\" widget for non-Pixel devices is actually holding up pretty good - it's not quite the same, but pretty good.</p> <p>I miss Pixel's \"Now Playing\"</p> <p>\ud83d\udc94 I do still miss this, but only when I remember I don't have it. I could follow a guide to get it, but the guides involve installing things and doing things that I feel are... sketchy.</p> <p>Friend complains of a screen glitch when charging</p> <p>\u26a1\ufe0f It's a glitch that I don't have, and thus not related.</p> <p>Finally, something that I think is... shitty behaviour from Fairphone \ud83d\udeab</p> <p>As part of the Android 14 update, Fairphone note the following:</p> <p>No more Always Show Time and Info: We removed the Always Show Time and Info feature on the Fairphone 5, as it was a major battery drainer, as per our data. With this removed, your Fairphone will now last longer than before on a single charge.</p> <p>Uhhhh... what?! Fairphone removed a feature that was on the phone? I get that the data may show that it's a major battery drainer, but I don't really think that a company should be allowed to remove existing features unless they have security issues or if no one is using those features.</p> <p>For me, this isn't an issue because I don't use that feature, but since I've already seen complaining that the feature has been removed, it's clearly a feature that people desire, despite the battery effects.</p> <p>I strongly urge Fairphone to re-enable the feature, and have it disabled by default, with a pop-up warning about the battery effects for those enabling it.</p> <p>So:</p> <ul> <li>6 issues solved \u2705</li> <li>3 issues mitigated myself \ud83d\udd27</li> <li>1 software issue outstanding \ud83e\udd37</li> <li>5 hardware/design issues \ud83d\udcf1</li> <li>1 desire for a feature \ud83d\udc94</li> <li>1 light wallet \ud83d\udcb0</li> <li>1 glitch-affected friend \u26a1\ufe0f</li> <li>1 feature removed \ud83d\udeab</li> </ul> <p>...progress?</p> <p>~ Harmelodic</p>"},{"location":"blog/reviews/fairphone-os-january-2025-bugs-and-fixes/","title":"Fairphone OS - January 2025 - Bugs and Fixes","text":"<p>Fairphone released a new version of Fairphone OS on January 27th 2025:</p> <ul> <li>Fairphone 5: https://support.fairphone.com/hc/en-us/articles/18682800465169-Fairphone-5-OS-Release-Notes</li> <li>Fairphone 4: https://support.fairphone.com/hc/en-us/articles/4405858220945-Fairphone-4-OS-Release-Notes</li> </ul> <p>The new version is: <code>B.098</code></p> <p>On my phone (a Fairphone 5), the build number is called: <code>FP5.UT2K.B.098.20250109</code></p> <p>I've seen two weird behaviours so far with it.</p>"},{"location":"blog/reviews/fairphone-os-january-2025-bugs-and-fixes/#nfc-disabled","title":"NFC Disabled","text":"<p>Weird that it was turned off.</p> <p>Easy fix: Re-enable NFC!</p>"},{"location":"blog/reviews/fairphone-os-january-2025-bugs-and-fixes/#device-unlock-with-fingerprint-didnt-work","title":"\"Device unlock\" with fingerprint didn't work","text":"<p>Very weird one.</p> <p>By going to <code>Settings &gt; Security and privacy &gt; Device unlock &gt; Face and fingerprint unlock</code>, I could see that <code>Unlock your phone</code> was enabled and that I had fingerprint(s) registered.</p> <p>I could also see that the fingerprint sensor still worked, as I could use it within apps where I could log in to the apps using a fingerprint.</p> <p>Things I tried that didn't work:</p> <ul> <li>Rebooting the phone</li> <li>Disabling Face/Fingerprint <code>Unlock your phone</code>, rebooting and then re-enabling.</li> <li>Removing the registered fingerprints and re-registering them.</li> </ul> <p>What did work was: Go to <code>Settings &gt; Security and privacy &gt; Device finders &gt; Find My Device</code> and disabled <code>Use Find My Device</code>. Thankfully, I found that re-enabling <code>Use Find My Device</code> afterwards did not re-break fingerprint unlock.</p> <p>Source of this finding was this reddit post which led me to this Fairphone Forum thread/comment - so, thanks to user <code>UPPERCASE</code> for finding the fix.</p> <p>That's all the bugs and fixes that I've seen for now.</p> <p>~ Matt Smith / Harmelodic</p>"},{"location":"blog/software-engineering/","title":"Software Engineering Overview","text":"<p>Currently just a bunch of various articles on Software Engineering-related topics.</p> <p>Will reorganise this at some point.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/","title":"12 Factor App Methodology is good","text":"<p>Originally published: 17 October 2020</p> <p>Back in 2011, Heroku published a methodology called 12 Factor App, which denotes a methodology for building systems that are portable and resilient.</p> <p>I've found the 12 factor app to be an incredibly powerful design methodology for organising, structuring and separating applications, founded in other solid Engineering principles, like \"Package-by-feature, build-by-layer\", \"Stateless Computation\", \"Unix Philosphy\", \"Composition over Inheritance\", \"KISS\".</p> <p>Below are the 12 factors in the metholodgy - it may be a long read, but it's super important to read and understand each.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#the-methodology","title":"The Methodology","text":""},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#codebase","title":"Codebase","text":"<p>There should be exactly one codebase for a deployed service with the codebase being used for many deployments.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#dependencies","title":"Dependencies","text":"<p>All dependencies should be declared, with no implicit reliance on system tools or libraries.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#config","title":"Config","text":"<p>Configuration that varies between deployments should be stored in the environment.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#backing-services","title":"Backing services","text":"<p>All backing services are treated as attached resources and attached and detached by the execution environment.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#build-release-run","title":"Build, release, run","text":"<p>The delivery pipeline should strictly consist of build, release, run.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#processes","title":"Processes","text":"<p>Applications should be deployed as one or more stateless processes with persisted data stored on a backing service.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#port-binding","title":"Port binding","text":"<p>Self-contained services should make themselves available to other services by specified ports.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#concurrency","title":"Concurrency","text":"<p>Concurrency is advocated by scaling individual processes.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#disposability","title":"Disposability","text":"<p>Fast startup and shutdown are advocated for a more robust and resilient system.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#devprod-parity","title":"Dev/Prod parity","text":"<p>All environments should be as similar as possible.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#logs","title":"Logs","text":"<p>Applications should produce logs as event streams and leave the execution environment to aggregate.</p>"},{"location":"blog/software-engineering/12-factor-app-methodology-is-good/#admin-processes","title":"Admin Processes","text":"<p>Any needed admin tasks should be kept in source control and packaged with the application.</p>"},{"location":"blog/software-engineering/an-ideal-ci-cd-pipeline-old/","title":"An ideal CI/CD pipeline (Old)","text":"<p>Originally published: 27 November 2018</p>"},{"location":"blog/software-engineering/an-ideal-ci-cd-pipeline-old/#not-applicable-anymore","title":"Not applicable anymore","text":"<p>Like with some of my other posts - this really isn't applicable anymore. I think on reflection I was documenting what I knew at the time, rather than critically thinking about what testing was needed and how to do it.</p> <p>Nowadays, I shift tests left a lot and have a strong recommendation against creating permanent or automated environments for testing.</p> <p>I'll leave this post here, and maybe I'll write a new one on testing.</p>"},{"location":"blog/software-engineering/an-ideal-ci-cd-pipeline-old/#original-post","title":"Original Post","text":"<p>CI/CD pipelines have become an industry standard for taking a project from source code to a deployed package on a platform.</p> <p>As part of these pipelines, we need to perform different types of testing and analysis to prove that the code that has been written complies with the standards that the software engineers define.</p> <p>Here is my ideal pipeline that I work towards:</p>"},{"location":"blog/software-engineering/an-ideal-ci-cd-pipeline-old/#pipeline","title":"Pipeline","text":"<ul> <li> <p>Unit Tests - tests the functionality for individual functions/methods within your application.   Requirement: A testing framework.</p> </li> <li> <p>Component Tests - performs component-level integration tests to test application functionality.   Requirement: An integration testing framework as well as mocks/stubs for an external services (Databases, APIs, I/O,   etc.)</p> </li> <li> <p>Code Analysis - perform static code quality analysis on your source code. The failure threshold can and should be   configured on a project by project bases.   Requirement: A static analysis tool(s) to do this, like SonarQube.</p> </li> <li> <p>Security Analysis - performs security and malware checks on your source code. Like with Code Analysis, the failure   threshold can and should be configured.   Requirement: A security analyis tool to do this, like Snyk or JFrog Xray.</p> </li> <li> <p>Build - builds your application into an artifact and store it in a Repository Manager/Registry.   Requirement: A build/packaging tool, like npm or Maven, plus any software/plugins required for artifact, like Docker   or the Fabric8 plugin.</p> </li> <li> <p>Deploy to SIT - automatically deploys to your first testing environment for System Integration Testing.   Requirement: Has passed all automated tests.</p> </li> <li> <p>Integration Tests - performs end-to-end integration tests on the entire system to ensure the application hasn't   broken existing functionality and proves new functionality.   Requirement: End-to-end integration test suite.</p> </li> <li> <p>Simple Performance Load Tests - performs a short performance load test to prove application can handle high   load.   Requirement: A performance testing tool, like JMeter.</p> </li> <li> <p>Deploy to UAT Environment - automatically deploys to second testing environment for User Acceptance Testing.   Requirement: Has passed all SIT tests.</p> </li> <li> <p>Manual UAT Testing - User Acceptance Testing performed by the client (or similar authoritative entity) to prove   system meets requirements &amp; expectations.   Requirement: UAT Test Team with appropriate resources &amp; tools.</p> </li> <li> <p>Deploy to Pre-prod Environment - manual deployment to Pre-prod Environemnt for final testing before production.   Requirement: Has passed all UAT tests.</p> </li> <li> <p>Penetration Testing - manual penetration testing performed by a security/ethical hacking team.   Requirement: Penetration Test Team with appropriate resources &amp; tools.</p> </li> <li> <p>Deploy to Production - manual deployment to Production.   Requirement: Has passed all Pre-prod tests.</p> </li> </ul>"},{"location":"blog/software-engineering/an-ideal-ci-cd-pipeline-old/#pull-request-pipelines","title":"Pull Request Pipelines?","text":"<p>I have purposefully left out things like Merge Requests/Code Reviews as these should happen before this pipeline is enacted.</p> <p>However, when a Pull Request is created, then a simplified Pull Request-focused pipeline should be run. I would suggest that this simplified pipeline would run from Unit Tests up to and including Build, however the build should be tagged as a temporary build and stored in a separate location in the Repository Manager, to prevent confusion and misconfiguration.</p> <p>After being built, the application could be automatically deployed into an isolated, temporary testing environment for the Pull Request Reviewers to check that the PR functions as expected.</p>"},{"location":"blog/software-engineering/an-ideal-ci-cd-pipeline-old/#overkill","title":"Overkill?","text":"<p>Many will view this pipeline as \"overkill\", and for most solutions, it is. This post, however, is not a \"you must implement this pipeline\", it is a proposed goal that all engineers should strive for. I do not expect all engineering teams in the world to abide by this, or follow it to the letter, as limitations (such as time and cost) can mean that only part of this pipeline could ever exist for a particular application/system. For example, Penetration Testing and Perfomance Testing could be done periodically, instead of for every change, or Performance Testing could be done in the Pre-prod environment instead of the SIT environment.</p> <p>This is simply my view of an ideal pipeline, based on my experience.</p>"},{"location":"blog/software-engineering/authorising-a-k8s-deployment-access-to-a-private-container-registry/","title":"Authorising a K8s deployment access to a private container registry","text":"<p>Originally published: 02 November 2018</p> <p>When a K8s deployment is created, when creating pods an image will need to be pulled down and used to create containers.</p> <p>If the Container Registry is private, then the Kubernetes deployment needs credentials, via a K8s secret, that can access the Container Registry.</p> <p>First create the Deploy Token (or just get some Registry credentials).</p> <p>Then create a <code>docker-registry</code> K8s secret, e.g.:</p> <pre><code>kubectl create secret docker-registry &lt;REPO&gt;-registry-deploy-token \\\n    --docker-server=\"registry.gitlab.com\" \\\n    --docker-username=\"gitlab+deploy-token-xxxxx\" \\\n    --docker-password=\"xxxxxxxxxxxxxxxxxxxx\" \\\n    --docker-email=\"xyz@example.com\"\n</code></pre> <p>Ensure your K8s deployment includes your image pull secret, e.g.:</p> <pre><code>apiVersion: apps/v1beta1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n        - name: example-app\n          # ...\n      imagePullSecrets:\n        - name: &lt;REPO&gt;-registry-deploy-token\n</code></pre> <p>Using a private repository: https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry</p> <p>ImagePullSecrets: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod</p> <p>Pulling an image from private repository: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/</p>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/","title":"Building and Deploying projects like a Pro (old)","text":"<p>Originally published: 30 June 2019</p>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#not-applicable-anymore","title":"Not applicable anymore","text":"<p>The world has moved on, and I have learned more and I don't really recommend many of what I put in this anymore, at all.</p> <p>Some is still relevant, but maybe some time in the future I'll write a post about what I think now... or not. I don't know, if you want to hear about it: let me know.</p>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#original-post","title":"Original post","text":"<p>A while ago, I wrote a blog about my ideal CI/CD pipeline.</p> <p>This post builds on that CI/CD pipeline post by going through the overarching Build and Deployment process of an ideal project. Again, like with that post, this is an ideal process. I am not dictating: \"Implement this or die\", but I am saying: \"I heavily recommend implementing this, or at the very least, getting as close as possible to implementing this, within your context\".</p>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#precursors","title":"Precursors","text":""},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#devops","title":"DevOps","text":"<p>You should be doing DevOps in your project team. I could go into this now, but I'll save that for another blog post.</p> <p>Basically what I mean by this is you should be managing your application development (the Dev) as well as your infrastructure &amp; CI/CD pipelines (Ops) all in your project team.</p>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#serverless","title":"Serverless","text":"<p>If you're doing things Serverless, there really isn't much to do when it comes to managing your infrastructure (hence it being called \"Serverless\") so only the Dev part really applies to you Serverless people.</p>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#development-practices","title":"Development Practices","text":"<p>Do the following:</p> <ul> <li>Agile (and not   the bad kind)</li> <li>TDD</li> <li>Trunk Based Development</li> </ul>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#containerization","title":"Containerization","text":"<p>You should be using Containers by now. When it comes to managing Containers, Kubernetes is leading the industry, so I recommend using that too.</p>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#automation","title":"Automation","text":"<p>You should be automating everything, wherever possible.</p> <p>For Dev automation: Write your application &amp; tests, use a build tool (Maven, npm, etc.), and execute in CI/CD pipelines. For Ops automation: Implement GitOps, using tools like Terraform and Ansible, and execute in CI/CD pipelines.</p>"},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#dev","title":"Dev","text":""},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#build","title":"Build","text":""},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#deployment-promotion","title":"Deployment &amp; Promotion","text":""},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#ops","title":"Ops","text":""},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#build_1","title":"Build","text":""},{"location":"blog/software-engineering/building-and-deploying-projects-list-a-pro-old/#deployment-promotion_1","title":"Deployment &amp; Promotion","text":""},{"location":"blog/software-engineering/connecting-ci-cd-to-kubernetes-through-a-kubernetes-service-account/","title":"Connecting CI/CD to Kubernetes through a Kubernetes Service Account","text":"<p>Originally published: 01 November 2018</p> <p>When using something like <code>kd</code> within GitLab's CI/CD, you'll need a K8s Service Account and thus a token for that K8s Service Account to allow the deployment agents access to cluster.</p> <pre><code>kubectl create serviceaccount gitlab-sa\n</code></pre> <p>This will create a service account but we'll need to grant it permission to perform things like deployments.</p> <pre><code>kubectl create rolebinding gitlab-edit --clusterrole=edit --serviceaccount=default:gitlab\n</code></pre> <p>Then you'll need to get the token:</p> <pre><code>kubectl get secrets\n</code></pre> <pre><code>kubectl get secret &lt;SECRET_NAME&gt; -o json | jq -r .data.token | base64 -D\n</code></pre> <p>K8s Rolebinding Permissions for Service Accounts: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions</p> <p>K8s User facing Cluster Roles: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles</p>"},{"location":"blog/software-engineering/double-not-with-truthy-falsy-values-javascript/","title":"Double-not with Truthy/Falsy values - JavaScript","text":"<p>Originally published: 05 October 2018</p>"},{"location":"blog/software-engineering/double-not-with-truthy-falsy-values-javascript/#truthyfalsy","title":"Truthy/Falsy","text":"<p>In JavaScript, not only do we have normal boolean (true/false) values, but we also have the idea of \"Truthy\" and \"Falsy\" values. These are values that may not boolean, but will still resolve a conditional statement.</p> <p>All the Falsy values are:</p> <pre><code>if (false) {\n}\nif (null) {\n}\nif (undefined) {\n}\nif (0) {\n}\nif (NaN) {\n}\nif ('') {\n}\nif (\"\") {\n}\nif (document.all) {\n}\n</code></pre> <p>Any value that isn't one of the above, is Truthy. For example:</p> <pre><code>if ({}) {\n}\nif (\"badger\") {\n}\nif ([\"Amy\", \"Steve\", \"Geoff\"]) {\n}\nif (new Date()) {\n}\n// etc.\n</code></pre> <p>You can read more about Truthy and Falsy on MDN:</p> <ul> <li>https://developer.mozilla.org/en-US/docs/Glossary/Falsy</li> <li>https://developer.mozilla.org/en-US/docs/Glossary/Truthy</li> </ul>"},{"location":"blog/software-engineering/double-not-with-truthy-falsy-values-javascript/#not-and-double-not","title":"Not and Double-Not","text":"<p>Most programmers are aware of the \"not\" logical operator: <code>!</code>; which flips the boolean output of a conditional statement. For example:</p> <pre><code>let something = \"Badger\";\n\nif (!(something === \"Steve\")) {\n}\n// This will resolve true, \n// as `something === \"Steve\"` is false, \n// and the `!` flips it to true.\n</code></pre> <p>But a lot of programmers aren't aware that you can chain these logical operators:</p> <pre><code>let something = \"Badger\";\n\nif (!!(something === \"Steve\")) {\n}\n// This will resolve false, \n// as `something === \"Steve\"` is false, \n// and the `!` flips it to true, \n// and the second `!` flips it to false.\n</code></pre>"},{"location":"blog/software-engineering/double-not-with-truthy-falsy-values-javascript/#combining-truthyfalsy-with-the-double-not","title":"Combining Truthy/Falsy with the Double-Not","text":"<p>Taking the above 2 concepts, we can combine them and create some clean conditional statements that convert Truthy/Falsy falues into actual boolean values.</p> <pre><code>let something = \"Badger\";\n\nif (!!something) {\n}\n// Resolves to true, \n// as \"Badger\" is Truthy, \n// which flips to a boolean false, \n// which flips to boolean true.\n</code></pre>"},{"location":"blog/software-engineering/double-not-with-truthy-falsy-values-javascript/#why-is-this-useful","title":"Why is this useful?","text":"<p>If you're building an application and you're passing variables around, sometimes you want to check if something actually has a value. This is common for choosing whether to render something, performing a validation check, or whether to run some code using a particular value. However, we often pass these variables to external functions &amp; components to perform this logic, and passing in the value of the variable can expose data to a function that shouldn't have it, or can cause quirky errors (this happens a lot when things resolve to Truthy because the code is working with an non-boolean value).</p> <p>So, if you want to anonymise and solidify these values, whilst also passing something useful to a function, the double-not combined with truthy/falsy is a clean way to do it.</p>"},{"location":"blog/software-engineering/effective-error-exception-handling/","title":"Effective error / exception handling","text":"<p>Error/Exception handling in programming is when we write code to \"handle\" what should happen when things go wrong as part of processing something.</p> <p>When developing any sort of application, \"things going wrong\" can be classified in the following ways:</p> <ol> <li>An issue that occurs that cannot be recovered from in the application code.</li> <li>An issue that occurs that can be recovered from in the application code.</li> </ol> <p>In some programming languages, there is a neat (in my opinion) separation between \"Errors\" and \"Exceptions\": the unrecoverable issues are called Errors and the recoverable ones are called Exceptions.</p> <p>Errors are innately unrecoverable, and the application will either crash or be killed by the Operating System. Exceptions may cause the application to fail processing something, but it probably shouldn't crash the application or cause the Operating System to kill the application's process.</p> <p>Since errors are innately unrecoverable and most of the time don't need \"handling\", I'll be talking about application development, the scope of this blog post is around Exception handling.</p>"},{"location":"blog/software-engineering/effective-error-exception-handling/#why-exception-handle","title":"Why Exception Handle","text":"<p>Exception handling is all about safety, user experience and maintenance.</p> <p>Safety: If an exception is thrown, then we want it to be handled in a controlled manner that doesn't result in vulnerabilities (information being exposed, errors being used as an attack vector) or unexpected issues.</p> <p>User Experience: If an exception is thrown, then we want the user to be informed of that in a controlled way, as well as data being treated correctly (data being saved, rollbacks being performed, etc.).</p> <p>Maintenance: As we develop software, we tend to split software into components &amp; layers, and abstract implementation details inside of classes, methods and functions. As we abstract the implementation details of how something is processsed, it is preferable to also abstract what can go wrong, rather than have higher-level components deal with all possible exceptions that come from our implementation. Whilst often this results in further exceptions being thrown, more contextually appropriate exceptions hide the implementation details.</p> <p>I've often heard developers discuss the questions: \"Should we exception handle, or not? If we should, what's the best way to do it?\".</p> <p>These question often arise from 2 areas:</p> <ol> <li>A frustration with feeling required to do exception handling when it isn't needed. Writing the the \"happy paths\" of    how to process things is relatively easy (and fun) to do, but writing all the code to handle when those things go    wrong is often a lot more work and often a lot less fun, since often a lot more can go wrong.</li> <li>A desire for an easy &amp; effective way to do exception handling, when we believe it is required. Exception handling can    often be quite involved, depending on the language, and many developers feel it clutters the code.</li> </ol> <p>These are understandable sentiments, and so I'm going to throw my thoughts into the conversation.</p> <p>Here's some bold statements:</p> <ul> <li>For most software that is written (not including hacks), developers must handle exceptions that occur, if it is to be   safe and useful.</li> <li>APIs should make it clear what can go wrong and force consumers of the API to choose not to handle them, rather than   choosing to handle them.</li> <li>There is no \"one rule that fits all\" when it comes to how to implement exception handling, it's more like \"two ways of   thinking and a few rules\".</li> </ul>"},{"location":"blog/software-engineering/effective-error-exception-handling/#implementing-exception-handling","title":"Implementing Exception Handling","text":""},{"location":"blog/software-engineering/effective-error-exception-handling/#checked-unchecked-exceptions","title":"Checked &amp; Unchecked Exceptions","text":"<p>In languages such as Java, Exceptions are split are either:</p> <ul> <li>Checked Exceptions: exceptions that the compiler checks to see if they have been handle.</li> <li>Unchecked Exceptions: exceptions that the compiler does not check to see if they've been handled.</li> </ul> <p>In Java, this results in the following class distinctions:</p> <ul> <li><code>Error</code> - unrecoverable issues.</li> <li><code>Exception</code> - Checked Exceptions</li> <li><code>RuntimeException</code> - Unchecked Exceptions.</li> </ul> <p>Personally, I think the name <code>RuntimeException</code> is a confusing one, as ALL Exceptions (and Errors) occur at runtime. Therefore, I find it helpful to think of <code>RuntimeException</code>s as exceptions that are handled at runtime, rather that being handled explicitly by developers in the code, thus at compile time.</p> <p>So, why do we even have unchecked exceptions? If executing piece of code may result in an exception being thrown, why not handle all of those cases? Wouldn't that be safer?</p> <p>Yes, but that's not always the most valuable thing for a piece of software or to a developer. Dun dun duuuunnnnn! Nuance?! Who knew?! Here are some common complaints I've heard/read from developers:</p> <p>Checked exception handling adds more code to an application's codebase. More code means more opportunities for bugs to occur inside of the code base.</p> <p>I often find this is not the case when doing exception handling as exception handling is usually the code you write to handle the class of bugs associated with \"what happens when processing goes wrong\".</p> <p>Checked exception handling adds more code to an application's codebase. More code means more code to read and understand before being able to contribute, edit and change code. This makes it harder to maintain, not easier.</p> <p>This can be absolutely true, for small, simple and/or non-critical applications. However as the application grows and/or the criticality of the application grows, the importance of handling exceptions carefully in the different components of the application usually becomes highly valuable for maintainance and developer confidence in the application's robustness.</p> <p>The added coverage of handling exceptions being present in the code can also convey how careful and serious a developer must be when performing changes to the application code.</p> <p>Finally, effective exceptional handling where we abstract exceptions just as we abstract implementation details, should result in changes to one part of an application not affecting all other parts of the application. This often makes refactoring &amp; maintenance easier. Ineffective exception handling absolutely makes maintenance harder. I think a lot of developers believe exception handling is ineffective because they've only experienced ineffective exception handling.</p> <p>Checked exception handling adds more code to an application's codebase. More code for exception handling can just result in code for the sake of code and provides no extra aide to the developer or the consumer of the application.</p> <p>Again, for small, simple and/or non-critical applications this can be absolutely true, and it can be much tidier in the code base to let all exceptions bubble up and be handled at the top of the stack, because the number of exceptions are small. As the number of exceptions increase, and as the desire for components to be more scoped on their own tasks than needing to throw or handle an endless list of exceptions, I prefer to handle exceptions where they occur, even if that means I'm often wrapping and throwing exceptions.</p> <p>For application developers that prefer using an Inversion of Control (IoC) system (where unchecked exceptions are caught by the IoC system and are handled according to instructions given to it by developers or handled in a default manner), unchecked exceptions can also be a neat way of offloading the exception handling to a separate system. Personally, I've not found this a better paradigm, as I value handling the exceptions in a controlled, explicit way where the the exception occurs more valuable to me when developing &amp; maintaining code than giving exception handling to a separate system - this allows me to contribute to parts of a codebase without needing to understand the system as a whole, which is very useful for larger or more complex systems.</p> <p>Sometimes the safest and quickest thing to do is to let a process/thread just crash by throwing an unchecked exception, rather than try to handle the exception and recover.</p> <p>Absolutely true, the key word in this is \"sometimes\".</p> <p>This complaint touches upon notions derived from Fail-fast systems. However, I think an important to distinguish this into two parts:</p> <ol> <li>Preventing exceptions</li> <li>Handling an issue by throwing an unchecked exception</li> </ol> <p>Fail-fast systems report issues with inputs or the current state of a system as earlier as possible to remove the number of possible exceptions that can go wrong, rather than handle each of those exceptions. This is preventing exceptions from happening, and should absolutely be done by implementing validation checks on inputs and verification checks on system state. However, it will not be possible to completely prevent all exceptions this way, thus normal \"try/catch\" exception handling will still apply.</p> <p>In certain situations, we as developers may introduce a programming error into an application inadvertently, which has a knock on effect of causing an exception. In these cases, these exceptions that are thrown are probably better handled as unchecked exceptions, so that the rest of the application doesn't need to deal with a fault that lies with the code, not with a runtime irregularity.</p> <p>For example, when generating encryption keys in Java, we can instantiate a <code>KeyPairGenerator</code> as follows:</p> <pre><code>KeyPairGenerator generator = KeyPairGenerator.getInstance(\"RSA\");\n</code></pre> <p>The <code>KeyPairGenerator::getInstance</code> method throws a <code>NoSuchAlgorithmException</code> because \"RSA\" might not be a valid algorithm. Given this is often just a hard-coded string in applications, this would produce the <code>NoSuchAlgorithmException</code> if it was incorrectly set - which is an checked exception. In this case, I would attempt to create the KeyPairGenerator at the start of the application, and if a incorrect algorithm was used, then just throw a runtime exception and error out, as there is no point continuing with a broken KeyPairGenerator.</p> <p>Incidentally, in this situation, I would prefer that the method took a enumeration rather than a String and thus not need to throw this exception as it would only compile if it had been given a valid enumeration - but since it doesn't, I'm using it as an example.</p> <p>To summarise Checked vs Unchecked Exceptions:</p> <ul> <li>Unchecked exceptions can be useful as they allow a developer to choose to ignore handling an exception within the   context of the code and instead either (a) handle the exception elsewhere if desired or (b) treat the exception as if   it were an Error.</li> <li>Checked exceptions force exception handling to be coded before compile time, which leads to a safer and (often, but   not always) more maintainable codebase.</li> <li>Given these trade-offs, it's important to critically think about the context of the software and its components of the   before choosing between handling an exception as a Checked or Unchecked exception. (Doing this more makes you faster   and better at it, and it soon becomes natural)</li> </ul> <p>Personally, I prefer to handle Checked Exceptions for most things and have the compiler enforce exception handling, then selectively use Unchecked Exceptions for exceptions that I want to simply handle as if they were Errors.</p> <p>Interestingly, Oracle has a small written piece on Unchecked vs Checked exceptions, in the Java Tutorials, that lines up with my opinion... mostly.</p>"},{"location":"blog/software-engineering/effective-error-exception-handling/#thinking-about-contracts","title":"Thinking about Contracts","text":"<p>In order for an exception to be handled, an exception must be thrown. Functions/methods throw exceptions.</p> <p>Just as with the function name, arguments and return type, the exceptions the functions throw are part of the function's definition or \"contract\". Upholding that contract to prevent breaking changes being introduced and causing problems is an important part of code maintenance - especially for functions that are widely used (e.g. in libraries or core components of systems).</p> <p>Using the above analysis of Checked vs Unchecked, it can be further beneficial to the consumers of functions to be given Checked exceptions as this provides a more explicit contract for what to expect the function to do. It is then up to the Consumer to decide how they wish to handle the exception, if they can.</p> <p>Some systems take a different approach, such as the Spring Framework for Java, which for... some reason, throws almost exclusively unchecked exceptions. This has the nice effect of not needing to wrap exceptions as <code>RuntimeException</code>s when you don't want to handle them, but has the negative effect of massively obscuring the contract for the Spring-provided functions. Maybe this is intentional, but it often makes exception-handling with Spring awkward, as you're not 100% confident that all the exceptions that the function might throw have been handled.</p> <p>Some people reason this choice to be because of \"best practice\" or reducing code clutter or following some design principles or increasing decoupling, but most of this is plain false when you actually think about it, and I've found little to no evidence of Spring's decision-making to use almost exclusively unchecked exceptions.</p> <p>My suspicion is that they intend for Spring to handle all exceptions via some Inversion of Control or Aspect-Oriented Programming paradigm - both of which result in exception handling being separated from the code that consumes methods that throw exceptions - in my opinion, this reduces code cohesion and makes control flow unclear.</p> <p>...or someone at Spring read \"Clean Code\" and took everything to heart.</p>"},{"location":"blog/software-engineering/effective-error-exception-handling/#catching-and-handling-exceptions","title":"Catching and handling Exceptions","text":"<p>Assuming a checked exception has been thrown that we wish to handle, there are a multiple ways to implement handling the exception:</p> <p>First, there is the \"catch, wrap and throw\" method, which is very common. This is where we catch the Exception thrown, wrap it in an exception more appropriate for the current layer/component to abstract the implementation and give context to a stack trace that may be generated later, and then throw the exception to the consumer of the function. This methodology has benefits such as keeping exceptions thrown to consumers more contextually-relevant and decouples the implementation details of the function from the function's contract. However, depending on how many exceptions are thrown during execution, and how many are thrown - this can introduce quite a lot of code to simply wrap &amp; throw exceptions to the consumer of the method. This can be acceptable pain in a small context, but when it's occuring a lot within a codebase, refactoring can be really beneficial to increase code-cohesion.</p> <p>Another method of exception handling is to \"catch, recover and continue\". This is where we catch the exception thrown, recover from the issue (by backing off to some default values, or retrying processes) and then continuing as normal. This methodology is highly dependent on whether recoverability is possibly given the business requirements, but has a neat benefit where we are explicitly trying to recover and continue the process, given it may be possible, to increase the robustness of the process. The down-side is that the process is not failing fast, and we are spending our development time improving robustness, when it could be easier for the consumer of the function (or the end user) to just retry the process themselves, as they require.</p> <p>The final method for exception handling is to \"not catch, and only throw\". This is where we implement zero exception handling and instead just allow the exception to be passed to the consumer directly. This benefits from basically zero code additions (other than stating in our function's contract that we throw the exception), but means that an implemention detail of our function is exposed to the consumer. Given this, I usually find this method only useful when the exception I would throw to the consumer is the same as the one caught - therefore I can save myself some lines of code and just throw the exception.</p> <p>As we can see, there are different trade-offs for different methods, and it's up to the developers of the software to decide how to handle each exception that is thrown. It sounds like a slow and nasty business, but gets quite easy and natural the more you do it, and the result is safe, maintainable code that has a good user/consumer-experience.</p>"},{"location":"blog/software-engineering/effective-error-exception-handling/#can-multiple-return-values-or-optionals-be-good-alternatives","title":"Can multiple return values or Optionals be good alternatives?","text":"<p>Some languages and developers don't want to handle exceptions in this \"throw and catch\" way, and instead return something different.</p> <p>The two main ways to do this is to either:</p> <ol> <li>Return multiple values from a function (the caller receives a successful result or an error/exception result)</li> <li>Return an Optional value, where a successful result may be present in the Optional.</li> </ol> <p>In the case of multiple values, I view this as basically the same as \"throw and catch\", however we're simply \"throwing\" the exception in a different way. Rather than catching the exception, we instead check if the error/exception result is not null before do exception handling. Personally, I don't find this any better than a try/catch system - in fact, I think it's slightly worse as I usually view return-values as indicative that everything went fine, and a try/catch system is used for when something went wrong, which I find a neat separation of concerns. Of course, sometimes you don't get a choice. Languages like Rust and Go do not provide an exception handling system, opting instead for handling errors in the return values. Languages like Java, JavaScript, C++ and Python provide a try/catch exception handling system. My advice would be to use the system the language provides you (and not try to force your preferences on a language system not designed for your preferences).</p> <p>In the case of Optionals, I think Optionals should be used for a different purpose than exception handling. Optionals in return values convey (to me) that a return value of <code>null</code> is an acceptable result where everything went right and the result was <code>null</code>. This should not be confused with exception-handling since that deals with the business of how to handle situations when things have gone wrong. Basically: Optionals aren't for when things go wrong, so don't use them for that!</p>"},{"location":"blog/software-engineering/effective-error-exception-handling/#conclusion","title":"Conclusion","text":"<p>Of course, some systems need to never fail, and every possibility should be known and factored out at compile time, such as systems for Space-travel, medical systems and... basically anything that could result in life or death.</p> <p>The less critical the application, or less maintenance it needs, or less we care about code cohesion, the more you can fail fast and care less about exception handling.</p> <p>If in doubt: Throw exceptions, handle exceptions, and be kind to your fellow developers &amp; users. I take the view that safer code usually leads to better software.</p> <p>~ Harmelodic / Matt Smith</p>"},{"location":"blog/software-engineering/git-basics/","title":"Git Basics","text":"<p>I've used Git a lot in my career and it has become a standard for me (and the rest of the industry) to use Git to manage code files.</p> <p>But what about the everyday folks &amp; new developers who don't already live and breath this tech-y stuff? Reading the official Git website documentation and going to sites like GitHub and GitLab can make learning and using Git quite daunting and confusing.</p> <p>Even worse, Cloud Service Providers like Amazon, Cloudflare, Google, and Microsoft often provide website-hosting services that \"source from Git\" and lock off features if you don't use Git!</p> <p>So, we have this daunting thing to learn that you kinda HAVE to use if you want to host a website.</p> <p>Assuming you are someone who is feeling like this, this post will hopefully help you - or maybe you learnt the basics of Git before and have just forgotten and need to remember the concepts and basic commands.</p>"},{"location":"blog/software-engineering/git-basics/#what-is-git","title":"What is Git","text":"<p>Git is a Version Control System (VCS) or a Source Control Management system (SCM). Those two terms are pretty much interchangeable - feel free to use the one you prefer.</p> <p>What a good VCS or SCM system (like Git) does for you is to provide a way to store code and perform changes to that code in systematic ways. This systematic method of handling changes empowers us to:</p> <ul> <li>Handle the merging of multiple changes to the code provided by different people (allowing for easier collaboration on   the code)</li> <li>Easily find who made certain changes and when they were made</li> <li>Undo certain changes if you need to</li> </ul> <p>all while not needing worry about weird quirks &amp; faults that come with using more automatic/guessing system of storing things.</p> <p>Git was authored by Linus Torvalds, the same guy who authored Linux.</p>"},{"location":"blog/software-engineering/git-basics/#a-note-or-two-on-commands","title":"A note or two on commands","text":"<p>Before we move forward, this guide features a lot of commands. Whilst there are some tools with windows and buttons that help people use Git, I personally find they make things more confusing that Git actually is.</p> <p>Also, whenever you are running any of these commands, you will most likely want to have your terminal be in the root of your Git repository (you'll find out about Git repositories shortly).</p>"},{"location":"blog/software-engineering/git-basics/#installing-git","title":"Installing Git","text":"<p>If you're on Windows, got to the Git website to download an installer and then follow the instructions.</p> <p>On macOS, I would recommend installing git through Homebrew - by running <code>brew install git</code></p> <p>On Linux, use your preferred package-manager to install the <code>git</code> package.</p>"},{"location":"blog/software-engineering/git-basics/#repositories","title":"Repositories","text":"<p>To start working with Git with our code, we first need to know about the idea of a \"Git repository\".</p> <p>A Git repository is a folder on a computer, that has a special folder inside it called <code>.git</code> that stores all the data that Git needs to function. Unless you know what you're doing, NEVER touch, use or delete this <code>.git</code> folder.</p> <p>A repository on your laptop or computer that you're working on is often referred to as your \"local\" repository.</p> <p>A repository on a different computer - e.g. stored on GitHub, GitLab or a server - is often referred to as a \"remote\" repository.</p> <p>Semantically, a repository most often contains the code for a single coding project - like a single website, application or logical bundle of configuration files. Many developers &amp; companies like bundling ALL or MANY of their coding projects into a single Git repository, referred to as a \"monorepo\" - the jury is still out on whether monorepos work effectively or not (personally, I don't think they do).</p>"},{"location":"blog/software-engineering/git-basics/#local-repositories","title":"Local Repositories","text":"<p>To make a Git repository on your local computer, create a folder to store your code in and then open a Command-line / Terminal window in that folder and run the command:</p> <pre><code>git init\n</code></pre> <p>(Incidentally, you can have code in this folder already, just make sure it's not in a sub-folder called <code>.git</code>)</p>"},{"location":"blog/software-engineering/git-basics/#remote-repositories","title":"Remote Repositories","text":"<p>If you want to share your code to collaborate with other people or connect your code with a 3rd party service (like Cloudflare or Google Cloud), or you just want to make your code publicly viewable (i.e. make it \"open-source\") then you'll need to create a remote Git repository.</p> <p>This can be done on websites like GitHub or GitLab. I recommend using GitHub, since it's most popular and thus other websites almost ALWAYS provide integrations with it AND is one of the sites that has support for unlimited private repositories (in case you don't want to make your code public).</p> <p>When first setting this up, you'll need to configure your local repository to point to your remote repository. GitHub provides nice documentation for how to do this, with troubleshooting tips.</p> <p>If you just need the command though, it should be:</p> <pre><code>git remote add &lt;remote_name&gt; &lt;remote_URL&gt;\n\n# e.g. adding a remote repo that you will connect to via SSH:\ngit remote add origin git@github.com:Harmelodic/my-cool-site.git\n\n# e.g. adding a remote repo that you will connect to via HTTPS:\ngit remote add origin https://github.com/Harmelodic/my-cool-site.git\n</code></pre> <p>If you want to learn about connecting to remote repos via SSH, I also have a guide on how to do this.</p> <p>Later, when you've \"committed\" your code &amp; code changes into Git, you'll be able to \"push\" them from your local repository to your remote repository. Then, if there are new changes in your remote repository but not locally, you can \"pull\" those changes down to your local repository.</p> <p>These actions are, respectively, done by performing the commands:</p> <pre><code>git push\ngit pull\n</code></pre> <p>You can think of this as kinda like uploading and downloading code changes.</p>"},{"location":"blog/software-engineering/git-basics/#status","title":"Status","text":"<p>Once we have a Git repository to work with, we can always check the status of our Git repository by performing the command:</p> <pre><code>git status\n</code></pre> <p>Personally, I do <code>git status</code> A LOT - often before and after every other command. This might seem paranoid, but it really helps me stay oriented with what is going on with my Git repository and the changes.</p>"},{"location":"blog/software-engineering/git-basics/#branches","title":"Branches","text":"<p>Up until now, you've probably been working on a single copy of your code. Branches allow us to work on different copies, ideally temporarily before \"merging\" that branch with all its changes back into our main copy of the code.</p> <p>If you're going to be working by yourself on code, then you'll probably never need think about branches and instead only ever work on the \"main\" branch, usually named \"main\" or \"master\".</p> <p>This main branch is automatically created for you when you initialise a Git repository.</p> <p>If you're working with other people and regularly pushing code, it can be easier to review and manage these changes by putting them on a branch and then creating a \"Pull Request\" or \"Merge Request\" (depending on what the website calls them) to merge the changes on your branch into the main branch. You can read more about basic branching with the following documentation:</p> <ul> <li>Git-SCM's \"Basic Branching and Merging\": https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging</li> <li>Atlassian's \"Using Branch\" tutorial: https://www.atlassian.com/git/tutorials/using-branches</li> </ul>"},{"location":"blog/software-engineering/git-basics/#commits","title":"Commits","text":"<p>When we work on our code, we make changes and save files to our computer. None of this changes with the introduction of Git, it's just that Git tracks the files inside your repository and detects what has been changed.</p> <p>However, at some point we need to tell Git: \"I'm done with the editing/writing of my code\" so that Git can efficiently bundle all your changes into one bigger change. For Git, this is when you \"commit\" your changes.</p> <p>Before we can commit, we need to prepare our changes. Sure we've changed lots of files, but that's not good enough for Git. We need to \"add\" the specific changes we want to commit to be \"ready to be committed\" (because sometimes we make changes but don't want to commit all of them). We add changes with the command:</p> <pre><code>git add &lt;file&gt;\n</code></pre> <p>If you want to add ALL changes to your commit, then just do:</p> <pre><code>git add .\n</code></pre> <p>Then to commit the files you've added, you can do:</p> <pre><code>git commit -m \"Some commit message summarising your change\"\n</code></pre> <p>and that's it!</p> <p>If you have a remote repository configured, you can push this commit to your remote repository by doing:</p> <pre><code>git push &lt;remote_name&gt; &lt;branch&gt;\n\n# e.g.\ngit push origin main\n</code></pre> <p>or just:</p> <pre><code>git push\n</code></pre>"},{"location":"blog/software-engineering/git-with-ssh/","title":"Git with SSH","text":"<p>When first starting out with Git and remote repositories, most people connect with HTTPS URLs, and thus use Username/Password to authenticate.</p> <p>However, this has a couple of drawbacks:</p> <ul> <li>Typing in your Username/Password every time can be really annoying and runs the risk of you accidentally leaking your   password, even if you save it in a password manager or when using Git Credential Manager.</li> <li>Alternatives to using Username/Password have their own flaws:<ul> <li>Custom command line tools like Github's <code>gh</code> establishes an element of \"vendor lock-in\" by using that tool. Also,   it just sucks to learn an extra thing for something that should be simpler.</li> <li>PATs (Personal Access Tokens) aren't supported by all remote-repository hosts, and are basically just... another   type of password that you'll have to manage.</li> </ul> </li> </ul> <p>SSH is a more elegant and more secure alternative.</p> <p>Rather than connecting to your remote repository host (like GitHub) over the HTTPS protocol, you would connect over the SSH protocol and thus we can take advantage of using SSH's way of handling authentication rather than Username/Password.</p> <p>SSH uses a \"key pair\" to do authentication. A key pair is, as it sounds, a pair of keys. Not physical keys, but \" cryptographic\" keys. The two keys in a key pair are:</p> <ul> <li>A Private key</li> <li>A Public key</li> </ul> <p>The Private Key is a file that you keep on your computer and keep secret. NEVER give your Private Key to anyone.</p> <p>The Public Key is a file that is on your computer, that you give out to other people.</p> <p>When we create key pair, these two keys are associated with one another, which produces the neat result where a piece of data encrypted with one of the keys can be decrypted with the other key - and since the key is explicitly associated with you, it can also be used for authentication!</p> <p>So... bringing this back to Git, if we configure Git to use SSH, we can securely communicate Git changes to &amp; from our remote repository host, all without needing to send credentials (like you do with Username/Password) to authenticate.</p>"},{"location":"blog/software-engineering/git-with-ssh/#configuring-git-to-use-ssh","title":"Configuring Git to use SSH","text":"<p>To configure Git to use SSH, follow the below steps - replacing the email in the commands with your email.</p>"},{"location":"blog/software-engineering/git-with-ssh/#1-generate-your-ssh-key-pair","title":"1. Generate your SSH key pair","text":"<ol> <li>Run the following command to begin:    <code>bash    ssh-keygen -t ed25519 -C \"me@example.com\"</code></li> <li>When asked what file to save it in, just press enter. This will create your key pair in the default file locations:    <code>~/.ssh/id_ed25519</code> for the private key, and <code>~/.ssh/id_ed25519.pub</code> for the public key.</li> <li>When asked for a passphrase to file... you can choose to use a passphrase if you want. Personally, I don't bother and    I don't think you should either (some security-minded folks might not like me for that).</li> </ol>"},{"location":"blog/software-engineering/git-with-ssh/#2-add-your-private-key-to-your-computers-ssh-agent","title":"2. Add your Private key to your computer's ssh-agent","text":"<p>This is an important step because Git will use the <code>ssh-agent</code> system on your computer to handle SSH connections. Whilst we've now created an SSH key pair, we need to tell the <code>ssh-agent</code> system that it's OK to use the Private key in this new key pair for SSH connections.</p> <ol> <li>Start the <code>ssh-agent</code> in the background, just in case it's not started:    <code>bash    eval \"$(ssh-agent -s)\"</code></li> <li>Add your Private key to the <code>ssh-agent</code> system:    <code>bash    ssh-add ~/.ssh/id_ed25519</code></li> </ol>"},{"location":"blog/software-engineering/git-with-ssh/#3-upload-your-public-key-to-your-remote-repository-host","title":"3. Upload your Public key to your remote repository host","text":"<p>For GitHub, you do this (in 2024) by doing:</p> <ol> <li>Get the content of your SSH Public key:    <code>bash    cat ~/.ssh/id_ed25519.pub</code>    It will look something like:    <code>ssh-ed25519 Asdflkjhasdflkjhasdflkjhasdflkjhasdflkjhasdflkjh me@example.com</code></li> <li>Open https://github.com</li> <li>Go to your profile settings.</li> <li>Go to the \"SSH and GPG keys\" section.</li> <li>In the SSH keys section, click \"New SSH key\"</li> <li>Give a title to the key (could be anything, I recommend the name of your computer).</li> <li>For \"Key type\", choose \"Authentication Key\".</li> <li>In the \"Key\" field, paste in the content of your SSH Public key.</li> <li>Click \"Add SSH key\"</li> </ol>"},{"location":"blog/software-engineering/git-with-ssh/#4-configure-git-to-use-ssh","title":"4. Configure Git to use SSH","text":"<p>This is as simple as using the SSH version of the URL for your Git repo, rather than the HTTPS version of the URL. For example:</p> <pre><code># HTTPS:\nhttps://github.com/Harmelodic/website.git\n\n# SSH:\ngit@github.com:Harmelodic/website.git\n</code></pre> <p>For GitHub, you can add a remote using the SSH URL by doing:</p> <pre><code>git remote add origin git@github.com:Harmelodic/website.git\n</code></pre> <p>If you already have a remote repository configured, you can switch to using the SSH URL by doing:</p> <pre><code>git remote set-url origin git@github.com:Harmelodic/website.git\n</code></pre> <p>Now you can do <code>git push</code> and <code>git pull</code> (etc.) and Git will use SSH instead of HTTPS!</p>"},{"location":"blog/software-engineering/installing-an-apache-http-server/","title":"Installing an Apache HTTP Server","text":"<p>Originally published: 23 September 2016</p>"},{"location":"blog/software-engineering/installing-an-apache-http-server/#introduction","title":"Introduction","text":"<p>This guide is a guide to installing the Apache HTTP Server on a Debian based machine.</p>"},{"location":"blog/software-engineering/installing-an-apache-http-server/#open-a-terminal","title":"Open a terminal","text":"<p>Open the applications menu/app drawer and find the one called Terminal. OR use the shortcut: Ctrl+Shift T.</p>"},{"location":"blog/software-engineering/installing-an-apache-http-server/#installation-of-apache-http-server","title":"Installation of Apache HTTP Server","text":"<p>Type in the following command:</p> <pre><code>sudo apt-get install apache2 apache2-doc apache2-utils\n</code></pre> <p>The above command does not contain PHP support to the installation, to install with PHP support, do:</p> <pre><code>sudo apt-get install apache2 apache2-doc apache2-utils php5 libapache2-mod-php5 php5-mcrypt\n</code></pre>"},{"location":"blog/software-engineering/installing-an-apache-http-server/#installation-of-webapps","title":"Installation of Webapps","text":"<p>Simply put webapp files in the directory: <code>/var/www/html/</code>. Then use the following command to restart the Apache HTTP Server, thus making your Webapps available:</p> <pre><code>sudo service apache2 restart\n</code></pre>"},{"location":"blog/software-engineering/keeping-infrastructure-in-order/","title":"Keeping infrastructure in order","text":"<p>Originally published: 09 June 2024</p> <p>In this post, I talk about organising infrastructure code (e.g. Terraform code) to keep it highly maintainable and reusable, with references to my personal setup. There is a diagram at the end for visual reference.</p>"},{"location":"blog/software-engineering/keeping-infrastructure-in-order/#understanding-layers","title":"Understanding layers","text":"<p>When creating a platform in the modern day, we often find ourselves doing so through Infrastructure as code. For me, this has pretty much always been through writing Terraform code. As such, I'll be referring to Terraform a lot - however the principles I highlight here should be cross-language compatible.</p> <p>Like with all code, it's quickly becomes apparent that breaking up our infrastructure code to keep the code easy to maintain. You can initially do this with files &amp; directories but eventually trying to execute a huge repository of infrastructure code takes a very long time to run, which is painful - especially so, when you might have only made a very small change.</p> <p>Incidentally, organising Terraform code into directories and referencing them as Terraform Modules actually goes against the design of Terraform's module system - modules are designed to be reusable, where we can create instances of modules to create infrastructure, but sorting all your DNS-related infrastructure into a <code>dns</code> folder would result in a single <code>dns</code> module to create that infrastructure. This will lead to increased confusion and code-complexity.</p> <p>Eventually, you'll reach the point where you want to separate your infrastructure into separate Git repositories.</p> <p>With application code, this is usually quite easy to do since the application will be self-contained. Infrastructure code is a little different however. The infrastructure it creates is not self-contained - it depends upon some underlying infrastructure to exist. Infrastructure upon infrastructure, leads us to building infrastructure in layers, where each layer builds on the last. These layers are how we should split up our infrastructure code.</p> <p>If we structure our Terraform code correctly in these layers, then creating our infrastructure is extremely easy: Start with the repository creating the bottom layer, and move your way up. The best benefit of this is being able to reuse our infrastructure code to create new environments of whatever platform/infrastructure we're making.</p> <p>Rule to remember: Never depend on infrastructure that is created in a layer above, because it won't exist yet!</p>"},{"location":"blog/software-engineering/keeping-infrastructure-in-order/#the-bottom-layer-automation","title":"The bottom layer &amp; automation","text":"<p>If you're using Terraform, it's likely you're using a Cloud provider, or have some physical hardware with some software installed on the hardware that providers an API for a Terraform provider to hook into. This establishes absolute fundamentals that we can build on.</p> <p>First, I recommend creating a repository for code that will create the infrastructure required to automate creating infrastructure. It is a bad idea to run all infrastructure code manually, so establishing some initial \"automation infrastructure\" that can then handle the CI/CD of further infrastructure code is a powerful thing. Since I use GitHub Actions to run my infrastructure, my automation-infrastructure creates the absolute minimal infrastructure that is needed to enable GitHub Actions to execute Terraform code for me.</p> <p>Once this has been made, all infrastructure created can be executed automatically, rather than manually.</p>"},{"location":"blog/software-engineering/keeping-infrastructure-in-order/#understanding-platforms","title":"Understanding Platforms","text":"<p>Before we go build infrastructure to actually run the software we're developing, we need to think about what goes into developing software. There is:</p> <ul> <li>Software Engineers working with code, documentation, bug &amp; issue tracking.</li> <li>CI systems to build that code into software.</li> <li>A \"business-runtime\" platform to run that software.</li> <li>CD systems to deploy that software to the \"business-runtime\" platform.</li> </ul> <p>Each one of these aspects of developing software requires some sort of infrastructure. For some, we could buy some Software-as-a-Service (SaaS) (e.g. GitHub for storing code). For others, we may want to insert our infrastructure to either support existing software (e.g. our own servers for running GitHub Actions). For everything else, we can build infrastructure specifically for that purpose.</p> <p>Separating this infrastructure, means we can build infrastructure tailored for each of these aspects of developing software, whilst reducing the blast radius when issues occur. As such, each of these aspects of developing software results in a \"platform\" being built for that purpose (or we buy it). At a small scale, such as a individual or a startup, these concerns are less important, often resulting in these platforms being merged into one big platform (plus purchasing SaaS products).</p> <p>Ultimately, this informs us of the next layer of repositories that we need to make. The initial, pre-environment infrastructure needed for each of these platforms. This is usually some sort of space defined in your Cloud setup for the platform and a place to store software artifacts for that platform that can be deployed to different environments.</p> <p>You can find an example of this in my personal setup with my personal-initial-infrastructure for creating infrastructure for my Personal platform.</p>"},{"location":"blog/software-engineering/keeping-infrastructure-in-order/#environments","title":"Environments","text":"<p>Before we create further infrastructure for each platform, we probably want to have multiple environments in each platform. A \"production\" (or \"prod\") environment where users &amp; systems interact with the released software, and other \" production-like\" environments for development needs (playgrounds, manual testing, etc.).</p> <p>These environments must be effectively identical to one another, for them to have any value as \"production-like\" environments. Only small differences like the name of the environment, size of servers and basic IAM/Firewall restrictions should be allowed as differences between the environments.</p> <p>Up until now, the infrastructure code we've written has been very straightforward: either the infrastructure exists or it doesn't. In order to support environments, we need to introduce another dimension into our infrastructure code: Templating. Templating allows us to construct our infrastructure code with variables in it. When we want to create infrastructure for a particular environment, we simply fill the variables with the appropriate values, and execute the infrastructure code.</p> <p>In Terraform, we can do with Terraform Workspaces and variables. I recommend having a 1-1 mapping between Terraform Workspaces and Environments, then using variables to define aspects of the envionment allowed to be configured differently in each environment.</p> <p>Rule to remember: All infrastructure code that will create infrastructure in a platform environment MUST be written in a \"templated\" way. This is to ensure that creating environments actually works.</p> <p>For small platforms, we can often be successful with just a single repository covering all platform infrastructure in an environment.</p> <p>For larger platforms, it is likely that breaking up infrastructure into further layers within an environment becomes useful, where each layer in the environment has it's own repository.</p> <p>You can find an simple example of a how a platform environment repository looks and works by looking at my personal-env-infrastructure repository.</p>"},{"location":"blog/software-engineering/keeping-infrastructure-in-order/#further-layers-in-larger-platforms","title":"Further layers in larger platforms","text":"<p>As mentioned above, larger platforms usually end up needing further breaking up of infrastructure code to keep the code maintainable.</p> <p>I don't have this in my personal setup, but have built these professionally. The following is an example structure of how you might split the layers/repositories:</p> <ul> <li><code>{platform}-management</code> for creating projects &amp; project-level IAM permissions.</li> <li><code>{platform}-networking</code> for creating DNS and networking infrastructure inside projects.</li> <li><code>{platform}-kubernetes</code> for creating a central Kubernetes cluster for your platform (assuming you only need 1 cluster   per environment).</li> <li><code>{platform}-service-accounts</code> for creating all the service accounts required for the applications that you plan on   deploying (do this centrally aids in governance &amp; issues with service account name uniqueness).</li> <li>and several deployment repositories (see below).</li> </ul> <p>In my personal setup, I don't have these splits, as it is a very small scale setup.</p> <p>*By \"project\", I mean GCP Project. Other clouds have their equivalent resource management concept.</p>"},{"location":"blog/software-engineering/keeping-infrastructure-in-order/#deployment-repositories","title":"Deployment repositories","text":"<p>As the industry continues to embrace GitOps, we are finding ourselves deploying software by building a new revision of the software, and then updating code to deploy that revision.</p> <p>In order to provide separation between logical groups of deployed software, we can have a dedicated deployment repository, for each logical grouping, that contains the code that defines how all the software should be deployed.</p> <p>This software that is being deployed, will likely depend on some infrastructure being in place (e.g. Databases, Storage Buckets, Message Busses, etc.). As such we could bundle the infrastructure code needed by that software, with the deployment code for the software into a single Deployment Repository.</p> <p>You can see an example of a Deployment Repository if you look at my personal-apps repository. However, note that Argo CD (a piece of CD software) is configured in this repository. Deploying your CD software to the same platform as your business-runtime applications has trade-offs, and many folks may find having a separate CD platform as more preferable.</p> <p>Note: for the \"business-runtime\" platform mentioned above that will run the product software for an organisation, I recommend doing some DDD work to figure out the bounded-contexts and subdomains, and organise these Deployment Repositories based on those results.</p>"},{"location":"blog/software-engineering/keeping-infrastructure-in-order/#diagram","title":"Diagram","text":"<p>Hope this helped!</p> <p>~ Harmelodic</p>"},{"location":"blog/software-engineering/logging-in-java-projects/","title":"Logging in Java projects","text":"<p>\"Logging in Java\" is quite the rabbit hole, that I've been down many times, and whilst the documentation &amp; knowledge is available, it can be a little hard to find.</p> <p>From my experience so far, here's how it works (as of 2024):</p>"},{"location":"blog/software-engineering/logging-in-java-projects/#logging-considerations","title":"Logging Considerations","text":"<p>Before you go write <code>System.out.println</code> or directly using the Java Logging API and shouting \"I'm done!\", there's a bunch of things to consider when you want to create a log message:</p> <ul> <li>What information do you want with your log message?<ul> <li>Timestamp?</li> <li>Class that logged the message?</li> <li>Thread the log was on?</li> <li>Logging level?</li> <li>Extra data about what was happening at the time?</li> <li>Custom labels/markers?</li> <li>etc.</li> </ul> </li> <li>How do you want that log to be formatted?<ul> <li>Plain string?</li> <li>Custom format?</li> <li>JSON object?</li> <li>XML object?</li> </ul> </li> <li>Where do you want the log to be sent?<ul> <li>Console output?</li> <li>A file?</li> <li>Directly into some logging system?</li> </ul> </li> <li>What about when you want to change your method of logging?<ul> <li>Another Log4Shell might come along and make your project vulnerable.</li> <li>How can we minimise changing the code a bunch and have logging be more \"plug &amp; play\"?</li> </ul> </li> </ul> <p>I'll be tackling all these points!</p>"},{"location":"blog/software-engineering/logging-in-java-projects/#logging-interface","title":"Logging Interface","text":"<p>Let's take the question of \"What about when you want to change your method of logging?\". The obvious solution to this is to have some sort of interface/bridge between our Java project code and the method of logging.</p> <p>There are two on offer:</p> <ul> <li> <p>Simple Logging Facade for Java (aka SLF4J) is a simple   logging facade for use in Java projects. It's built/maintained   by QOS.ch Sarl, a Swiss software company.</p> </li> <li> <p>Commons Logging is a adapter for bridging your logging to another   logging system. It's built/maintained by the Apache Software Foundation.</p> </li> </ul> <p>Which should you use? Simple answer: SLF4J.</p> <p>Why?</p> <ol> <li>SLF4J gives a decent reason in their FAQ.</li> <li>The SLF4J/QOS.ch developers expanded on    the issues with Commons Logging in 2009.</li> <li>I've been doing development for 10 years now, and never seen Commons Logging in real usage that wasn't a legacy code    base. So my experience/observations tell me that the modern solution is SLF4J.</li> <li>As of writing, there    are 518K references to the slf4j-api dependency    on GitHub, and    only 171K references to the commons-logging dependency -    which, whilst it is a slightly flawed metric, at least implies that over 3x more projects are using SFL4J.</li> </ol> <p>SLF4J code looks a bit like the following:</p> <pre><code>package com.harmelodic;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.slf4j.MarkerFactory;\n\npublic class Application {\n    private static final Logger logger = LoggerFactory.getLogger(Application.class);\n\n    public static void main(String[] args) {\n        logger.atInfo()\n                .addKeyValue(\"args\", args)\n                .addMarker(MarkerFactory.getMarker(\"BOOT\"))\n                .log(\"Application started.\");\n        logger.info(\"A quick message\");\n        // ...\n    }\n}\n</code></pre>"},{"location":"blog/software-engineering/logging-in-java-projects/#logging-implementation","title":"Logging implementation","text":"<p>Now that we have our interface to log stuff to (SLF4J), we need some implementation underneath that those logs and actually log things!</p> <p>Here are/were the options:</p> <ul> <li> <p>Log4j 1.x is end of life. It was maintained by   the Apache Software Foundation.</p> </li> <li> <p>Log4j2 is the newer Log4j, built / maintained by   the Apache Software Foundation. It is/was used widely... except it had   the Log4Shell vulnerability and now I wouldn't be surprised if no developer   or organisation would touch it with a barge pole. Maybe it'll make a reprise... but I doubt it.</p> </li> <li> <p>Logback is another \"new Log4j\", and has a lot of configurability to get logs just the way   you want them. It's also maintained by the same QOS.ch folks who maintain SLF4J... which is a nice   indicator. Spring Boot currently defaults to   using Logback.</p> </li> <li> <p>The Java Logging API (aka JUL)   is a logging API built into Java, but... there's a reason why the logging libraries exist. JUL is a simple logging   system, and if you want neat things like structured logging, filtering, or automatic log rotation, then you need a   logging library.</p> </li> </ul> <p>For simple things, JUL can work. For everything else: Logback.</p>"},{"location":"blog/software-engineering/logging-in-java-projects/#logback","title":"Logback","text":"<p>This section is a very brief overview of the basic configuration of Logback. More information and advanced configuration options can be read about in the Logback manual.</p> <p>View the Logback artifacts on Maven Repository. I recommend grabbing <code>logback-classic</code>, since that's the Logback dependency that hooks into SLF4J.</p>"},{"location":"blog/software-engineering/logging-in-java-projects/#logback-configuration","title":"Logback Configuration","text":"<p>Basic Logback configuration uses the following components:</p> <ul> <li> <p>Appenders - the things that say where the logs will be placed (Console, File, etc.)</p> </li> <li> <p>Encoders &amp; Layouts - the things that take the Log Events from Logback, and prepares them for writing.</p> <ul> <li>Encoders do that by transforming a Log Event into a byte array as well as writing out that byte array into an   <code>OutputStream</code>.</li> <li>Layouts just transform the Log Event into a String.</li> <li>Note: Often you can find yourself putting a Layout inside a <code>LayoutWrappingEncoder</code> Encoder - This is fine and   normal.</li> </ul> </li> <li> <p>Patterns / Formatters are the configuration inside of Encoders and Layouts that informs that specific Encoder or   Layout on how to format the log when it is written.</p> </li> </ul> <p>A typical simple Logback configuration file to output plain string logs to the console might look like:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n&lt;configuration&gt;\n    &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;encoder&gt;\n            &lt;!-- Pattern syntax: https://logback.qos.ch/manual/layouts.html#conversionWord --&gt;\n            &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg %kvp%n&lt;/pattern&gt;\n        &lt;/encoder&gt;\n    &lt;/appender&gt;\n\n    &lt;root level=\"info\"&gt;\n        &lt;appender-ref ref=\"STDOUT\"/&gt;\n    &lt;/root&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"blog/software-engineering/logging-in-java-projects/#structured-logging-with-logback","title":"Structured Logging with Logback","text":"<p>When developing systems where logs are collected into a central system for querying &amp; analysis, it is extremely useful to implement \"structured logging\" - which in 2024 basically means that you output your logs into a consistent JSON format.</p> <p>To do this with Logback, you have a few choices (from my knowledge):</p> <ul> <li> <p>The <code>logback-json-classic</code> and <code>logback-jackson</code> libraries are old \"Logback Contrib\" libraries available for   Logback. \"Logback Contrib\" is the name for the old, official Logback open-source contributions community. The   libraries are versioned to <code>0.1.5</code>, only ever had 5 versions, and was last updated in 2016, at time of writing.   Probably not a good idea to use these, but they work, and are referenced a lot in guides for how to do JSON logging   with Logback... so, clearly people like them.</p> </li> <li> <p>The logstash-logback-encoder library was initially built to   put logs in the JSON format used by Elastic's Logstash   system, but is now a more generic structured logging library. It gets semi-regular updates, and works pretty well!</p> </li> <li> <p>Cloud Providers also can offer their own Logback implementations. For example,   the google-cloud-logging-logback   and spring-cloud-gcp-logging   libraries are built by Google and contain a bunch of code to support logging both directly and indirectly to the   Google Cloud Logging API (previously known as Stackdriver).   Since it's all just implementations of Logback interfaces, you could use the <code>StackdriverJsonLayout</code> Layout with a   regular <code>ConsoleAppender</code> to get structured JSON logs to your console.   Incidentally, Google's Logback library has the added benefit of being able to use and write your own   <code>LoggingEventEnhancer</code>s, which allow you to add custom key-values into the JSON log output.</p> </li> </ul> <p>Thanks for reading!</p> <p>~ Harmelodic \u2764\ufe0f</p>"},{"location":"blog/software-engineering/moving-from-docker-to-podman/","title":"Moving from Docker to Podman","text":""},{"location":"blog/software-engineering/moving-from-docker-to-podman/#background","title":"Background","text":"<p>Back on 31st August 2021, Docker announced a change for Docker Desktop, where businesses with more than 250 employees or more than $10 million annual revenue, would have to start a paid subscription with Docker if they wanted to use Docker Desktop.</p> <p>Since businesses don't like to pay for things, I probably won't be able to use Docker Desktop for work anymore, which is where most of my development work happens. Also, I don't want different tooling for work and personal stuff, so I'll change there.</p> <p>I also write a lot of stuff in Java and like using the Testcontainers library when I need to write integration tests that require a Test Double to be in place (Databases, Event Busses, etc.), and Testcontainers requires Docker. In November 2021, I opened a GitHub issue with Testcontainers (Java) to ask if they could remove Docker as a requirement. At time of writing Testcontainers haven't changed, AFAIK.</p> <p>So to switch over from Docker to something else, my requirements were:</p> <ul> <li>Support macOS.</li> <li>Has to have a decent CLI, ideally like the Docker one.</li> <li>Has to support Testcontainers.</li> <li>Be free &amp; open-source.</li> <li>Be well supported/maintained.</li> <li>Be pretty easy to setup &amp; configure.</li> <li>I don't care about a GUI.</li> </ul> <p>The options came down to Rancher Desktop, Podman or Colima.</p> <p>I tried Rancher Desktop first, as it initially seemed like an easy hot-replacement for Docker Desktop, but then I was met with configuration issues, socket issues, and macOS start-up issues.</p> <p>I had tried Colima a while back, but the CLI was a bit fiddly, and I know a few other folks have tried to use it and had configuration &amp; socket issues.</p> <p>I tried Podman, and it's been seamless... so, Podman it is!</p>"},{"location":"blog/software-engineering/moving-from-docker-to-podman/#migrating","title":"Migrating","text":"<ol> <li>Uninstall Docker &amp; Docker Desktop, completely.</li> <li>Install Podman by running:    <code>bash    brew install podman</code></li> <li>Since Podman requires a VM for macOS, we need download and run the Podman VM by running:    <code>bash    podman machine init</code></li> <li>To get the Docker socket aliasing required for Testcontainers, run:    <code>bash    sudo podman-mac-helper install</code></li> <li>Now, start the Podman VM by running:    <code>bash    podman machine start</code></li> <li>Verify Podman socket aliasing is configured:    <code>bash    ls -la /var/run/docker.sock</code>    The output points to a podman.sock file such as:    <code>/var/run/docker.sock -&gt; /Users/username/.local/share/containers/podman/machine/podman.sock</code></li> <li>Verify Podman is running:    <code>bash    podman images</code></li> </ol> <p>and that's it.</p> <p>If you restart your machine, then you need to start the Podman VM again by running:</p> <pre><code>podman machine start\n</code></pre> <p>If you need to run a CLI command, use <code>podman</code> instead of <code>docker</code>. Otherwise the CLI is pretty much the same.</p> <p>e.g. <code>podman images</code> will list your current container images. e.g. <code>podman ps</code> will list your running containers.</p>"},{"location":"blog/software-engineering/moving-from-docker-to-podman/#references","title":"References","text":"<ul> <li>Podman Docs: https://docs.podman.io/en/latest/</li> <li>Podman Mac Helper doc: https://podman-desktop.io/docs/migrating-from-docker/using-podman-mac-helper</li> <li>Podman CLI commands: https://docs.podman.io/en/latest/Commands.html</li> </ul>"},{"location":"blog/software-engineering/moving-from-docker-to-podman/#linux","title":"Linux","text":"<p>I've been using Linux distros a fair bit more recently, specifically Ubuntu. If you need to run containers, podman is available there too (though since containers are native to Linux, you don't need to mess about with any podman virtual machine stuff). Just install podman with:</p> <pre><code>sudo apt install podman\n</code></pre> <p>Or if you're using a different distro, whatever package manager is the one you use for your distro.</p> <p>Happy container-ing!</p> <p>~ Harmelodic</p>"},{"location":"blog/software-engineering/moving-gpg-keys-to-a-new-machine/","title":"Moving GPG keys to a new machine","text":"<p>Sometimes, you get a new machine and you need to move your GPG key to that new computer. This is how to do it.</p> <p>GPG keys come in two parts:</p> <ul> <li>Public Keys</li> <li>Private Keys (or \"Secret\" keys).</li> </ul> <p>When exporting GPG keys from GPG, the exported \"Secret\" key file contains both the Public and Private Key, so we only need to move one file!</p> <p>In order to move them, we need to do the following steps:</p> <ol> <li>Get the Key name</li> <li>Export to a secret key file</li> <li>Move the file to your new computer</li> <li>Import the key pair from the file</li> <li>Trust the key pair on your new computer.</li> </ol>"},{"location":"blog/software-engineering/moving-gpg-keys-to-a-new-machine/#get-the-key-name","title":"Get the key name","text":"<pre><code>gpg --list-secret-keys\n</code></pre> <p>You'll get an output that looks like:</p> <pre><code>/home/matt/.gnupg/pubring.kbx\n-----------------------------\nsec   ed25519 2024-08-28 [SC]\n      593E00FDAAE3C390075AB63948E2FABDE0B3B2DE\nuid           [ultimate] Test &lt;test@example.com&gt;\nssb   cv25519 2024-08-28 [E]\n</code></pre> <p>That <code>593E00FDAAE3C390075AB63948E2FABDE0B3B2DE</code>-looking thing is your key name.</p>"},{"location":"blog/software-engineering/moving-gpg-keys-to-a-new-machine/#export-to-a-secret-key-file","title":"Export to a secret key file","text":"<p>We're going to export to an ASCII armored file:</p> <pre><code>gpg --armor --export-secret-key &lt;key_name&gt; &gt; secret.asc\n\n# e.g.\ngpg --armor --export-secret-key \"593E00FDAAE3C390075AB63948E2FABDE0B3B2DE\" &gt; secret.asc\n</code></pre> <p>This creates a <code>secret.asc</code> file containing your GPG public-private keypair.</p>"},{"location":"blog/software-engineering/moving-gpg-keys-to-a-new-machine/#move-the-file-to-your-new-computer","title":"Move the file to your new computer","text":"<p>USB stick?</p> <p>Ideally, something \"offline\" / not via the internet.</p>"},{"location":"blog/software-engineering/moving-gpg-keys-to-a-new-machine/#import-the-key-pair-from-the-file","title":"Import the key pair from the file","text":"<p>Once you've got the file on your new computer, run:</p> <pre><code>gpg --import secret.asc\n</code></pre>"},{"location":"blog/software-engineering/moving-gpg-keys-to-a-new-machine/#trust-the-key-pair-on-your-new-computer","title":"Trust the key pair on your new computer","text":"<p>To trust the key pair, we need to edit the key to open a new gpg key management prompt for the key.</p> <pre><code>gpg --edit-key &lt;key_name&gt;\n# e.g. \ngpg --edit-key \"593E00FDAAE3C390075AB63948E2FABDE0B3B2DE\"\n</code></pre> <p>Using the gpg prompt, trust the key:</p> <pre><code>gpg&gt; trust\n</code></pre> <p>This will prompt you as follows:</p> <pre><code>Please decide how far you trust this user to correctly verify other users' keys\n(by looking at passports, checking fingerprints from different sources, etc.)\n\n  1 = I don't know or won't say\n  2 = I do NOT trust\n  3 = I trust marginally\n  4 = I trust fully\n  5 = I trust ultimately\n  m = back to the main menu\n\nYour decision?\n</code></pre> <p>assuming it's your GPG key, select <code>5</code> for ultimate trust, confirm with <code>y</code>.</p> <p>When you are returned to the gpg prompt, simply do:</p> <pre><code>gpg&gt; save\n</code></pre> <p>And you're done! Enjoying using your GPG keys on your new machine.</p> <p>You can see that both Public and Private keys exist and are trusted, by running the following commands:</p> <pre><code>gpg --list-keys\ngpg --list-secret-keys\n</code></pre> <p>~ Harmelodic / Matt Smith</p>"},{"location":"blog/software-engineering/my-personal-ci-cd-setup-2024/","title":"My personal CI/CD setup (2024)","text":"<p>Originally published: 18 May 2024</p> <p>Most software I make for myself is web stuff:</p> <ul> <li>\"Frontend\" web apps (with JavaScript, npm, React etc.)</li> <li>\"Backend\" services (with Java, Maven, Spring, etc.)</li> <li>Cloud infrastructure (with GCP, Terraform, etc.)</li> </ul> <p>But I've also got some little hobby projects here and there that aren't that.</p> <p>I build this stuff (CI) in GitHub Actions using the respective build tool for each ecosystem I interact with:</p> <ul> <li>For Java, I use Maven with JIB to make container images.</li> <li>For JavaScript, I use npm with a Dockerfile to make container images.</li> <li>For Terraform... you don't build it, just validate &amp; deploy it using <code>terraform</code> commands.</li> </ul> <p>Some times this interacts with my Pact Broker, if I've written contract testing.</p> <p>and I deploy this stuff (CD) with:</p> <ul> <li>GitHub Actions, to Terraform code against GCP.</li> <li>Argo CD, for containers deploying to Kubernetes.</li> </ul> <p>I have an Argo CD instance running, which picks up changes in GitHub and deploys them. I'm lazy right now and just set the container version to <code>latest</code>, but I could set it up to just version the containers (probably with commit SHAs) if I wanted to do things more \"properly\".</p> <p>I could run an Atlantis/Scalr/TFC deployment system for my Terraform, but... I really don't need that - running some terraform commands in a GitHub Action works just fine, and my GitHub Actions are configured to use Workload Identity, so I'm not passing any Service Account Keys around.</p> <p>I've got some improvements to makes, but honestly this works pretty well, and reflects a very basic version of what I've used in professional scenarios, so it keeps me pretty sharp with \"how things are done\".</p>"},{"location":"blog/software-engineering/onboarding-checklist-guide/","title":"Onboarding Checklist Guide","text":"<p>Originally published: 26 November 2019</p> <ul> <li>ALM / SDLC Tools<ul> <li>VCS</li> <li>CI/CD</li> <li>Documentation</li> <li>Ticket Management</li> </ul> </li> <li>Mission<ul> <li>Original Problem</li> <li>Proposed Solution</li> <li>How far are we along?</li> </ul> </li> <li>Side Quests<ul> <li>What are we hoping to demonstrate/prove in the process?</li> </ul> </li> <li>Project Management System<ul> <li>Agile / Waterfall</li> <li>SCRUM / Kanban</li> <li>SAFe / LeSS / DaD / None</li> </ul> </li> <li>Project Development Structure<ul> <li>Gathering Requirements</li> <li>Requirements Analysis</li> <li>Modelling</li> <li>Architecture &amp; Design</li> <li>Technology Decisions</li> <li>Networking</li> <li>System Topology</li> <li>Proof of Concept</li> <li>Subsequent Project Planning</li> <li>Development</li> <li>Adoption</li> </ul> </li> <li>Development &amp; Ownership<ul> <li>Who is responsible for all of the above?</li> <li>What is automated?</li> </ul> </li> <li>Project Team<ul> <li>Team Members</li> <li>Team Lead</li> </ul> </li> <li>Management<ul> <li>Programme Lead</li> <li>Others &amp; Externals</li> </ul> </li> <li>3rd Party Systems<ul> <li>Who do we connect to?</li> <li>Why do we connect to them?</li> <li>How do we connect to them? (Protocol)</li> </ul> </li> <li>First steps<ul> <li>What tasks need doing that I can do?</li> <li>What bits are questionable? (All of it :D)</li> </ul> </li> </ul>"},{"location":"blog/software-engineering/principles-for-managing-npm-packages/","title":"Principles for managing npm packages","text":"<p>Originally published: 26 November 2018</p>"},{"location":"blog/software-engineering/principles-for-managing-npm-packages/#why-do-we-need-them","title":"Why do we need them?","text":"<p>When developing a frontend or a Node.js application, it is highly likely (and recommended) that developers use npm to manage their dependencies.</p> <p>However, npm has a lot of...risk.</p> <p>Unlike communities around other languages where established frameworks, libraries and best practices have been put into place and a project's dependency tree is relatively small, the JavaScript community has been quite the opposite.</p> <p>Since around 2009/2010 (when Node.js &amp; npm were launched), including lots packages in JavaScript projects has become increasingly popular, given how easy it is to write JavaScript and how easy npm makes it to publish packages for public use.</p> <p>Some advantages to this have been:</p> <ul> <li>An increase in code sharing and reuse.</li> <li>Public access to libraries &amp; frameworks written by more skilled/knowledgable developers.</li> </ul> <p>However, there are plenty of disadvantages:</p> <ul> <li>An increase in instabilty of projects using unstable or out of date packages.</li> <li>Packages being used by unknown, untrusted individuals.</li> <li>Inclusion of code that may not be tested or follow established standards/patterns.</li> <li>Large amounts of redundant code included in artifact/bundle.</li> <li>Risk of unsecure, malicious packages being included and hidden in a large dependency tree.</li> </ul> <p>These instablity and security concerns have been publicly voiced and:</p> <ul> <li> <p>How one developer just broke Node, Babel and thousands of projects in 11 lines of JavaScript</p> </li> <li> <p>I\u2019m harvesting credit card numbers and passwords from your site. Here\u2019s how.</p> </li> <li> <p>A package owner transferred ownership of a relatively popular package to a hacker</p> </li> </ul> <p>We need principles to follow.</p> <p>I've written some:</p>"},{"location":"blog/software-engineering/principles-for-managing-npm-packages/#the-principles","title":"The Principles","text":"<ol> <li> <p>Only use mature packages.    (i.e. has clear, easily discoverable documentation and has at least one stable major version - e.g. <code>1.0.0</code>).</p> </li> <li> <p>Don't install large packages when you're only going to use a single piece of functionality from it.    If you need more functionality later, you can refactor.</p> </li> <li> <p>Don't blindly follow trends, only use packages you need.</p> </li> <li> <p>When you're about to install a new package, ask yourself the question: How long would it take me to write and test the functionality I need out of this package?</p> <ul> <li>If the answer is: Not very long, then don't install the package and just write it yourself.</li> <li>If the answer is: A while, then consider writing it yourself, if you have time.</li> <li>If the answer is: A long time, then it's likely you will require it (or a similar package).</li> </ul> </li> <li> <p>Ensure packages are maintained.    (Rule of thumb: Has had a release in the past 2 months).</p> </li> <li> <p>Ensure packages are well-tested.    (Ideally, have a test coverage above 75%)</p> </li> <li> <p>Implement package testing in your testing CI/CD pipeline.    These tests should include:</p> <ul> <li>The build doesn't break with new &amp; upgraded packages.</li> <li>Passing an <code>npm audit</code> (Ideally should find 0 vulnerabilities).</li> <li>Complies with all of the above principles.</li> </ul> </li> </ol>"},{"location":"blog/software-engineering/principles-for-managing-npm-packages/#the-why","title":"The Why","text":"<ol> <li> <p>Immature packages carry with them:</p> <ul> <li>Instability.</li> <li>Unknown security vulnerabilities.</li> <li>Lack of support/documentation to refer to when attempting to fix issues.</li> </ul> </li> <li> <p>Including large packages for small pieces of functionality introduces a lot of redundant code into your final    bundle/artifact.    Code-splitting can remedy this partially. However, engineers should be skilled enough to write single pieces of    functionality when needed.    If they are not, then this provides a good learning opportunity for them (or the task can be handed off to a more    experienced developer).</p> </li> <li> <p>Engineers should think about what is required when developing a solution.    Following trends for the sake of following trends can be rewarding from a marketting perspective, but can introduce    inefficiencies, new unsolved problems and messy code.</p> </li> <li> <p>Engineers should actively avoid situations like    the the left-pad incident.    If a piece of code can be written easily and quickly, then there is little harm in writing, for the sake of stability    and security.    Implementing an extreme level of code-reuse can lead to a volatile project, where you're attempting to manage    packages (things out of your control) more than you are managing your own code (things in your control).</p> </li> <li> <p>Unmaintained packages lead to:</p> <ul> <li>Code being included in your bundle that could eventually contain security flaws that are never fixed.</li> <li>Locking your project into a unsustainable architecture that is difficult to refactor out of.</li> <li>Incidents similar to the event-stream issue</li> </ul> </li> <li> <p>No one knows whether something works unless tests have been written for it.</p> </li> <li> <p>Continuously testing your dependencies for stability &amp; security issues leads to a higher likelihood of security &amp;    stability in your project.</p> </li> </ol>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/","title":"RestClient vs. WebClient vs. RestTemplate (Configuring)","text":"<p>There are 3 main HTTP clients available from the Spring Framework: RestClient, WebClient and RestTemplate. This blog posts aims to cover a comparison of the three from a perspective of a developer who wants to use and configure them.</p>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#overview","title":"Overview","text":"Header RestClient WebClient RestTemplate Available since Spring 6.1 Spring 5.0 Spring 3.0 Ecosystem Web / Synchronous Reactive / non-blocking Web / Synchronous My perspective? Recommended for most applications Useful for reactive applications Legacy <ul> <li>RestClient   Javadoc: https://javadoc.io/doc/org.springframework/spring-web/latest/org/springframework/web/client/RestClient.html</li> <li>WebClient   Javadoc: https://javadoc.io/doc/org.springframework/spring-webflux/latest/org/springframework/web/reactive/function/client/WebClient.html</li> <li>RestTemplate   Javadoc: https://javadoc.io/doc/org.springframework/spring-web/latest/org/springframework/web/client/RestTemplate.html</li> </ul> <p>Unless otherwise configured, when used in a Spring Boot context, these clients will automatically encode and decode Java classes into and from JSON structures (using Jackson).</p>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#getting-a-client-with-dependency-injection","title":"Getting a client with Dependency Injection","text":"<p>You could instantiate a client inside your class, but it's much more likely (and better for testing) to perform dependency injection to get a client (Spring Boot starters will even autoconfigure a Spring Bean for you):</p> <pre><code>import org.springframework.stereotype.Component;\nimport org.springframework.web.client.RestClient;\n\n@Component\nclass ExampleClient {\n    private final RestClient restClient;\n\n    ExampleClient(RestClient.Builder restClientBuilder, String baseUrl) {\n        this.restClient = restClientBuilder\n                .baseUrl(baseUrl) // e.g. https://api.example.com\n                .build();\n    }\n}\n</code></pre> <pre><code>import org.springframework.stereotype.Component;\nimport org.springframework.web.reactive.function.client.WebClient;\n\n@Component\nclass ExampleClient {\n    private final WebClient webClient;\n\n    ExampleClient(WebClient.Builder webClientBuilder, String baseUrl) {\n        this.webClient = webClientBuilder\n                .baseUrl(baseUrl) // e.g. https://api.example.com\n                .build();\n    }\n}\n</code></pre> <pre><code>import org.springframework.stereotype.Component;\nimport org.springframework.web.client.RestTemplate;\n\n@Component\nclass ExampleClient {\n    private final RestTemplate restTemplate;\n    private final String HOST; // e.g. https://api.example.com\n\n    ExampleClient(RestTemplate restTemplate, String host) {\n        this.restTemplate = restTemplate;\n        this.HOST = host;\n    }\n}\n</code></pre>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#calling-to-get-a-resource","title":"Calling to GET a resource","text":"<p>If you're writing something quick &amp; dirty and not error handling response codes or exceptions, handling data is trivial with all three clients:</p> <pre><code>import org.springframework.stereotype.Component;\nimport org.springframework.web.client.RestClient;\nimport org.springframework.web.client.RestTemplate;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport reactor.core.publisher.Mono;\n\n@Component\nclass ExampleClient {\n    private final RestClient restClient;\n    private final WebClient webClient;\n    private final RestTemplate restTemplate;\n    private final String REST_TEMPLATE_HOST;\n\n    Thing fetchThingWithRestClient(String thingId) {\n        return restClient.get()\n                .uri(uriBuilder -&gt; uriBuilder\n                        .path(\"/thing/{id}\")\n                        .build(thingId))\n                .retrieve()\n                .body(Thing.class);\n    }\n\n    // Mono&lt;&gt; is a reactive publisher that could contain 0 or 1 thing,\n    // kind of like Optional&lt;&gt; but reactive. \n    Mono&lt;Thing&gt; fetchThingWithWebClient(String thingId) {\n        return webClient.get()\n                .uri(uriBuilder -&gt; uriBuilder\n                        .path(\"/thing/{id}\")\n                        .build(thingId))\n                .retrieve()\n                .bodyToMono(Thing.class);\n    }\n\n    Thing fetchThingWithRestTemplate(String thingId) {\n        // getForObject() is a handy shorthand method for very simple HTTP calls\n        // exchange().is more widely used for it's better flexibility\n        return restTemplate.getForObject(\n                UriComponentsBuilder.fromUri(URI.create(restTemplateBase))\n                        .path(\"/thing/{id}\")\n                        .build(thingId)\n                        .toString(),\n                Thing.class\n        );\n    }\n\n    record Thing(String id, String name) {\n    }\n}\n</code></pre>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#calling-to-get-a-list-of-resources-with-a-query-parameter","title":"Calling to GET a list of resources, with a query parameter","text":"<p>If you're writing something quick &amp; dirty and not handling error response codes, but wanting to do simple things like:</p> <ul> <li>Deal with a list of objects rather than a single object</li> <li>Add a query parameter to your call.</li> </ul> <p>then you'll find RestClient &amp; WebClient easy to modify, but RestTemplate requires a change in method.</p> <pre><code>import org.springframework.stereotype.Component;\nimport org.springframework.web.client.RestClient;\nimport org.springframework.web.client.RestTemplate;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport reactor.core.publisher.Flux;\nimport reactor.core.publisher.Mono;\n\n@Component\nclass ExampleClient {\n    private final RestClient restClient;\n    private final WebClient webClient;\n    private final RestTemplate restTemplate;\n    private final String REST_TEMPLATE_HOST;\n\n    List&lt;Thing&gt; fetchThingWithRestClient(String thingId) {\n        return restClient.get()\n                .uri(uriBuilder -&gt; uriBuilder\n                        .path(\"/things\")\n                        .queryParam(\"nameBeginsWith\", \"a\")\n                        .build())\n                .retrieve()\n                .body(new ParameterizedTypeReference&lt;&gt;() {\n                });\n    }\n\n    // Flux&lt;&gt; is a reactive publisher that could contain N items,\n    // kind of like List&lt;&gt; but reactive.\n    Flux&lt;Thing&gt; fetchThingWithWebClient(String beginWith) {\n        return webClient.get()\n                .uri(uriBuilder -&gt; uriBuilder\n                        .path(\"/things\")\n                        .queryParam(\"nameBeginsWith\", \"a\")\n                        .build())\n                .retrieve()\n                .bodyToFlux(Thing.class);\n    }\n\n    Thing fetchThingWithRestTemplate(String thingId) {\n        // Note how changing we need to move to the more flexible exchange() method\n        return restTemplate.exchange(\n                RequestEntity.get(UriComponentsBuilder\n                                .fromUri(URI.create(restTemplateBase))\n                                .path(\"/things\")\n                                .queryParam(\"nameBeginsWith\", \"a\")\n                                .encode()\n                                .toUriString())\n                        .build(),\n                new ParameterizedTypeReference&lt;List&lt;Thing&gt;&gt;() {\n                }\n        ).getBody();\n    }\n\n    record Thing(String id, String name) {\n    }\n}\n</code></pre>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#calling-to-postputetc-some-data","title":"Calling to POST/PUT/etc. some data","text":"<p>If you're wanting to send data, RestClient and WebClient is again easy to modify, with RestTemplate it's a little more involved, but provided you're used to the <code>exchange()</code> method, not too complicated:</p> <pre><code>import org.springframework.http.RequestEntity;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.client.RestClient;\nimport org.springframework.web.client.RestTemplate;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport reactor.core.publisher.Mono;\n\n@Component\nclass ExampleClient {\n    private final RestClient restClient;\n    private final WebClient webClient;\n    private final RestTemplate restTemplate;\n    private final String REST_TEMPLATE_HOST;\n\n    Thing sendThingWithRestClient(Thing thing) {\n        return restClient.post()\n                .uri(\"/thing\")\n                .body(thing) // Sending in the request body\n                .retrieve()\n                .body(Thing.class); // Assuming a Thing is returned in the response body\n    }\n\n    // Mono&lt;&gt; is a reactive publisher that could contain 0 or 1 thing,\n    // kind of like Optional&lt;&gt; but reactive.\n    Mono&lt;Thing&gt; sendThingWithWebClient(Thing thing, Mono&lt;Thing&gt; monoThing) {\n        // Either provide a value as the request body:\n        return webClient.post()\n                .uri(\"/thing\")\n                .body(BodyInserters.fromValue(thing))\n                .retrieve()\n                .bodyToMono(Thing.class);\n\n        // or provide a Mono as the request body: \n        return webClient.post()\n                .uri(\"/thing\")\n                .body(monoThing, Thing.class)\n                .retrieve()\n                .bodyToMono(Thing.class);\n    }\n\n    Thing sendThingWithRestTemplate(Thing thing) {\n        // Continuing usage with the more flexible exchange() method\n        return restTemplate.exchange(\n                RequestEntity.post(UriComponentsBuilder\n                                .fromUri(URI.create(restTemplateBase))\n                                .path(\"/thing\")\n                                .toUriString())\n                        .body(thing),\n                Thing.class\n        ).getBody();\n    }\n\n    record Thing(String id, String name) {\n    }\n}\n</code></pre>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#handling-http-responses","title":"Handling HTTP responses","text":"<p>Sometimes, you need to look at the HTTP response. This could be to handle different HTTP status codes in different ways or to manipulate the response in some way. Both RestTemplate and RestClient can return the <code>ResponseEntity</code> class (since the latter is built using the former), whereas WebClient uses the reactive <code>ClientResponse</code> class to allow you to work with HTTP responses whilst staying in the reactive ecosystem:</p> <pre><code>import org.springframework.http.RequestEntity;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.client.RestClient;\nimport org.springframework.web.client.RestTemplate;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport reactor.core.publisher.Mono;\n\n@Component\nclass ExampleClient {\n    private final RestClient restClient;\n    private final WebClient webClient;\n    private final RestTemplate restTemplate;\n    private final String REST_TEMPLATE_HOST;\n\n    Thing sendThingWithRestClient(Thing thing) {\n        return restClient.post()\n                .uri(\"/thing\")\n                .body(thing) // Sending in the request body\n                .retrieve()\n                .body(Thing.class); // Assuming a Thing is returned in the response body\n    }\n\n    // Mono&lt;&gt; is a reactive publisher that could contain 0 or 1 thing,\n    // kind of like Optional&lt;&gt; but reactive.\n    Mono&lt;Thing&gt; sendThingWithWebClient(Thing thing, Mono&lt;Thing&gt; monoThing) {\n        // Either provide a value as the request body:\n        return webClient.post()\n                .uri(\"/thing\")\n                .body(BodyInserters.fromValue(thing))\n                .retrieve()\n                .bodyToMono(Thing.class);\n\n        // or provide a Mono as the request body: \n        return webClient.post()\n                .uri(\"/thing\")\n                .body(monoThing, Thing.class)\n                .retrieve()\n                .bodyToMono(Thing.class);\n    }\n\n    Thing sendThingWithRestTemplate(Thing thing) {\n        // Continuing usage with the more flexible exchange() method\n        return restTemplate.exchange(\n                RequestEntity.post(UriComponentsBuilder\n                                .fromUri(URI.create(restTemplateBase))\n                                .path(\"/thing\")\n                                .toUriString())\n                        .body(thing),\n                Thing.class\n        ).getBody();\n    }\n\n    record Thing(String id, String name) {\n    }\n}\n</code></pre>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#exception-handling","title":"Exception handling","text":"<p>Assuming you're writing more critical production code, you'll need to be handling response codes and exceptions in order to produce safer code and less buggy features. Both RestTemplate and RestClient can produce the exceptions:</p> <ul> <li><code>HttpClientErrorException</code> for 4xx response codes.</li> <li><code>HttpServerErrorException</code> for 5xx response codes.</li> <li><code>ResourceAccessException</code> for network or other I/O exceptions (including timeouts)</li> </ul>"},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#configuring-timeouts","title":"Configuring timeouts","text":""},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#configuring-buffer-sizes","title":"Configuring buffer sizes","text":""},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#configuring-customising-all-clients-in-an-application","title":"Configuring / Customising all clients in an application","text":""},{"location":"blog/software-engineering/restclient-vs-webclient-vs-resttemplate-configuring/#conclusion","title":"Conclusion","text":"<p>I find RestTemplate always a bit confusing to discover and figure out how I should be writing my code to perform a HTTP call, given there are handy shorthand methods available, but also the common and powerful <code>exchange()</code> method. The problem is that learning how to write readable, maintainable, flexible <code>exchange()</code> methods takes a while to learn and become familiar with (figuring out what classes are involved, what they mean, and what helpful builders are available in order to configure what you want).</p> <p>As such, I find the new fluent APIs of RestClient and WebClient a welcome addition. It still takes a hot minute to figure out the builders / lambdas on how to configure it, but since there are less varying methods and options, it is much quicker to learn how things should be written. I also find that the final result is much more readable and intuitive.</p> <p>WebClient however, requires the introduction of reactive programming into an application, which isn't required (you could always just call <code>block()</code> on the <code>Mono&lt;&gt;</code> or <code>Flux&lt;&gt;</code> publishers) but then if you're not going to rewrite your application code to be completely reactive, then there is little point in using WebClient over RestClient.</p> <p>As for whether to do reactive programming or not... I'll leave that for another post, but personally, I've found multiple reasons to not use it, and little reason to do so - and the reasons why I may want to are probably going to be squashed by Java's upcoming Virtual Threads (Project Loom) feature.</p>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/","title":"Setting up a large Minecraft server","text":"<p>Originally published: 23 September 2016</p>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#introduction","title":"Introduction","text":"<p>This is a guide on how to set up a Spigot Server on Linux using Multicraft, BungeeCord and tmux.</p> <p>We're going to be using a Ubuntu 14.04 Server on AWS along with my domain name harmelodic.com so we can fiddle with domain names too.</p> <p>When this will be put into production, I'll be using a Debian 8 installation but as Ubuntu 14.04 is based on Debian, I don't foresee any major changes to the setup process (with the possible exception of small configuration changes)</p> <p>There will be a few occasions during this blog where I will mention configuring or doing something on your own. This is usually because I'll be doing something that involves private information or it will be so long-winded and tedious that it's not worth mentioning. If you aren't sure of what to do in these scenarios, I'd highly recommend reading the related documentation.</p>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#aws-instance-setup","title":"AWS Instance Setup","text":"<ul> <li>Ubuntu 14.04 Server (64-bit)</li> <li>8GB RAM</li> </ul>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#preparation","title":"Preparation!","text":"<p>Add a user for minecraft stuff called minecraft and add the user to the sudo group so that it can perform <code>sudo</code> commands:</p> <pre><code>sudo adduser minecraft\nsudo adduser minecraft sudo\nsu - minecraft\n</code></pre> <p>Setup Java 8:</p> <pre><code>sudo add-apt-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\njava -version\n</code></pre> <p>Install Git and tmux:</p> <pre><code>sudo apt-get install git-all tmux\n</code></pre> <p>Download the archives we need into their own folders:</p> <p>Download the BuildTools.jar and run it to get the most recent version of Spigot:</p> <pre><code>mkdir spigot\ncd spigot\nwget -O BuildTools.jar \"https://hub.spigotmc.org/jenkins/job/BuildTools/lastSuccessfulBuild/artifact/target/BuildTools.jar\"\nsudo java -jar BuildTools.jar\ncd ..\n</code></pre> <p>Download the BungeeCord archive:</p> <pre><code>mkdir bungee\ncd bungee\nwget -O BungeeCord.jar \"http://ci.md-5.net/job/BungeeCord/lastSuccessfulBuild/artifact/bootstrap/target/BungeeCord.jar\"\ncd ..\n</code></pre>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#installation-of-multicraft","title":"Installation of Multicraft","text":"<p>First thing to setup is Multicraft. Change directory to <code>/home/minecraft/</code>:</p> <pre><code>wget -O multicraft.tar.gz \"http://www.multicraft.org/download/linux64\"\nsudo tar xvzf multicraft.tar.gz\ncd multicraft\nsudo ./setup.sh\n</code></pre> <p>Follow the instructions and install multicraft.</p> <p>You'll have noticed in the install instructions that we need to run install.php file. This requires PHP and the PHP sqlite/mysql extension(s) (both of which don't come auto-installed on AWS Ubuntu Servers. This also assumes you're using a sqlite database). Install PHP:</p> <pre><code>sudo apt-get install php5\nsudo apt-get install php5-sqlite\nsudo apt-get install php5-mysql\nsudo apt-get install php5-mysqlnd\nsudo service apache2 restart\n</code></pre> <p>I'm going to use MySQL as a database as it's a little easier to configure; As such I need to install MySQL server, login to the MySQL CLI and create the 2 databases needed:</p> <pre><code>sudo apt-get install mysql-server\nsudo service mysql start\nmysql -u root -p\n</code></pre> <p>then in the MySQL shell:</p> <pre><code>CREATE\nDATABASE multicraft_panel;\nCREATE\nDATABASE multicraft_daemon;\n</code></pre> <p>Ensure the databases are showing up inside MySQL Server with:</p> <pre><code>SHOW\nDATABASES;\n</code></pre> <p>Then exit out of the MySQL CLI.</p> <p>Open a web browser and go to <code>http://&lt;your-server-ip&gt;/multicraft/install.php</code> (You may need to edit your AWS Security Group to allow HTTP connections)</p> <p>Follow the installation process, you may need to do a little fiddling with the webserver to make sure everything on the install list succeeds!</p> <p>On the last page, you'll be shown the Daemon Configuration and at the bottom your Daemon. If you don't see your Daemon, perform:</p> <pre><code>sudo /home/minecraft/multicraft/bin/multicraft start\n</code></pre> <p>I found a couple of issues with this. Firstly, it failed to start the TCP server, as it was already running. I had to find the PID number of the multicraft service and then kill the service:</p> <pre><code>sudo netstat -tulpn | grep :25465\nkill -9 &lt;pid_number&gt;\n</code></pre> <p>I then had an issue where it just didn't appear in the Daemon list, but seemed to start. After performing the start up command with Debug mode on:</p> <pre><code>sudo /home/minecraft/multicraft/bin/multicraft -n start\n</code></pre> <p>I found it couldn't connect to MySQL database as the database connection details it had in <code>/home/minecraft/multicraft/multicraft.conf</code> were empty.</p> <p>Once the daemon appears, you can go to <code>http://&lt;your-server-ip&gt;/multicraft/index.php</code>, perform:</p> <pre><code>sudo rm /var/www/html/multicraft/install.php\n</code></pre> <p>And you're done!</p>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#installation-of-serversspigot","title":"Installation of servers/Spigot","text":"<p>From the Multicraft menu, you can just create your servers! But before you can actually START them, you'll need to use the most recent Spigot jar.</p> <p>Thankfully in Multicraft if you go to <code>Settings &gt; Update Minecraft</code> you can download, install and update versions of the Minecraft Server (including Spigot).</p> <p>At the time of writing this, Minecraft and Spigot version 1.9.2 is the most recent Minecraft version. Unfortunately, Multicraft would only update Spigot to version 1.9.0 so I'm going to have to add a custom Spigot jar.</p> <p>In our preparation we've already got the most recent version of Spigot generated (by running BuildTools.jar). So all we need to do is:</p> <pre><code>cp /home/minecraft/spigot/spigot-1.9.2.jar /home/minecraft/multicraft/jar/.\n</code></pre> <p>Now that the most recent version of spigot is multicraft, let's go back to the Multicraft panel.</p> <p>Create a server and only enter the <code>Name</code> field. All other fields will be set to default and can later be configured.</p> <p>Once the server has been created, Go to <code>Servers</code> and click on the server we've just created.</p> <p>Go down to <code>JAR File</code>, select <code>Default</code> in the upper drop-down menu and type <code>spigot-1.9.2.jar</code>. Click Save Then, click Accept EULA Click Save again.</p> <p>Now start your server! You should be able to connect by going into Minecraft and using <code>&lt;your-server-ip&gt;</code>.</p> <p>As we're going to be using BungeeCord as a front end proxy, we'll need to shutdown each server and change the port number for each server to <code>25566</code>, then <code>25567</code>, then <code>25568</code> and so on for each server we've setup.</p>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#installation-of-bungeecord","title":"Installation of BungeeCord","text":"<p>Ah...BungeeCord! A lot of people hate BungeeCord and complain it's too complicated.</p> <p>I suspect these people don't really understand what BungeeCord is. (FYI it's just a proxy server designed to handle multiple Minecraft Servers)</p> <p>As per our preparation we already have BungeeCord.jar in <code>/home/minecraft/multicraft/bungee/</code>. What we need to do now is generate the configuration files. To do this we need to run BungeeCord:</p> <pre><code>java -jar BungeeCord.jar\n</code></pre> <p>Once you see:</p> <p>18:48:23 [INFO] Listening on /0.0.0.0:25577 &gt;</p> <p>Hit Ctrl+C to end BungeeCord running. If you run <code>ls</code> now, you'll see we have a load of configuration files!</p> <p>Open <code>config.yml</code> in your favourite terminal editor and configure it how you want it to run! However I'd highly recommended setting the following things:</p> <p>Under <code>servers:</code>, define all your servers that you're using and name them how you want. By default you'll have one server there already called <code>lobby:</code>. You'll want to define the <code>address:</code> for each server to <code>localhost:</code> with <code>25566</code>, then <code>25567</code>, then <code>25568</code> and so on. The default minecraft server port is <code>25565</code> but we're going to use that for BungeeCord itself. This will mean that when we do <code>/server &lt;server_name&gt;</code> in game, we can change/jump to that server! Wooo! Speaking of which, under <code>listeners:</code>, define <code>query_port:</code> to <code>25565</code> and then define <code>host:</code> to <code>0.0.0.0:25565</code>.</p> <p>Once you've defined everything, open a tmux session called bungee, run BungeeCord then detach from the session:</p> <pre><code>tmux new -s bungee\njava -jar BungeeCord.jar\n</code></pre> <p>Ctrl+B D</p> <p>BungeeCord is now set up and running! We're done!</p>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#installing-plugins-and-worlds","title":"Installing Plugins and Worlds","text":"<p>If we want to install any new plugins to our server, we can either:</p> <ul> <li>Dump and configure them in <code>/home/minecraft/multicraft/servers/server&lt;#&gt;/plugins</code>.   or</li> <li>FTP to our server and install them through there. (FTP Access can be found out from   <code>Servers &gt; &lt;server_name&gt; &gt; Files &gt; FTP File Access</code>).</li> </ul> <p>If we want to install any new worlds to our server, we can either:</p> <ul> <li>Dump and configure them in <code>/home/minecraft/multicraft/servers/server&lt;#&gt;/plugins</code>.   or</li> <li>FTP to our server and install them through there. (FTP Access can be found out from   <code>Servers &gt; &lt;server_name&gt; &gt; Files &gt; FTP File Access</code>).</li> </ul> <p>For the latter (FTP) option, we'd need to install a FTP client on your server and configure it with Multicraft.</p> <p>Remember you'll if you're installing a new plugin on a server you'll need to restart that server for the plugin to be registered.</p>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#domain-name-registration","title":"Domain Name Registration","text":"<p>I'm going to be using AWS's Route 53 to configure DNS Registration but most DNS providers will work in a similar way.</p> <p>Within a domain's hosted zone, simply create a new Record Set that is of type A.</p> <p>So for me, I'm entering my <code>harmelodic.com</code> hosted zone and creating an A type Record Set.</p> <p>Then name it your chosen sub-domain (I'm going to use <code>mc.harmelodic.com</code>) and define the value of the Record Set to <code>&lt;your-server-ip&gt;</code>.</p> <p>Create/Confirm it and you're done!</p>"},{"location":"blog/software-engineering/setting-up-a-large-minecraft-server/#end-to-end-pattern","title":"End-to-end pattern","text":"<p>Assuming you've done all the above, you should now have a server that maps as follows:</p> <ol> <li>Requests going to <code>mc.DOMAIN.TLD</code> are directed to your dedicated server.</li> <li>BungeeCord (running in a tmux session) listens at port <code>25565</code> for your normal Minecraft requests.</li> <li>BungeeCord redirects requests to a Multicraft managed Minecraft Server (defaulting to a lobby styled server).</li> <li>Each server has it's own plugins and worlds.</li> </ol> <p>Congratulations!</p>"},{"location":"blog/software-engineering/setting-up-an-nginx-ingress/","title":"Setting up an NGINX ingress","text":"<p>Originally published: 03 November 2018</p>"},{"location":"blog/software-engineering/setting-up-an-nginx-ingress/#requirements","title":"Requirements","text":"<ul> <li><code>kubectl</code> - connected to your cluster.</li> <li><code>helm</code></li> </ul>"},{"location":"blog/software-engineering/setting-up-an-nginx-ingress/#installation","title":"Installation","text":"<p>Install <code>tiller</code> on the cluster:</p> <pre><code>helm init\n</code></pre> <p>Setup the default kube-system service account as a cluster admin, this allows tiller to deploy things:</p> <pre><code>kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default\n</code></pre> <p>Install the NGINX Ingress Controller</p> <pre><code>helm install --name nginx --set rbac.create=true stable/nginx-ingress\n</code></pre> <p>Check the nginx-ingress is up by doing:</p> <pre><code>kubectl get services\n</code></pre> <p>In any <code>ingress.yaml</code> files for any applications you make, ensure that under <code>metadata &gt; annotations</code>, you define the <code>ingress.class</code> as <code>\"nginx\"</code>. For example:</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: example-app\n  namespace: example-namespace\n  labels:\n    app: example-app\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n</code></pre>"},{"location":"blog/software-engineering/setting-up-an-nginx-ingress/#tls","title":"TLS","text":"<p>Taken from the output of installing the nginx-ingress.</p> <p>An example Ingress that makes use of the controller:</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n  name: example\n  namespace: foo\nspec:\n  rules:\n    - host: www.example.com\n      http:\n        paths:\n          - backend:\n              serviceName: exampleService\n              servicePort: 80\n            path: /\n  # This section is only required if TLS is to be enabled for the Ingress\n  tls:\n    - hosts:\n        - www.example.com\n      secretName: example-tls\n</code></pre> <p>If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: example-tls\n  namespace: foo\ndata:\n  tls.crt: &lt;base64 encoded cert&gt;\n  tls.key: &lt;base64 encoded key&gt;\ntype: kubernetes.io/tls\n</code></pre>"},{"location":"blog/software-engineering/setting-up-cert-manager-on-k8s-with-lets-encrypt/","title":"Setting up cert-manager on K8s with Let's Encrypt","text":"<p>Originally published: 18 November 2018</p> <p>Cert-manager is a certificate management controller for Kubernetes (K8s).</p> <p>You can connect it up to Let's Encrypt and other services to enable HTTP over TLS (HTTPS). This doc details how to set it up on Kubernetes for the domain <code>example.com</code>.</p>"},{"location":"blog/software-engineering/setting-up-cert-manager-on-k8s-with-lets-encrypt/#notes","title":"Notes","text":"<p>This uses the DNS-01 validation challenge, which allows us to obtain wildcard certificates. You can use HTTP-01 validation, however, cert-manager will not support obtaining wildcard certificates through HTTP-01. This guide also only covers Route53; Other providers are available.</p> <p>This requires you to be using K8s 1.9+ as RBAC is required.</p> <p>This guide details how to setup cert-manager to work with NGINX Ingress. Other Ingress solutions/architectures may work differently.</p>"},{"location":"blog/software-engineering/setting-up-cert-manager-on-k8s-with-lets-encrypt/#guide","title":"Guide","text":"<ol> <li> <p>Ensure you have Helm installed on your machine.</p> </li> <li> <p>Install Tiller on your cluster:</p> </li> </ol> <p><code>bash    helm init</code></p> <ol> <li>Give Tiller the RBAC cluster-admin permissions it needs:</li> </ol> <p><code>bash    kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default</code></p> <ol> <li>Install cert-manager using Helm:</li> </ol> <p><code>bash    helm install \\        --name cert-manager \\        --namespace kube-system \\        stable/cert-manager</code></p> <ol> <li>Prove it's started with:</li> </ol> <p><code>bash    kubectl -n kube-system get pods</code></p> <p>you should see:</p> <p><code>NAME                                               READY     STATUS    RESTARTS   AGE    cert-manager-xxxxxxxxxx-xxxxx                      1/1       Running   0          46s    ...</code></p> <ol> <li> <p>Fetch your Hosted Zone ID by going to: <code>AWS &gt; Route53 &gt; Hosted zones</code> and copying the Hosted Zone ID from your    appropriate domain name. (Here on out, the example Hosted Zone ID will be <code>EGZONEIDFROM53</code>)</p> </li> <li> <p>Setup a Route53 IAM Policy by going to: <code>AWS &gt; IAM &gt; Policies &gt; Create policy &gt; JSON</code> and pasting:</p> </li> </ol> <p><code>json    {        \"Version\": \"2012-10-17\",        \"Statement\": [            {                \"Effect\": \"Allow\",                \"Action\": \"route53:GetChange\",                \"Resource\": \"arn:aws:route53:::change/*\"            },            {                \"Effect\": \"Allow\",                \"Action\": \"route53:ChangeResourceRecordSets\",                \"Resource\": \"arn:aws:route53:::hostedzone/EGZONEIDFROM53\"            }        ]    }</code></p> <p>Name this policy: <code>CertManager-DNS01-Validation-example.com</code></p> <ol> <li>Create a new AWS user for cert-manager to use to perform DNS-01 validation by going to <code>AWS &gt; IAM &gt; Users &gt; Add User</code>    and creating a new user with the following properties:<ul> <li>Programmatic access</li> <li>Name this user <code>cert-manager-example.com</code>.</li> <li>In Permissions, choose <code>Attach existing policies directly</code> and select the   <code>CertManager-DNS01-Validation-example.com</code> policy we just created.</li> </ul> </li> </ol> <p>Copy the Access Key ID and Secret Access Key generated.    Here on out, these will be referred to as <code>ACCESSKEYID</code> and <code>SECRETACCESSKEY</code>, respectively.</p> <ol> <li>Create a secret to hold the Secret Access Key:</li> </ol> <p><code>bash    kubectl create secret generic cert-manager-route53 --from-literal=secret-access-key=SECRETACCESSKEY</code></p> <ol> <li> <p>Create an Issuer (this provides the interface to your CA, from which x509 certificates can be obtained):</p> <p><code>yaml apiVersion: certmanager.k8s.io/v1alpha1 kind: Issuer metadata:   name: letsencrypt-staging spec:   acme:     server: https://acme-staging-v02.api.letsencrypt.org/directory     email: user@example.com     privateKeySecretRef:       name: letsencrypt-staging     dns01:       providers:       - name: route53         route53:           region: eu-west-1           hostedZoneID: EGZONEIDFROM53           accessKeyID: ACCESSKEYID           secretAccessKeySecretRef:             name: cert-manager-route53             key: secret-access-key</code></p> <p>Note: You can create a ClusterIssuer, which is the same as an Issuer except that it will issue certificates across all K8s namespaces, whereas a normal Issuer will only issue certificates to the namespace it is deployed in. If you choose to use a ClusterIssuer, you will have to rename all references to <code>Issuer</code> to <code>ClusterIssuer</code> and all references to <code>certmanager.k8s.io/issuer</code> to <code>certmanager.k8s.io/cluster-issuer</code>, from here on out.</p> <p>Note: You'll have noticed we are providing AWS credentials here. However, this is optional as cert-manager, when using the Route53 provider, can use ambient credentials, but only under certain circumstances.</p> <p>Note: If you're using a K8s Service Account (e.g. within a CI/CD process) to create this issuer, you'll need to bind the <code>cert-manager</code> ClusterRole to the Service Account. For example, if you're using a GitLab CI/CD with a K8s Service Account called <code>gitlab</code> in the <code>default</code> namespace you'd perform:</p> <p><code>bash kubectl create rolebinding gitlab-cert-manager --clusterrole=cert-manager --serviceaccount=default:gitlab</code></p> </li> <li> <p>Create an NGINX Ingress with the following extra values:</p> <p>```yaml</p> </li> <li> <p>Wait for the Issuer to go perform the validation checks and obtain certificates.     After about 5 minutes, perform:</p> <p><code>bash kubectl get secrets</code></p> <p>and you should see a new secret called <code>tls-staging-cert-wildcard-example</code> of type <code>kubernetes.io/tls</code>.</p> <p>If you access <code>https://example.com</code>, you will fail the browser's trust, but if you check the site's certificate you should see that the certificate is a <code>Fake LE Root X1</code> certificate.</p> <p>This means that your cert-manager &amp; Issuer configuration is working</p> </li> <li> <p>Configure for production by removing <code>-staging</code> from all places in your Issuer &amp; Ingress YAML configurations; Then     redeploy the Issuer and Ingress, wait a few minutes and access your newly secured and trusted HTTPS domain.</p> </li> </ol>"},{"location":"blog/software-engineering/setting-up-cert-manager-on-k8s-with-lets-encrypt/#_1","title":"...","text":"<p>metadata:</p>"},{"location":"blog/software-engineering/setting-up-cert-manager-on-k8s-with-lets-encrypt/#_2","title":"...","text":"<p>annotations:   # ...   kubernetes.io/tls-acme: \"true\"   certmanager.k8s.io/acme-dns01-provider: route53   certmanager.k8s.io/acme-challenge-type: dns01   certmanager.k8s.io/issuer: letsencrypt-staging</p>"},{"location":"blog/software-engineering/setting-up-cert-manager-on-k8s-with-lets-encrypt/#_3","title":"...","text":"<p>spec:   # ...   tls:   - hosts:     - \"*.example.com\"     - example.com     secretName: tls-staging-cert-wildcard-example ```</p>"},{"location":"blog/software-engineering/setup-clion-on-windows/","title":"Setup CLion on Windows","text":"<p>Originally published: 15 November 2016</p>"},{"location":"blog/software-engineering/setup-clion-on-windows/#installation","title":"Installation","text":"<ul> <li>Install CLion.</li> <li>Install Cygwin.</li> </ul>"},{"location":"blog/software-engineering/setup-clion-on-windows/#preparing-cygwin","title":"Preparing Cygwin","text":"<ol> <li> <p>Run the Cygwin <code>setup-*.exe</code> file you downloaded.</p> </li> <li> <p>If an Administration pop-up window appears. Click Yes.</p> </li> <li> <p>Once the Cygwin Setup window appears. Click Next.</p> </li> <li> <p>Select Install from Internet. Click Next.</p> </li> <li> <p>Select your Root Directory, I recommend using the root directory of a particular drive (for example <code>D:\\cygwin64</code>)</p> </li> <li> <p>Select Install for All Users. Click Next.</p> </li> <li> <p>Select your local package directory, I recommend creating a folder in your home folder or in your Downloads folder    for this. Click Next.</p> </li> <li> <p>Select Direct Connection. Click Next.</p> </li> <li> <p>Select your chosen mirror. I use <code>http://www.mirrorservice.org</code>. Click Next.</p> </li> <li> <p>You will then be presented with a screen to select your packages to install. To select a packge for install, simply     click on where it says \"Skip\" for that respective package.     You need: <code>cmake</code>, <code>cmake-gui</code>, <code>make</code>, <code>gcc-core</code>, <code>gcc-g++</code>, <code>gdb</code> (all from the Devel category).     I recommend also getting: <code>vim</code>, <code>curl</code>, <code>wget</code>, <code>ssh</code>, <code>ncurses</code>.     Once done, click Next.</p> </li> <li> <p>Deselect Create icon on Desktop. Click Finish.</p> </li> </ol>"},{"location":"blog/software-engineering/setup-clion-on-windows/#setting-up-clion","title":"Setting up CLion","text":"<p>You should already have installed CLion, now find the <code>CLion</code> program.</p> <ol> <li> <p>Select your theme. CLick Next: Toolchains.</p> </li> <li> <p>Under Environment, ensure the Cygwin option is selected.</p> </li> <li> <p>Under CMake executable, select Custom, and type in the location of your <code>cmake.exe</code> inside the <code>bin</code>    folder inside where you installed Cygwin. For example, if you installed Cygwin under <code>D:\\cygwin64</code>, then you should    use <code>D:\\cygwin64\\bin\\cmake.exe</code>.</p> </li> <li> <p>All of the tools at the bottom should load with tick symbols at the start. If any are red circles with an <code>i</code> in    them, then you need to ensure that you installed all the correct Cygwin packages back in the Cygwin preparation    stage. If all are ticks, Click Next: Default Plugins</p> </li> <li> <p>Disable any plugins you want, I didn't disable any. Click Next: Featured Plugins</p> </li> <li> <p>Install any featured plugins you want, I installed Lua, Markdown and Go as I may be using them in my development time    using CLion. Start using CLion.</p> </li> </ol> <p>And you're done! Enjoy programming in C/C++ on your Windows machine!</p>"},{"location":"blog/software-engineering/some-software-development-principles/","title":"Some Software Development Principles","text":"<p>Originally published: 01 May 2020</p> <p>Developing software can be a complicated business. Thankfully, various principles are on hand to help engineers make good design decisions.</p> <p>Here are some that have helped me.</p>"},{"location":"blog/software-engineering/some-software-development-principles/#kiss","title":"KISS","text":"<p>Keep it simple, stupid or Keep it small &amp; simple <sub>[C.H.]</sub></p>"},{"location":"blog/software-engineering/some-software-development-principles/#test-pyramid","title":"Test Pyramid","text":"<ul> <li>Unit Tests are fast and cheap, so make more of your tests Unit Tests.</li> <li>Service Tests are slower and more expensive, so only make some of those.</li> <li>UI tests are very slow and super expensive, make as few as possible.</li> </ul>"},{"location":"blog/software-engineering/some-software-development-principles/#3-types-of-functional-testing-state-collaboration-contract","title":"3 Types of Functional Testing - State, Collaboration, Contract","text":"<p>Provided you design your solution well - i.e. into components that connect to interfaces that have other components that implement those interfaces.</p> <ul> <li>State testing is the testing you do on the component you've made.</li> <li>Collaboration testing is the testing you do between your component and it's interfaces.</li> <li>Contract testing is the testing you do to check your external interface implementations adhere to the interface in the   way that your component expects them to (aka - adhering to the Contract).</li> <li>The only other functional testing that is required is the few integrated tests that you need to test components   outside of your domain.</li> </ul> <p>This is my own principle based on J.B. Rainsburger's talk about how Integrated Tests are a Scam. It's a very good talk with very compelling points.</p> <p>Non-functional testing, such as security, performance and reliability and acceptance testing cannot be fully covered by this methodology.</p>"},{"location":"blog/software-engineering/some-software-development-principles/#testing-permutations","title":"Testing Permutations","text":"<p>When testing a function that return Collections, ensure you have a test for each of the following:</p> Scenario Explanation Zero When the function returns nothing One When the function returns a single entity Many When the function returns a list of entities Lots When the function returns an absurd amount of entities (for: pagination, filtering, long-load times) Oops When the function errors"},{"location":"blog/software-engineering/some-software-development-principles/#12-factor-app","title":"12 Factor App","text":"Factor Description Codebase There should be exactly one codebase for a deployed service with the codebase being used for many deployments. Dependencies All dependencies should be declared, with no implicit reliance on system tools or libraries. Config Configuration that varies between deployments should be stored in the environment. Backing services All backing services are treated as attached resources and attached and detached by the execution environment. Build, release, run The delivery pipeline should strictly consist of build, release, run. Processes Applications should be deployed as one or more stateless processes with persisted data stored on a backing service. Port binding Self-contained services should make themselves available to other services by specified ports. Concurrency Concurrency is advocated by scaling individual processes. Disposability Fast startup and shutdown are advocated for a more robust and resilient system. Dev/Prod parity All environments should be as similar as possible. Logs Applications should produce logs as event streams and leave the execution environment to aggregate. Admin Processes Any needed admin tasks should be kept in source control and packaged with the application."},{"location":"blog/software-engineering/some-software-development-principles/#unix-philosophy","title":"Unix Philosophy","text":"<ul> <li>Write programs that do one thing and do it well.</li> <li>Write programs to work together.</li> <li>Write programs to handle text streams, because that is a universal interface.</li> </ul>"},{"location":"blog/software-engineering/some-software-development-principles/#domain-driven-design","title":"Domain-driven Design","text":"<p>Goals:</p> <ul> <li>Placing the project's primary focus on the core domain and domain logic.</li> <li>Basing complex designs on a model of the domain.</li> <li>Initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that   addresses particular domain problems.</li> </ul> <p>Where the model concepts include:</p> Concept Description Context The setting in which a word or statement appears that determines its meaning. Domain A sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software. Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. Ubiquitous Language A language structured around the domain model and used by all team members to connect all the activities of the team with the software."},{"location":"blog/software-engineering/some-software-development-principles/#test-driven-development","title":"Test-driven development","text":"<ol> <li>Write your test</li> <li>Prove it fails</li> <li>Write your code</li> <li>Prove it works</li> <li>Refactor your solution as necessary</li> <li>Prove it still works</li> </ol>"},{"location":"blog/software-engineering/some-software-development-principles/#solid","title":"SOLID","text":"Principle Description Single-responsibility A class should only have a single responsibility, that is, only changes to one part of the software's specification should be able to affect the specification of the class. Open\u2013closed Software entities ... should be open for extension, but closed for modification. Liskov substitution Objects in a program should be replaceable with instances of their subtypes without altering the correctness of that program.\" See also design by contract. Interface segregation A language structured around the domain model and used by all team members to connect all the activities of the team with the software. Dependency inversion One should depend upon abstractions, not concretions."},{"location":"blog/software-engineering/some-software-development-principles/#cap-theorem","title":"CAP Theorem","text":"<p>It is impossible for a distributed data-store to simultaneously provide more than two out of the following three guarantees:</p> Guarantee Description Consistency Every read receives the most recent write or an error. Availability Every request receives a (non-error) response, without the guarantee that it contains the most recent write. Partition tolerance A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain."},{"location":"blog/software-engineering/some-software-development-principles/#acid","title":"ACID","text":"<p>When developing database transactions, abiding by the following principles guarantee validity even in the event of errors:</p> Property Description Atomicity The transaction must either completely succeed or completely fail. If succeeded, the entire transaction occurs. If failed, the database is unchanged. Consistency The transaction must bring the database from one valid state to another valid state. Isolation If transactions are executed concurrently, the database MUST achieve the same state that it would have if the transactions were executed sequentially. Durability Once a transaction is committed, it will remain committed even in the case of system failure."},{"location":"blog/software-engineering/some-software-development-principles/#agile","title":"Agile","text":"<ul> <li>Individuals and interactions over processes and tools</li> <li>Working software over comprehensive documentation</li> <li>Customer collaboration over contract negotiation</li> <li>Responding to change over following a plan</li> </ul> <p>That is, while there is value in the items on the right, we value the items on the left more.</p>"},{"location":"blog/software-engineering/special-characters-urls-and-percent-encoding/","title":"Special characters, URLs and %-encoding","text":"<p>Originally published: 10 November 2016</p> <p>Handling special characters with %-encoding is a rather...tricky thing to do.</p> <p>Well, it isn't really. The actual application of it is relatively simple. You just reference the relevant library/package and apply whatever the standard %-encoding function is to your URL. Simple, right?</p> <p>Wrong. The wider implementation of handling special characters when building HTTP requests is quite fiddly and often implemented badly. (You can find my suggested implemention at the bottom of the page)</p> <p>If you wanted to provide an interface/service/adapter/bit-of-code for the rest of your application to perform HTTP requests, for example:   <code>http://harmelodic.com/api/services/testing.html?username=nedflanders&amp;password=howdy+neighbour</code> What do you do with your special characters?</p> <p>Well, the natural thing to do would be to %-encode them, but that puts you in a tricky situation with lovely ( horrible), wonderful (awful) things called reserved characters.</p> <p>Reserved characters can be some of the most frustrating things for people to use and/or not use when handling HTTP requests. The idea of them, is that a select few characters are reserved in the HTTP standard for delimiting parts of your URL.  They are as follows:</p> <p><code>!</code> <code>#</code> <code>$</code> <code>&amp;</code> <code>'</code> <code>(</code> <code>)</code> <code>*</code> <code>+</code> <code>,</code> <code>/</code> <code>:</code> <code>;</code> <code>=</code> <code>?</code> <code>@</code> <code>[</code> <code>]</code></p> <p>The most used ones like <code>/</code>, <code>?</code>, <code>=</code> and <code>&amp;</code> actually make sense as being reserved characters: <code>/</code> is used for delimiting when defining where a resource is (the <code>/api/services/testing.html</code> bit). <code>?</code> is used to define the start of the query string (the <code>?username=nedflanders&amp;password=howdy+neighbour</code> bit) <code>&amp;</code> is used to delimit each query string parameter (and its associated value) on the query string. <code>=</code> is used to delimit between a query string parameter and it's associated value (e.g <code>username=nedflanders</code>).</p> <p>That's okay but what about something like a <code>+</code>?  <code>+</code> isn't used for any hugely important logic in a HTTP request like <code>/</code> or <code>?</code> is. <code>+</code> is used to represent a space character. That's pretty okay I guess, you can neatly replace each space that occurs with a <code>+</code> and you'll all dandy! That's really simple! There's no problems with that, yes?</p> <p>Well...no.</p> <p>What if you have a URL with a <code>+</code> in a query string value? That <code>+</code> could be pretty important. It could be a character in an email address or an API key. If your 3rd party server reads that <code>+</code> symbol as a space character when it's not supposed to be a space character, then it can screw up your data and that's something we DO NOT want. So what's the work around?</p> <p>You %-encode it!</p> <p>But that leads to a further problem with our initial scenario. We said:</p> <p>What if you wanted to provide an interface/service/adapter/bit-of-code for the rest of your applicaiton to perform HTTP requests.</p> <p>This is a perfectly valid scenario. You want to abstract your code for performing HTTP requests from your application so that your application uses a standard HTTP process. Moreover, abstracting this code means that you can remove any context that may have been required in certain scenarios and instead just handle the HTTP request for the caller.</p> <p>The problem that we have with this scenario, is what the definitive answer is to the following question:</p> <p>What if whatever is calling your code, provided you with a URL with <code>+</code> symbol in it. Do you handle it as a space or as a <code>+</code>?</p> <p>If it's meant to be a space, then you should leave it as a <code>+</code> and the normal, underlying HTTP process will handle that correctly. If it's meant to be a <code>+</code>, then you should %-encode it to retain data integrity! But how do you tell?</p> <p>The answer is: You can't, without context.</p> <p>The whole point of abstracting your code is to remove context, especially this sort of context. You are intentionally removing the necessity for whatever is calling your code to provide any underlying HTTP logic.</p> <p>Which means we need to implement this abstraction with a better standard. Here's the one that makes the most sense to me:</p> <p>If whatever is calling your code, requires a special character to be in the URL (whether it be a reserved character or not) they should provide it to your service as the literal special character only and nothing else. That means: No %-encoding, No <code>+</code> symbols that are supposed to mean spaces. None of that stuff.</p> <p>This way, your abstracted code has complete control over everything to do with the HTTP process, including encoding and handling special characters properly.</p> <p>Sounds simple and logical, right? Why am I even on about this? Because I've been in too many situations where people get this horrendously wrong and, to be frank, it's really bugging me.</p> <p>Keep your code clean.</p>"},{"location":"blog/software-engineering/special-characters-urls-and-percent-encoding/#final-note-on-implementation","title":"Final note on implementation:","text":"<p>Don't (for heavens sake) simplify your HTTP code down to providing a single static function that takes an entire URL (including query string parameters) and %-encode that.</p> <p>The problem with implementing it this way is: If whatever is calling your code, provides a query string parameter (for example, <code>username</code>) with a value that contains a reserved character like <code>&amp;</code> or <code>=</code> or something, then you're going to get into a very tricky situation where, without context, you don't actually know what is valid data, and what are valid reserved characters.</p> <p>A good way to implement this standard would be to have a class (or object) that performs the HTTP code. This way, whatever calls your code has to instantiate this class as a variable and then use Setter methods/functions available to this variable to set up the URL, Port, URI and each query string parameter (and anything else you need to set up). This means you can handle the %-encoding for each method and handle all of the actual HTTP logic within. Better yet, it means that whatever calls your code, doesn't provide HTTP delimiters <code>?</code>, <code>=</code> and <code>&amp;</code> which they could mistakenly encode, thinking its helping when it's not.</p>"},{"location":"blog/software-engineering/using-oneplus-bullets-wireless-aptx-with-macos/","title":"Using OnePlus Bullets Wireless (or any AptX device) with macOS","text":"<p>Originally published: 31 October 2018</p> <p>I just bought a pair of OnePlus Bullets Wireless, which is an AptX Bluetooth Audio device.</p> <p>It paired beautifully with my OnePlus 6, but when it came to pairing it with my MacBook Pro, it didn't show up in the Bluetooth menu!</p> <p>That's because the AptX codec was not enabled in my defaults. To enable it, open a terminal and perform:</p> <pre><code>sudo defaults write bluetoothaudiod \"Enable AptX codec\" -bool true\n</code></pre> <p>And boom! OnePlus Bullets Wireless now appear in my macOS Bluetooth menu!</p>"},{"location":"blog/software-engineering/vertically-align-anything/","title":"Vertically align anything","text":"<p>Originally published: 30 September 2018</p>"},{"location":"blog/software-engineering/vertically-align-anything/#reflection-dont-use-this-anymore","title":"Reflection: Don't use this anymore","text":"<p>If you're doing things when you set <code>display</code> to <code>block</code> or <code>inline-block</code> then this might still be useful for you.</p> <p>However, I actually recommend switching to using Flexbox or Grids. Personally, I find Flexbox more powerful as Flexbox is one-dimensional and Grids is two-dimensional, so you can have Flexbox be two-dimensional by putting Flex inside Flex, which can work really nicely when dynamically handling different sized screens.</p>"},{"location":"blog/software-engineering/vertically-align-anything/#original-post","title":"Original Post","text":"<p>Taken from here</p> <p>It's a bit of CSS that will allow you to instantly vertically align a <code>&lt;div&gt;</code> within another <code>&lt;div&gt;</code>:</p> <pre><code>.element {\n    position: relative;\n    top: 50%;\n    transform: translateY(-50%);\n}\n</code></pre> <p>Note: In Firefox, you may need to set the parent element's <code>height</code>. This was discovered when using this CSS on a child of <code>body</code> to position an element in the middle of the screen;</p> <pre><code>body {\n    height: 100%;\n}\n</code></pre>"},{"location":"blog/software-engineering/what-is-homebrew/","title":"What is... Homebrew (the macOS package manager)","text":"<p>Originally published: 25 May 2019</p> <p>Homebrew is a third party, open source package manager for macOS.</p> <p>This adds a third, \"standard\" method of installing software onto machines running macOS. The other 2 methods being:</p> <ul> <li>Install from the App Store, which requires developers to pay for being part of the Apple Developer Program, as well as   require developers to get their software verified by Apple.</li> <li>Install from a <code>.pkg</code> or <code>.dmg</code> file, which doesn't require any verification or payment, but does require the software   to have an system within itself to be able to update itself; Otherwise the user has to manually download new versions   of the software every time they want to update.</li> </ul>"},{"location":"blog/software-engineering/what-is-homebrew/#how-it-works","title":"How it works","text":"<p>Upon installing Homebrew, like with other package managers, Homebrew comes with an in-built list of repositories that it can pull software from for users to easily install software onto their machine with a single command.</p>"},{"location":"blog/software-engineering/what-is-homebrew/#terminology","title":"Terminology","text":""},{"location":"blog/software-engineering/what-is-homebrew/#formulae","title":"Formulae","text":"<p>A formula (plural = formulae) is a package definition file containing the instructions for installing a piece of software onto your machine. This software is usually a program (i.e. a CLI tool).</p>"},{"location":"blog/software-engineering/what-is-homebrew/#tap","title":"Tap","text":"<p>A tap is a repository that Homebrew can pull software from.</p>"},{"location":"blog/software-engineering/what-is-homebrew/#cask","title":"Cask","text":"<p>A cask is a package definition file containing the instructions for installing a piece of software onto your machine. This software is usually an application (i.e. has a GUI)</p>"},{"location":"blog/software-engineering/what-is-homebrew/#installing-software","title":"Installing Software","text":"<p>To install software using a formula:</p> <pre><code>brew install &lt;formula&gt;[@&lt;version&gt;]\n</code></pre> <p>To add a new tap:</p> <pre><code>brew tap &lt;tap&gt;\n</code></pre> <p>To install software using a cask:</p> <pre><code>brew cask install &lt;cask&gt;\n</code></pre>"},{"location":"blog/software-engineering/what-is-homebrew/#upgrading","title":"Upgrading","text":"<p>To update the list client-side list of repositories:</p> <pre><code>brew update\n</code></pre> <p>To upgrade formulae:</p> <pre><code>brew upgrade [&lt;formuala&gt;]\n</code></pre> <p>To upgrade casks:</p> <pre><code>brew cask upgrade [&lt;cask&gt;]\n</code></pre>"},{"location":"blog/software-engineering/what-is-homebrew/#listing-installed-software","title":"Listing installed software","text":"<p>Formulae:</p> <pre><code>brew ls\n</code></pre> <p>Casks:</p> <pre><code>brew cask ls\n</code></pre>"},{"location":"blog/software-engineering/what-is-homebrew/#removing-software","title":"Removing Software","text":"<p>To remove software using a formula:</p> <pre><code>brew uninstall &lt;formula&gt;\n</code></pre> <p>To remove a tap:</p> <pre><code>brew untap &lt;tap&gt;\n</code></pre> <p>To remove software using a cask:</p> <pre><code>brew cask uninstall &lt;cask&gt;\n</code></pre>"},{"location":"blog/software-engineering/what-is-homebrew/#advanced-commands","title":"Advanced Commands","text":"<p>Fetch a non-shallow copy of all homebrew/core (homebrew/core is the core tap) formulae:</p> <pre><code>git -C \"$(brew --repo homebrew/core)\" fetch --unshallow\n</code></pre>"},{"location":"blog/software-engineering/what-is-programming/","title":"What is Programming","text":"<p>Originally published: 23 September 2016</p> <p>You're probably reading this because you've recently thought something along the lines of \"This coding or programming malarkey is getting to be quite popular, I should give it a go...or at least look into it and see if it's for me!\"</p> <p>Well, you've just made your first step into the world of programming, so congratulations! Give yourself a pat on the back! I'm not actually being sarcastic here, a lot of people find it difficult to understand the world of technology past a smart-phone or laptop etc., so well done, you've done really well to get this far.</p> <p>But now you want to know more about programming or coding. (Quick note, it doesn't really matter which term you use, neither makes you sound any more or less cool, just go with what you feel comfortable with, I tend to interchange depending on the context).</p>"},{"location":"blog/software-engineering/what-is-programming/#what-actually-is-programming","title":"What actually is programming?","text":"<p>Good Question! Programming is simply: Writing instructions (or \"code\") for a computer to do something, usually so that a human doesn't have to.</p>"},{"location":"blog/software-engineering/what-is-programming/#what-is-code","title":"What is code?","text":"<p>Code is a set of instructions written for a computer to perform (or \"execute\").</p>"},{"location":"blog/software-engineering/what-is-programming/#whats-a-programming-language","title":"What's a programming language?","text":"<p>A programming language is a language for developers to use to write their instructions (\"code\") for computers.</p> <p>Sadly, you can't just write a load of English or French or German in a text file and expect a computer to understand it all as instructions, we have to be a bit more specific because computers are dumb (seriously!).</p>"},{"location":"blog/software-engineering/what-is-programming/#what-programming-languages-do-computers-understand","title":"What programming languages do computers understand?","text":"<p>Aha! There's the million pound question! There is in fact, technically, only one answer to that question:</p> <p>Binary (or as some people call it: \"Machine Code\").</p> <p>Binary is simply a language made up of '1's and '0's. It's that simple. That's all computers can understand because computers are stupid; The quicker you get that fixed in your mind, the better!</p>"},{"location":"blog/software-engineering/what-is-programming/#so-why-do-other-programming-languages-exist-when-computers-only-understand-binary","title":"So, why do other programming languages exist when computers only understand Binary?","text":"<p>The simple answer to that is: Humans don't understand Binary very well and developers don't like programming in it.</p> <p>It's a pain to try to develop a mobile app in binary or a website in binary. It's difficult, so we don't do it and that's where other languages come in: To make it easier for developers to write code.</p>"},{"location":"blog/software-engineering/what-is-programming/#so-how-does-coding-in-these-other-programming-languages-work-if-computers-only-understand-binary","title":"So, how does coding in these other programming languages work if computers only understand Binary?","text":"<p>Well, let's do a quick history lesson that explains it all. Grab a drink, get a little comfy because this stuff is going to take a few paragraphs (but is still quite interesting).</p> <p>Note: I'm only going to cover the important stuff and add in a few interesting facts but if you want to read further into anything, feel free!</p> <p>Some clever people back in the mid-late 20th Century sat down and came up with an idea to let developers not have to write in Binary: What if developers could write their code in an understandable way (i.e. looked like English) and then get something else to convert all that code in '1's and '0's? Wouldn't that be so much easier than faffing on writing in Binary? The idea worked and the method of converting code into '1's and '0's became known as: Compiling. People even made computer programs called Compilers to do the compiling really quickly for developers.</p> <p>Compiling became hugely popular but it was still early days for programming languages and there wasn't really any standard language(s) for developers to use. There were loads of languages hanging around the place, each designed for a different use (though usually some form of mathematical or scientific purpose).</p> <p>Then a REALLY clever chap called Dennis Ritchie came along. He designed and developed a programming language called C. C was designed to be a sort of \"general purpose\" programming language for developers to use that could also compile into Binary really efficiently and HOLY CRAP it became popular. C is one of the most widely used programming languages of all time. To give you some sort of scope to that statement; you're probably running a Windows, Mac or Linux machine right now, right? Well, those Operating Systems are written in C, even the modern versions are!</p> <p>But C was only the beginning. Soon more standardized programming languages began to appear: C++, Java, C#, Python, Objective-C, Swift, JavaScript, PHP. Some simpler to code in but made slower programs, some more specialized for specific things than others but all standardized and a lot easier to code in than Binary.</p>"},{"location":"blog/software-engineering/what-is-programming/#so-what-language-should-i-learn","title":"So what language should I learn?","text":"<p>Another great question and one that only you can answer, but I can provide some help:</p> <p>If you are only just starting out at programming and/or only want to learn the basics, I would recommend learning a rather popular simple language called Python. Codecademy has a great, completely free tutorial on learning Python that I would definitely recommend (Fun fact: It's where I started programming!).</p> <p>If you're wanting to get into Web Development, I'd recommend learning JavaScript (and maybe PHP).</p> <p>If you want to make iOS apps, you'll want to learn Objective-C or Swift.</p> <p>If you want to make Android apps, you'll want to learn Java.</p> <p>If you want to make games, you'll probably want to look into learning C++.</p> <p>If you want to make developing software for servers, corporations, etc., you'll want to look into learning Java and/or C# and/or C++.</p> <p>If you want to develop Operating Systems and similar software, you'll want to learn C and C++.</p> <p>And that's it, go have fu...oh, you have another question?</p>"},{"location":"blog/software-engineering/what-is-programming/#wait-what-the-hell-are-interpreters-sdks-and-runtime-environments","title":"WAIT! What the hell are Interpreters, SDKs and Runtime Environments?","text":"<p>Ah...yes! How could I forget?! It's important stuff after all! Let's tackle each of those subjects individually.</p>"},{"location":"blog/software-engineering/what-is-programming/#interpreters","title":"Interpreters","text":"<p>Interpreters are kind of like compilers except with one fundamental difference: Where compilers take ALL of your code and compiles it before you run your program; an interpreter runs your program and then when it needs to execute some code, it compiles that bit of code and then executes it.</p> <p>This means Interpreters allow you to instantly execute your code without having to wait for a compiler to compile all of your code all at once but at the sacrifice of running your program slower because it has to compile the code as it goes along. This is great for simple programs and programs that don't need to run really quickly.</p> <p>Code written in some languages, such as JavaScript and Python, can only be executed using an interpreter (i.e. you can't compile it all at once).</p>"},{"location":"blog/software-engineering/what-is-programming/#sdks","title":"SDKs","text":"<p>An SDK is a Software Development Kit.</p> <p>Most programming languages are maintained and developed by a Software Company or Foundation. These organizations understand that developing for a programming language is hard on it's own, so they often provide a neat little package of development tools; such as pre-built code, Frameworks, Runtime Environments and more.</p> <p>These packages are called Software Development Kits and if you can get your hands on one for the language you're wanting to develop with, it's highly recommended you get it and use it.</p>"},{"location":"blog/software-engineering/what-is-programming/#runtime-environments","title":"Runtime Environments","text":"<p>With \"low-level\" (i.e. nearer to Binary) programming languages like C and C++ where a developer compiles their code and executes it, the developer has to take into account details about the machine/system the code will be executed on. For example, the developer will have to add support for every Operating System (etc.) their program might come into contact with. This can be a good thing but sometimes it's not the case.</p> <p>That's where a Runtime Environment comes in. It is essentially an Operating System for programs coded in a particular language. This provides an extra layer between the machines Operating System and the program so that the code becomes platform independent. Which is a fancy term meaning \"will run on pretty much any operating system\".</p> <p>This means a developer can write a bunch of code and it will run on Windows, Mac, Linux etc. without any need to change the code itself as long as the Runtime Environment is installed.</p> <p>The downside is that the program will run slower than it would if it was coded in a language that didn't run code in a Runtime Environment. This is because the code has to essentially compile/be executed twice, once from the developers code to the Runtime Environment and once from the Runtime Environment to the System.</p> <p>Code written in some programming languages like Java, Python and JavaScript (Node.js), require a Runtime Environment to be able to be executed.</p>"},{"location":"blog/software-engineering/build-and-deploy/","title":"Introduction","text":"<p>I'm starting a new category of blog posts that I'm calling \"Build &amp; Deploy\".</p> <p>Each post will be focusing on a different \"project type\" (Java web service, Frontend webapp, Terraform infrastructure, Java Library, etc.) and how to build and/or deploy that project type.</p> <p>I won't be covering every way of building &amp; deploying these project types. Instead, I'll be using my experience to make opinionated decisions on how I think developers should be building, deploying and running these projects. I'll endeavour to give reasons as to those decisions to provide clarity, but I'll be keeping them relatively succinct in order to stay focused on the \"Build &amp; Deploy\" stuff (expanded reasoning can be done in a separate post, if there's enough interest from me &amp; readers).</p> <p>I also will be, most likely, focusing on the \"Build &amp; Deploy\" processes of web-based technologies using the following languages (Java, JavaScript/TypeScript/ECMAScript, Terraform, Go, since that's my expertise.</p> <p>I will also, often, only be focusing on the either building or deploying - since they often separate processes. Many project types get built into the same artifact (e.g. a container image), which then means the deployment process is the same for all those project types. Rather than waste my time repeating myself, I'll instead be writing separate blog posts and linking them.</p>"},{"location":"blog/software-engineering/build-and-deploy/#build-deploy-generic","title":"Build &amp; Deploy (Generic)","text":"<p>From a very high-level point of view, the build &amp; deployment process of software works as follows:</p> <p>Building software is the process of taking code and creating deployable artifact. This is involves multiple steps:</p> <ul> <li>Code linting / scanning</li> <li>Compiling</li> <li>Running Tests</li> <li>Packaging</li> <li>Packaging again (sometimes)</li> <li>Publishing<sup>1</sup></li> </ul> <p>Deploying software is the process of taking a deployable artifact and \"deploying\" it to where it needs to be executed. This also involves multiple steps:</p> <ul> <li>Defining deployment configuration</li> <li>Pulling / pushing the deployable artifact to a machine for execution</li> <li>Initiating the execution</li> <li>Enabling usage/rollout of features, post-execution.<sup>2</sup></li> </ul> <p>and that's pretty much it!</p> <p><sup>1</sup> Depending on the project type, \"Publishing\" could be part of the Deployment process, but for most cases I'd strongly argue that publishing a deployable artifact is part of the build process.</p> <p><sup>2</sup> Strictly speaking, \"Enabling usage/rollout of features, post-execution.\" could be an entirely separate process called \"Releasing\" but that's a bit more advanced and often unnecessary</p> <p>~ Harmelodic</p>"},{"location":"blog/software-engineering/build-and-deploy/apache-beam/","title":"Build &amp; Deploy: Apache Beam projects","text":"<p>Apache Beam is an open-source programming model for batch and stream processing ( e.g. ETL processes)</p> <p>These sorts of processes tend to be embarrasingly parrallel workloads, and Apache Beam's programming model (and compatible runners) can take advantage of this to achieve exceptional performance of data processing.</p> <p>Unlike other server software where build and deployment is done via containerization of applications ( see: Build &amp; Deploy: Containers), Apache Beam projects are handled a little differently... although not as different as you might think, when deploying to Dataflow.</p> <p>This blog post covers a high-level view of how Apache Beam Java projects are built, and then deployed to run on an Apache Beam Pipeline Runner (specifically, GCP's Dataflow).</p> <p>Apache Beam has alternative SDKs (Python, Go and Scio (Scala)) and Runners (Direct, Flink, Spark, etc.) - however my experience lies primarily with Java projects and Google Cloud, so I'll be putting my focus on them. The overarching process should be about the same for SDKs &amp; Runners, but swap out the tooling.</p>"},{"location":"blog/software-engineering/build-and-deploy/apache-beam/#building-apache-beam-java-projects","title":"Building Apache Beam Java projects","text":"<p>Apache Beam projects are essentially just normal Java programs, starting from the <code>main</code> method, that construct a <code>Pipeline</code>, before calling <code>.run()</code> on that pipeline.</p> <p>Building Apache Beam projects is essentially the process of taking source code and outputting a Pipeline.</p> <p>If we're using Java then that means we're most likely using Maven or Gradle to handle our build (personally, I prefer Maven and have found Maven to be much more widely used, so I'll be putting more focus on that).</p> <p>To learn about generically building Maven projects, see Build &amp; Deploy: Maven projects.</p> <p>Tests are run in the normal way for Java / Maven projects (JUnit and Surefire/Failsafe). There are some useful documents from both Apache and Google Cloud for how to test your pipelines.</p> <p>Since Apache Beam projects just normal Java programs that run in the <code>main</code> method, that we can simply package them as a JAR. If we were going to deploy to servers with Java &amp; the needed dependencies available, then this can be done with the <code>maven-jar-plugin</code>, but if you want to keep everything together in one bundle, then you can do this with the <code>maven-shade-plugin</code>.</p> <p>Once the JAR has been built, simply push it to a registry where you can store JARs.</p> <p>In a professional setup, I would recommend having 1 central registry where you host these JARs, where developers &amp; PRs can build &amp; publish <code>SNAPSHOT</code> versions and only the <code>main</code> branch can publish JARs used for production (and production Runners should only be able to run production JARs).</p> <p>Before moving on to deploying this Pipeline: When deploying pipelines to a managed solution, like GCP's Dataflow, it is important to consider how deployment to that managed solution works, and if there are extra or different build steps that we should add to our build process.</p>"},{"location":"blog/software-engineering/build-and-deploy/apache-beam/#building-dataflow-templates","title":"Building Dataflow Templates","text":"<p>GCP recommends handling the packaging of pipelines as Dataflow templates, specifically Flex templates (\"classic\" templates are available, but the way to go is Flex templates).</p> <p>Flex templates are packaged into 2 Artifacts:</p> <ul> <li>The pipeline, as a container image (surprise, surprise)</li> <li>A template specification, as a file in Cloud Storage.</li> </ul> <p>Therefore, we need to be able to build our pipeline into a container image, publish it to a container image registry, and publish a template specification to Cloud Storage.</p> <p>In order to build a container image (rather than a JAR as we did before), we can use of the <code>maven-jib-plugin</code> ( coincidentally, also built &amp; maintained by Google Cloud).</p> <p>Once an image has been produced, we need to produce a template specification. Unfortunately, there is no build plugin available for this for Maven (or Gradle), and instead we must use a <code>gcloud</code> command, as Google instructs:</p> <pre><code># Reference: https://cloud.google.com/sdk/gcloud/reference/dataflow/flex-template/build\ngcloud dataflow flex-template build &lt;args-and-flags&gt;\n</code></pre> <p>By using this command, and supplying the <code>--image</code> flag with the container image that we have built (if you don't supply this then Google Cloud will attempt to build a fresh image using GCP Cloud Build).</p> <p>As part of this command we can also supply nearly all the necessary information for the Pipeline will need to run in Dataflow, as part of the Dataflow Template. This means that information only related to the running of the Dataflow Template can be part of the deployment/execution step (environment information and job-specific arguments).</p> <p>\u2757\ufe0f Important considerations: Don't call <code>.waitUntilFinish()</code> after calling <code>.run()</code> for Flex Templates - as per Google's comments in their samples.</p> <p>To further understand the Dataflow Pipeline lifecycle, and be prompted with sensible optimisations to consider, I highly recommend reading Google Cloud's Dataflow Pipeline lifecycle document and their Dataflow pipeline best practices document.</p> <p>In a professional setup, I recommend having 2 central container image registries (as I explain in my Build &amp; Deploy: Containers post). Similarly, I recommend having 2 template specification buckets for the same reason: One for production template specifications, one for non-production template specifications. Only production template specifications should be allowed to be used in production. This improves the security of not allowing anything not built for production to be put into production.</p>"},{"location":"blog/software-engineering/build-and-deploy/apache-beam/#deploying-apache-beam-pipelines-to-dataflow","title":"Deploying Apache Beam pipelines to Dataflow","text":"<p>When we want to deploy Apache Beam pipelines to runners, we need to ensure we have servers provisioned with \"Runner\" software that can execute Apache Beam pipelines. Rather than handle all that, we can take advantage of Cloud Providers offering managed solutions for deploying &amp; executing our pipelines - Google Cloud's Dataflow is such a product, and what I will be focusing on here.</p> <p>As mentioned above, Dataflow templates are the way to go for Google Cloud, so here we're talking about deploying Dataflow templates, specifically Flex templates.</p> <p>Assuming we already have a Flex template built (both pipeline container image and template specification) then the only thing left to do is inform Dataflow that we wish to run that template, which will create a Dataflow \"job\" that executes the pipeline on GCP's Dataflow runners.</p> <p>You can do this by simply calling the Dataflow API. This can be done from code, from the <code>gcloud</code> CLI, or via a Cloud Scheduler operation (built-into Dataflow Pipelines). For example, to call using the <code>gcloud</code> CLI, perform:</p> <pre><code># Reference: https://cloud.google.com/sdk/gcloud/reference/dataflow/flex-template/run\ngcloud dataflow flex-template run \"&lt;job_name&gt;\" \\\n --template-file-gcs-location \"&lt;template-specification-file-in-gcs&gt;\" \\\n --region \"&lt;region&gt;\" \\\n &lt;further-args-and-flags&gt;\n</code></pre> <p>\u2757\ufe0f Note: We don't specify the container image in this call, because the image is referenced inside the template specification file.</p> <p>~ Harmelodic</p>"},{"location":"blog/software-engineering/build-and-deploy/containers/","title":"Build &amp; Deploy: Containers","text":"<p>Containers are the output of software going through Containerization.</p> <p>Important disambiguation:</p> <ul> <li>A container image is the deployable artifact that contains the software that you wish to execute.</li> <li>A container is an actively-executing instance of that container image.</li> </ul> <p>The term \"containers\" is sometimes used as an umbrella term for both container images and containers (as I have done in the title of this blog post). I will endeavour to not do that in the rest of the blog post.</p> <p>This post mainly focuses on the deployment of container images, with a brief section on how to generically build a container image.</p> <p>\u2757\ufe0f Note: When containers became a huge deal in the 2010s, most people started packaging their software into Docker container images, as the Docker company were industry-leaders at the time and thus defined the standards for building container images and running containers. Due to a variety of reasons, the industry has begun shifting to using the Open Container Initiative (OCI) standard for building and interacting with container images &amp; containers. Even Docker (the company) now provides the tooling/options to build OCI container images.</p>"},{"location":"blog/software-engineering/build-and-deploy/containers/#building-container-images","title":"Building Container Images","text":"<p>To package an application, we should write some code to define how the packaging should be done, and then be able to run a command that executes that code.</p> <p>There are a variety of ways to build a container image, for different project types. For those more \"specialised\" methods, I will cover those in dedicated blog posts for those project types. I recommend using those over the more \" generic\" build method described below.</p> <p>For generically packing applications into container images, we write our instructions in a Dockerfile and then run a CLI tool such as <code>docker</code> or <code>podman</code> to take that <code>Dockerfile</code> and build the image.</p> <p>Container images are packaged into layers. When we want to build a container image, 99% of the time we will take an existing layer and then write our own layer on top of that, which does the containerization of our application. In a <code>Dockerfile</code> that is done by defining a <code>FROM</code> command.</p> <p>After the <code>FROM</code> command, we can define lots of different commands that perform the tasks to package your application into a container image. For example, if you had a folder in your repo called <code>public</code> that contained files for hosting on a NGINX webserver, and had a custom NGINX config file defined as <code>nginx.conf</code>, you might write a Dockerfile that looks something like:</p> <pre><code>FROM nginx:1.27.0\n\nCOPY public/ /usr/share/nginx/html/\nCOPY nginx.conf /etc/nginx/conf.d/default.conf\n</code></pre> <p>For security (and efficiency) reasons, I recommend using a distroless container image as the image you define in your <code>FROM</code> command.</p> <p>Once you have your Dockerfile defined, you can build your container image by using a CLI tool, for instance with podman:</p> <pre><code>podman build -t my-image:1.0.0 .\n</code></pre> <p>This will look in <code>.</code> (current directory) for a <code>Dockerfile</code>, and then build your image on the local machine, naming it <code>my-image</code> and tag with version <code>1.0.0</code> - Versioning can depend on your use case, but generally I recommend sticking to Semantic Versioning.</p> <p>In order to publish your image, you need to push it to a container image registry (such as Dockerhub or GCP's Artifact Registry). You'll need to do some preconfiguration &amp; special tagging in order to prepare your image for pushing to the registry of your choice, but once done, it should be as simple as then running:</p> <pre><code>podman push &lt;image&gt;:&lt;tag&gt;\n</code></pre> <p>When setting up a container image registry for a organisation's container images, I recommend setting up at least 2 container image registries. One for container images that are to be deployed to production environments, and one for all other container images (dev-built images). Doing this allows you to do some sensible security-hardening:</p> <ol> <li>Ensure only specific parts of your build systems are allowed to push to the container image registry for production    images.</li> <li>Ensure your container runtime system (see below) is allowed to pull from the container image registry for production    images.</li> </ol> <p>This ensures that no unauthorized images can be run in production environments.</p> <p>Alternatively, if you are only running 1 environment (production) then you can have 1 container image registry, and developers/PRs/etc. don't &amp; cannot push to it.</p>"},{"location":"blog/software-engineering/build-and-deploy/containers/#deploying-container-images","title":"Deploying Container Images","text":"<p>There are multiple ways to deploy container images and run containers.</p> <p>Most use-cases for running containers involves running them on servers. That is what I'll be focusing on in this post, in which case there are basically two options available to you:</p> <ol> <li>Use Kubernetes to \"orchestrate\" your containers on a cluster of servers, with continous    deployment being handled by a GitOps continuous delivery (CD) system such    as Argo CD or Flux CD.</li> <li>Run your container in a \"serverless\" solution such as GCP's Cloud Run,    AWS's App Runner, or    Azure's Container Apps, where deployment is    handled via some sort of infrastructure as code (e.g. Terraform).</li> </ol> <p>There are absolutely other alternatives to running containers (docker compose on a VM, just running containers manually in VMs, cloud-provider-specific container-orchestration systems) but ultimately the industry standard solution for managing containers is Kubernetes, and the serverless solutions exist for when you don't want to or need to actively manage a cluster of servers yourself.</p> <p>From here I will describe two solutions, and the solutions I actually recommend:</p> <p>Kubernetes with Argo CD for deploying a lot of containers to a cluster of servers, and fine-control over those deployed containers is readily availble.</p> <p>GCP's Cloud Run, with Terraform for quickly deploying a container with basic needs.</p>"},{"location":"blog/software-engineering/build-and-deploy/containers/#kubernetes-with-argo-cd","title":"Kubernetes with Argo CD","text":"<p>To run containers on servers, you need to have servers. These servers should probably be clustered together, so that you can interact with them as one entity rather than as separate entities. You should be able to interact with those servers in a nice way in order to run containers on them and scale those containers, if needed. It'd be nice if those servers recognised if one of the containers was failing and thus automatically restarted it or took the container out of being active in the network. Speaking of networks, it'd be nice if the containers were networked together in a nice way, and could interact and communicate with one another easily.</p> <p>All of that work can be described as \"container orchestration\".</p> <p>Kubernetes is a \"container orchestration\" system, for automating the deployment, scaling and management of containers.</p> <p>You can run containers on all sorts of machines and clusters of machines, and using Cloud Providers. Different Cloud Providers provide \"managed\" versions of Kubernetes, where you don't even need to worry about how to upgrade to new versions of Kubernetes or manage the Kubernetes Control Plane servers - e.g. GCP's Google Kubernetes Engine (GKE).</p> <p>Once you have a GKE cluster provisioned, you need to be able to deploy containers to that cluster.</p> <p>Initially, this is possible by simplying applying \"Kubernetes resources\" to your Kubernetes server. These Kubernetes resources are simply configuration that the Kubernetes system will use in order to run and manage your containers. The main resource you will likely need to do use is a Deployment, though other sorts of \"workload\" resources are available. If your container needs to be communicated with then other Kubernetes resources that handle cluster networking &amp; communication can be used, such as the Service and Ingress resources. Further configuration and advanced configuration can be found in the Kubernetes reference documentatation.</p> <p>Manually deploying these resources can be a pain. Automation to the rescue! You could store your Kubernetes resources in a Git repo and then setup automation stuff (e.g. in GitHub Actions) to apply those resources to your Kubernetes cluster, but... what if you delete a Kubernetes resource from your Git repo? How do you ensure that it is removed from your Kubernetes clsuter? Or what if you haven't made a change in a while, and one of the resources gets deleted for some reason (hacking/accident)? How can we ensure that the resource is recreated? It'd be better to have a system in place that continuously checks that all the Kubernetes resources in your Git repo are applied to your Kubernetes cluster. This sort of thinking is called \"GitOps\", because we're handling our Operations through Git.</p> <p>Argo CD is a GitOps continuous delivery tool for Kubernetes. After being setup and configured to look at your Git repositories and Kubernetes cluster(s), it will ensure that the two are in sync, always taking the Kubernetes resources in your Git repository as the source of truth.</p> <p>Argo CD uses its Application resource in order to evaluate where collections of Kubernetes resources are found in order to deploy systems. Since the Application resource is a type of Kubernetes resource, you can build a tree of Applications, being sourced from other Applications. This is referred to as the app of apps pattern.</p> <p>Argo CD itself runs on Kubernetes, and can be installed via a Helm Chart. When building new Kubernetes systems, I tend to provision Kubernetes with Terraform code, and then install the Argo CD Helm Chart via Terraform, the deploy a root Argo CD application via Terraform, and let Argo CD then handle the deployment from there.</p> <p>Wait... Helm Chart? What's that???</p>"},{"location":"blog/software-engineering/build-and-deploy/containers/#helm-charts-kustomize","title":"Helm Charts &amp; Kustomize","text":"<p>Often a system your building will be made up of multiple Kubernetes resources, possibly even multiple containers and deployments. You might even need to deploy all these resources to different environments, and so don't want to copy-paste the resources all over the place.</p> <p>Therefore bundling these resources together into a single deployable entity becomes a nice way of maintaining your Kubernetes resources. There are two ways to bundle your Kubernetes resources together:</p> <p>A Helm Chart, which packages together \"templates\" of a collection of Kubernetes resources, which you can then inject values into those templates as needed for an environment.</p> <p>A Kustomize Kustomization, which packages together the collection of Kubernetes resources as a \"base\" set of resources, which then allows for \"overlays\" to customise &amp; patch the base set of resources as needed for an environment.</p> <p>If you're building and running your own applications and not providing them to others, then Kustomize is a better solution, where each overlay corresponds to an environment you need to run in. This keeps the Kubernetes resources simple and easy to read &amp; maintain as the need for heavy customisability will be low, because you control the context of your applications.</p> <p>If you're building a system for other people to use on their Kubernetes setups (e.g. an open-source project), then a Helm Chart is a better option, as it can be customized, versioned and shipped as its own entity. Publicly available Helm Charts are available on Artifact Hub.</p> <p>\u2757\ufe0f Note: Helm Charts have recently started supporting being packaged according to the OCI standard - meaning you can start treating Helm Charts like any other OCI image and storing them in an OCI image registry (like container images).</p> <p>\u2757\ufe0f Note: Helm (the CLI tool that can install &amp; manage Helm Charts) has its own way to managing the lifecycle &amp; upgrading of a Helm Chart. Some CD systems like Flux CD support and work with this lifecycle process, others like Argo CD do not and simply evaluate what a Helm Chart would create and then creates those resources. I prefer the Argo CD way, but if you want to use Helm's lifecycle, then Flux CD would be a better option for you.</p>"},{"location":"blog/software-engineering/build-and-deploy/containers/#cloud-run-with-terraform","title":"Cloud Run with Terraform","text":"<p>Cloud Run is a \"serverless\" solution for running containers - which basically means that rather than setting up servers/VMs and installing software on them to be able to run your container, you instead just provide a Cloud provider (in this case GCP) some basic deployment configuration that defines what containers you'd like to run and how, and the Cloud provider will do the rest.</p> <p>Incidentally, in the case of Cloud Run, Google are basically just running Kubernetes behind the scenes.</p> <p>I won't go into depth on how to best handle deploying/executing Terraform code, as I'll leave that for another blog post.</p> <p>To deploy something to Cloud Run via Terraform, you should define your Cloud Run deployment configuration in Terraform code using the google_cloud_run_v2_service resource. Then apply your Terraform code!</p> <p>You can read more about doing further configuration of your Cloud Run deployment, in the Cloud Run docs.</p> <p>~ Harmelodic</p>"},{"location":"blog/software-engineering/build-and-deploy/maven/","title":"Build &amp; Deploy: Maven projects","text":"<p>Apache Maven, or just Maven, is a build tool for Java projects.</p> <p>In this Build &amp; Deploy post, I'll obviously not be talking about Deploy at all, but instead be covering the finer details of generically building Java projects with Maven, including some recommendations for plugins and POM management.</p>"},{"location":"blog/software-engineering/build-and-deploy/maven/#building-maven-projects","title":"Building Maven projects","text":"<p>A normal Maven project is structured in the following way:</p> <pre><code>src/\n  main/\n  test/\n  site/\ntarget/\npom.xml\n</code></pre> <p><code>src/main</code> if for your code. <code>src/test</code> is for your tests. <code>src/site</code> is for the website related to your project (can probably ignore it). <code>target</code> is the output directory where artifacts are placed. <code>pom.xml</code> is the Maven configuration file.</p>"},{"location":"blog/software-engineering/build-and-deploy/maven/#the-pom-file-structure","title":"The POM file structure","text":"<p>The POM file is the main configuration file for your project - no need to have several configuration files with their own config system, everything goes in the POM file.</p> <p>The POM file has a specific structure, which features a lot of different configuration options, but for the most part you'll probably just be using the following:</p> <pre><code>\n&lt;project&gt;\n    &lt;modelVersion&gt;...&lt;/modelVersion&gt;\n\n    &lt;parent&gt;...&lt;/parent&gt;\n\n    &lt;groupId&gt;...&lt;/groupId&gt;\n    &lt;artifactId&gt;...&lt;/artifactId&gt;\n    &lt;version&gt;...&lt;/version&gt;\n    &lt;packaging&gt;...&lt;/packaging&gt;\n\n    &lt;properties&gt;...&lt;/properties&gt;\n\n    &lt;dependencyManagement&gt;...&lt;/dependencyManagement&gt;\n\n    &lt;dependencies&gt;...&lt;/dependencies&gt;\n\n    &lt;build&gt;...&lt;/build&gt;\n&lt;/project&gt;\n</code></pre> <p><code>modelVersion</code> is the Maven model version to use. As of 2024, you should use <code>4.0.0</code>.</p> <p><code>parent</code> is used to define if this POM inherits its configuration from another POM.</p> <p>The <code>groupId</code>, <code>artifactId</code>, <code>version</code> elements are for basic project information, and should be self-explanatory ( check the POM reference for more info).</p> <p><code>packaging</code> is another basic project element, which defines how we will be packaging this project into it's final form - the default is <code>jar</code>. The setting of this element results in Maven configuring reasonably sensible defaults for the build process for that packaging type.</p> <p><code>properties</code> are basically just variables to use in your POM. I recommend minimising the usage of these and use them exclusively for inputs to your build process or setting properties that dependencies &amp; plugins use. Setting dependency versions in properties is a weird common practice that I've found makes maintenance more difficult.</p> <p><code>dependencyManagement</code> is for pulling in specific dependencies (other POMs) that will manage other dependencies for you (i.e. which versions to use).</p> <p><code>dependencies</code> is the section where you define the dependencies (libraries, frameworks, etc.) you actually want to use for your Java project. If these dependencies are managed by another POM (either <code>parent</code> or <code>dependencyManagement</code> dependency) then you don't have to supply versions (helpful for maintenance).</p> <p><code>build</code> is to define what plugins to use in the build process.</p>"},{"location":"blog/software-engineering/build-and-deploy/maven/#maven-lifecycles","title":"Maven Lifecycles","text":"<p>Maven has 3 built-in lifecycles that we work with, made up of phases.</p> <p>The first lifecycle to know is the <code>default</code> lifecycle, which is as follows:</p> <ul> <li><code>validate</code> - validate the project is correct and all necessary information is available</li> <li><code>compile</code> - compile the source code of the project</li> <li><code>test</code> - test the compiled source code using a suitable unit testing framework. These tests should not require the   code be packaged or deployed</li> <li><code>package</code> - take the compiled code and package it in its distributable format, such as a JAR.</li> <li><code>verify</code> - run any checks on results of integration tests to ensure quality criteria are met</li> <li><code>install</code> - install the package into the local repository, for use as a dependency in other projects locally</li> <li><code>deploy</code> - done in the build environment, copies the final package to the remote repository for sharing with other   developers and projects.</li> </ul> <p>N.B. 1: Ignore the semantic meaning of <code>deploy</code> here, just treat it as a \"publish\" phase.</p> <p>N.B. 2: There are more phases to the lifecycle, but it's likely you'll never need to use or interact with them.</p> <p>A neat thing about the <code>default</code> Maven lifecycle, is that if you run a phase, say <code>mvn package</code>, it will run all the phases up to and including the <code>package</code> phase. Better yet, when configuring Maven plugins, we can simply hook the plugin into the lifecycle (if it doesn't hook itself). This makes creating a CI pipelines dead easy with Maven, since you never need to worry about running custom commands in CI, instead you just run the lifecycle phase you want to run (plus some properties).</p> <p>The second lifecycle is the <code>clean</code> lifecycle, which just usually empties the <code>target</code> directory in your Maven project ( where artifacts such as reports &amp; JARs are placed).</p> <p>The final lifecycle is the <code>site</code> lifecycle, which I've basically never used nor seen used in practice, so we'll ignore that.</p>"},{"location":"blog/software-engineering/build-and-deploy/maven/#build-recommendations","title":"Build Recommendations","text":"<p>This section pretty much exclusively focuses on the <code>build</code> section of the POM file, for the <code>default</code> lifecycle.</p> <p>A build is made up of plugins. Each plugin has associated \"goals\". A goal is basically just some build code that will run and do something. We can run goals either directly or bind them to a lifecycle phase.</p> <p>The following plugins are very useful:</p> <ul> <li><code>maven-pmd-plugin</code>: for validating code style/best-practices using PMD.</li> <li><code>maven-compiler-plugin</code>: for compiling code.</li> <li><code>maven-surefire-plugin</code>: for running unit &amp; some integration tests.</li> <li><code>maven-failsafe-plugin</code>: for running some integration/integrated tests.</li> <li><code>maven-jar-plugin</code>: for packaging into normal JARs.</li> <li><code>maven-assembly-plugin</code>: for packaging into different types of artifacts, including JARs, which include their   dependencies by extracting classes from dependency JARs (can cause class name conflicts)</li> <li><code>maven-shade-plugin</code> for packaging into \"Uber-JARs\" by renaming dependency packages (preventing class name conflicts).</li> <li><code>maven-install-plugin</code>: for installing JARs into your local Maven repository.</li> <li><code>jib-maven-plugin</code>: for packaging into Container Images.</li> <li><code>maven-deploy-plugin</code>: for publishing artifacts.</li> <li><code>spring-boot-maven-plugin</code>: for packaging/running Spring Boot projects.</li> </ul> <p>For different packaging types, Maven will bind specific plugins' &amp; goals to the lifecycle phases by default. For example, for <code>jar</code> projects (the default), Maven will bind the following (and more):</p> <ul> <li><code>maven-compiler-plugin</code>'s <code>compile</code> goal to the <code>compile</code> phase.</li> <li><code>maven-surefire-plugin</code>'s <code>test</code> goal to the <code>test</code> phase.</li> <li><code>maven-jar-plugin</code>'s <code>jar</code> goal to the <code>package</code> phase.</li> <li><code>maven-install-plugin</code>'s <code>install</code> goal to the <code>install</code> phase.</li> <li><code>maven-deploy-plugin</code>'s <code>deploy</code> goal to the `deploy phase.</li> </ul> <p>For any Java project, I recommend configuring <code>maven-pmd-plugin</code>'s <code>check</code> goal to the <code>validate</code> phase.</p> <p>For other different Java/Maven project types, other plugins can be used and bound to different phases. I'll recommend those in more specific Build &amp; Deploy blog posts.</p>"},{"location":"blog/software-engineering/build-and-deploy/maven/#ci-recommendations","title":"CI Recommendations","text":"<p>I recommend running <code>mvn clean verify</code> in Pull Requests, though depending on your project &amp; testing strategy, you could just run <code>mvn clean test</code> or <code>mvn clean package</code>.</p> <p>I recommend running <code>mvn clean deploy</code> when wanting to publish, either on merge to <code>main</code> or after tagging with a version (depending on your project and desired publishing process).</p>"},{"location":"blog/software-engineering/build-and-deploy/maven/#pom-organisation-recommendations","title":"POM Organisation Recommendations","text":"<p>Having everything is a single POM is fine for a couple of projects, but as you or your organisation starts building multiple projects, it soon becomes painful to maintain individual POMs.</p> <p>The recommended solution to this is to use <code>parent</code> POMs and <code>dependencyManagement</code> POMs ( aka BOM POMs) to offload most of the dependency management and build process. This leaves normal Maven projects to just define a parent and BOMs to use and their dependencies.</p> <p>I recommend organising this POM hierarchy as the following:</p> <ul> <li>A Root Parent POM which defines dependencyManagement, dependencies and build configuration for all projects.</li> <li>A Project Parent POM per project type (e.g. library, web-service, desktop-app, etc.) which inherits from the Root   Parent POM.</li> <li>Use off-the-shelf BOMs, e.g. <code>junit-bom</code>, <code>spring-boot-dependencies</code>, <code>spring-cloud-gcp-dependencies</code>, and configure   them in the relevant parent POM.</li> <li>Define your own BOM only if you have a LOT of internal libraries and want to co-ordinate them all together, and then   configure it in the relevant parent POM (usually just the Root Parent POM).</li> </ul> <p>Projects then just inherit from the relevant Project Parent POM, and everything should work out of the box.</p> <p>Enjoy using Maven!</p> <p>~ Harmelodic</p>"},{"location":"blog/windows-10/1-introduction/","title":"Introduction","text":"<p>Originally published: 28 May 2016</p> <p>Well it's that time again when a new version of Windows has arrived. This time: Windows 10.</p> <p>Which is oddly named in one aspect because we've gone from 8.1 to 10 but I guess 8.1 was the 9th version of Windows...kind of. Well, it doesn't really matter anyway what name it has. All that matters is the Operating System (OS) itself.</p> <p>First off, Upgrading to Windows 10 has been the most hilarious, frustrating, disappointing launch I have seen in a rather long time (and I've monitored World of Warcraft launches). It seems upgrading is the buggiest piece of crap and downloading it isn't that much better. So this blog post will cover nice neat step by step instructions of how to actually get the god damn OS on your computer/laptop. Then, in another blog post, I shall go through configuring it ( hopefully a simpler job but knowing Microsoft, it's going to be a pain) and how to get all the bloatware and crap that you don't want that comes pre-installed with the OS.</p> <p>Second, I'd like to list the different version of Windows 10 available and which one YOU should get.</p> <ul> <li>Windows 10 Home - Standard version of 10.</li> <li>Windows 10 Home N - Same as the Home version but without a media player (For European countries to obide   anti-competitive laws)</li> <li>Windows 10 Home KN - Same as standard version but for South Korean users (also removes some media capabilities)</li> <li>Windows 10 Home Single Language - Same as standard version but only supports 1 language (which you choose)</li> <li>Windows 10 Pro - Same as standard version but includes some Business features (such as Remote Desktop, BitLocker and   Client Hyper-V)</li> <li>Windows 10 Pro N - Same as Pro version but without a media player (For European countries to obide anti-competitive   laws)</li> <li>Windows 10 Enterprise - Version for business purposes</li> <li>Windows 10 Education - Version for educational purposes</li> </ul> <p>Users using the following will upgrade to Windows 10 Home:</p> <ul> <li>Windows 7 Starter</li> <li>Windows 7 Home</li> <li>Windows 7 Home Premium</li> <li>Windows 8</li> <li>Windows 8 with Bing</li> <li>Windows 8.1</li> <li>Windows 8.1 with Bing</li> </ul> <p>Users using the following will upgrade to Windows 10 Pro:</p> <ul> <li>Windows 7 Professional</li> <li>Windows 7 Ultimate</li> <li>Windows 8.1 Pro</li> <li>Windows 8.1 for Students</li> </ul> <p>Note: If you use Windows RT you will get a Windows update that includes some Windows 10 features but that's it.</p> <p>Some more information about versions can be found at: https://www.microsoft.com/en-us/WindowsForBusiness/Compare</p> <p>SO, HOW DO I GET IT?!</p> <p>Well it kind of depends. Which is not the answer that you want to hear, I know, but bear with me. There are 2 ways to get it (that I know of):</p> <ol> <li>Download the .ISO image of Windows 10, burn it to a disk or USB and install it manually (as a clean slate or    dual-boot).</li> <li>Upgrade from your existing version of Windows (only applicable to users of Windows 7 and above)</li> </ol> <p>In the following blog posts, I'll explain how to do each of these.</p> <p>~Harm</p>"},{"location":"blog/windows-10/2-downloading-installation-media/","title":"Downloading installation media","text":"<p>Originally published: 28 May 2016</p> <ol> <li> <p>Download the Windows 10 Media Creation Tool    This is the little tool that's going to get you a .ISO version of Windows 10 (A .ISO file kind of like a .zip file.    It's an archive of ALL the files you need to install an Operating System, which in this case is Windows 10).</p> <ol> <li> <p>To get the Windows 10 Media Creation Tool, you need to go to the following web address (URL): https://www.microsoft.com/en-us/software-download/windows10</p> </li> <li> <p>Now scroll down to the bottom, and you should see 2 purple buttons.    \"Download Tool Now (32-bit version)\"    \"Download Tool Now (64-bit version)\"</p> </li> <li> <p>Click on the one that applies to you.    If you don't know your System type (32-bit or 64-bit), you can find it at:    (For Windows 7 users) Start &gt; Computer &gt; System Properties    (For Windows 8 users) Right-side Panel Menu &gt; Settings &gt; Change PC Settings &gt; PC and devices &gt; PC Info</p> </li> </ol> </li> <li> <p>Going through the Windows 10 Media Creation Tool</p> <ol> <li> <p>Once it's downloaded. Open your downloads folder or wherever you saved the Tool and find the file.    It should be called either:    MediaCreationToolx64.exe    or    MediaCreationTool.exe</p> </li> <li> <p>A window will pop up. Select \"Create installation media for another PC\" and click Next.</p> </li> <li> <p>Select your relevant Language, Edition and Architecture and Click Next.    Language - You should know this.    Edition - We talked about it earlier/above.    Architecture - This will be your System Type, just use the one that's the same as your current system.</p> </li> <li> <p>Now, I recommend burning an ISO file to a DVD and storing it somewhere safe so you always have a physical copy of    your original installation disc.    However, if you do not have a Disc Drive or you really want to use a USB, then you can use a USB, but it MUST be    at least 3GB big and COMPLETELY EMPTY.    For \"ISO file\" users:    ISO-i: Select ISO file and click Next.    ISO-ii: A Window will pop up, and you can choose where to save the .ISO file. I recommend the Desktop.    ISO-iii: Once you click \"Save\", the file will start downloading.    ISO-iv: It's going to take a LONG time, so just leave it overnight to download. You can use your computer whilst    it's downloading but be careful not to crash your computer as you'll lose all your download progress, the .ISO    file will be corrupted, and you'll have to start again.    For \"USB flash drive\" users:    USB-i: Select USB flash drive and click Next.    USB-ii: Insert your USB drive into your computer/laptop. Click Refresh drive list once your USB drive has been    detected.    USB-iii: Click on the USB drive you want to install the Windows Installation on and click Next and the download    will start.    USB-iv: It's going to take a LONG time, so just leave it overnight to download. You can use your computer whilst    it's downloading but be careful not to crash your computer as you'll lose all your download progress, and you    will have to wipe your USB drive and start all again.</p> </li> <li> <p>Now if your went through the USB route, you all done! However, .ISO file users: You need to do a few more simple    steps.</p> </li> <li> <p>(For ISO file users only)    i) Now put a blank DVD of at least 4GB (Most are 4.7GB) into your computer.    ii) Once it's been detected, find your .ISO file, right-click it and click \"Burn disc image\".    iii) A window will pop up. Select your Disc burner (your disc drive), check the box \"Verify disc after burning,    and click Burn.    iv) It will then proceed to burn the .ISO image to your DVD. It shouldn't take too long, usually around 5    minutes, so go make a cup of tea.    v) Once it's all done, it will open your Disc drive and present you with a newly burnt DVD with a Windows 10    installation on it. Take it out and close your disc drive.</p> </li> </ol> </li> </ol> <p>~Harm</p>"},{"location":"blog/windows-10/3-getting-your-product-key/","title":"Getting your product key","text":"<p>Originally published: 28 May 2016</p> <p>The first thing that you'll want to do before installing or upgrading to Windows 10 is to get your product key or \" activation key\". This key is how you verify to Microsoft that you have a valid copy of Windows. Even pirated copies of Windows need to be activated so chances are if you have a pirated copy of Windows, you'll have a product key too.</p> <p>Now, if you bought a physical copy of Windows and if you still have the original box, with all it's contents, of your copy of Windows then this entire step is easy. Just go get your box and somewhere in the box will be your product key.</p> <p>If you bought an online copy of Windows then you should be able to find your product key in your confirmation email.</p> <p>If you don't have either of the above then we're going to get your product key from your Operating System. To find the product key the easiest (and assured by me, the safest) way, we need to put a Visual Basic (or VB) script on your computer that we can run which will then retrieve the product key from your system. To do this, do the following steps:</p> <p>Copy the following code from this script: https://github.com/Harmelodic/Script_Library/blob/master/Windows/GetProductKey.vbs</p> <p>Open Notepad (or an equivalent program) and paste the code inside.</p> <p>Save the file somewhere (I suggest the Desktop) as \"productkey.vbs\". The \".vbs\" extension is VERY important as this tells Windows that the file is a Visual Basic Script.</p> <p>When you want to view your product key, just double-click the file to run it and a box will pop up showing you your product key.</p> <p>~Harm</p>"},{"location":"blog/windows-10/4-installation/","title":"Installation","text":"<p>Originally published: 28 May 2016</p> <p>To perform a clean installation of Windows 10 you need the following:</p> <ul> <li>Your product key. (optional)</li> <li>A copy of Windows 10 on a DVD or USB</li> <li>An empty partition/hard drive to install Windows 10 on. (Unless you plan on wiping ALL your files/data and starting   fresh)</li> </ul> <p>To get the first 2, you can find instructions in my previous blog posts. The 3rd you have to sort yourself (I'd advise googling how to partition your hard drive, or you can just buy and fit in a new hard drive).</p> <p>Before we start, my advice is to bring up this blog on your phone or print it out and continue to read the instructions. You got the instructions off your computer now? Awesome, let's do this thing:</p> <ol> <li> <p>Turn off your computer.</p> </li> <li> <p>Now we're going to turn your computer back on, but we need to know something first:    We need to make sure that your computer reads your Disc drive/USB drive first before booting from your hard drive. To    do this, we need to enter \"The BIOS\". Which sounds terrifying and used to be but nowadays, it really isn't.    You'll notice when you boot your computer, a logo appears before ANYTHING else (FUN FACT: This is the manufacturer of    your motherboard). For me, it's: ASUS.    Anyway, when this logo appears, there should be some writing at the bottom or top of the screen saying \"Press F9 to    enter the BIOS\" or something like that. The button to press varies from brand to brand. For me, it's \"DEL\" but I    believe industry standard is F9.    So when that logo appears, Hit that button and enter the BIOS! You should have about 2 to 3 seconds to hit the    button, so act quickly.</p> </li> <li> <p>Now we're in the BIOS, comes the mildly tricky part.    We want to find were it saying \"Boot Order\" or \"Boot Menu\" or something like that. Depending on your BIOS screen this    could be anywhere, but it tends to be on the main screen that initially displays. It should look like a list or a    flow-chart diagram of some sort.</p> </li> <li> <p>Once you've found the Boot Order, we need to make sure that your CD Drive/USB Drive is the first thing to boot, then    your hard drive after that. If it's not in the correct order, change it. If it is, then you don't need to change    anything.</p> </li> <li> <p>Navigate your way back to the main screen and exit the BIOS (usually an exit button at the top right or an option in    the menu).    You should go onto to normally booting into your operating system.</p> </li> <li> <p>Now we have your boot order set in place, and your computer is turned on. Put in your Windows DVD/USB Drive and turn    off your computer.</p> </li> <li> <p>Reboot your computer. You should see, shortly after you the initial logo disappears, it states on the screen: \"Press    any key to boot from CD drive\" or something similar if you're booting from a USB drive. Hit any key, and we'll be    taken to the Windows Installation Setup. You'll have about 4 or 5 seconds before this option disappears and your    computer boots normally.</p> </li> <li> <p>The screen should go purple and the Windows Setup will start.    Select your \"Language to install\", \"Time and currency format\", and \"Keyboard or Input method\".    Click \"Next\".</p> </li> <li> <p>Click Install now. The window will disappear and the words \"Setup is starting\" will appear.</p> </li> <li> <p>\"Setup is starting\" will disappear and a window will appear to enter your product key. Enter your product key. Click     next (You can skip this process if you want to enter it later).</p> </li> <li> <p>The Licence terms will appear, check the box detailing that you accept the licence terms and Click Next.</p> </li> <li> <p>A window will appear giving you 2 options to Upgrade or Custom. Seeing as we want to perform a clean install, click     Custom.</p> </li> <li> <p>This window is where we decide where we want to install Windows.     If you don't have an empty partition select some unallocated space and create a new partition to install it on.     Once you have an empty partition to install Windows 10 on, select it and click Next.</p> </li> <li> <p>Now you'll start installing Windows 10 on your selected hard drive. It shouldn't take too long.     NOTE: You're computer will reboot itself a couple of times.</p> </li> <li> <p>After a couple of reboots, you should get a screen titled \"Get going fast\".     NOTE: If you skipped entering your product key, you'll get a screen to enter your key now, you can skip this again     which will take you to the \"Get going fast\" screen.</p> </li> <li> <p>If you want to be smart; click \"Customise Settings\". DO NOT click, \"Express Settings\", it lets Microsoft pry A LOT     into your private stuff.</p> </li> <li> <p>Turn OFF everything in 'Personalisation' and 'Location', click Next.</p> </li> <li> <p>Turn OFF everything in 'Browser and protection' and 'Connectivity and error reporting', remember there is a scroll     bar so turn off EVERYTHING. Click Next.</p> </li> <li> <p>That's your Computer Settings customized and your privacy secured a bit better.     You'll get a loading screen and then a \"Who owns this PC?\" screen.</p> </li> <li> <p>Pick the option that applies to you. I'm picking \"I own it\".</p> </li> <li> <p>Next you'll have an option to enter your Microsoft Account. I don't really care what you do here, but I'm clicking     \"Skip this step\". You probably should do too.</p> </li> <li> <p>We'll have another quick loading screen, and then we need to create our first user for Windows 10.</p> </li> <li> <p>Enter your chosen Username, Password and Password hint. Click Next.</p> </li> <li> <p>Now we get some stupid screens saying \"Hi\", \"We're setting things up for you\" and \"This won't take long\" etc.     Basically this is Microsoft's fancy loading screen.     Just leave it to do its thing.</p> </li> <li> <p>We get to the Desktop and YOU'RE DONE! Enjoy Windows 10. I'd advise you reading my \"Configuring Windows 10\" blog     post.</p> </li> <li> <p>By the way: Normally you'd have to install at least an Ethernet Controller so you can start using the internet but     Windows 10 doesn't need this.</p> </li> </ol> <p>FINAL NOTE: If you want to quickly install LOADS of programs at once, there is a great website called www.ninite.com for that.</p> <p>~Harm</p>"},{"location":"blog/windows-10/5-upgrading-from-windows-7-8-or-8.1/","title":"Upgrading from Windows 7, 8 or 8.1","text":"<p>Originally published: 28 May 2016</p> <p>First off, you need to update your Windows system to the most recent version.</p> <p>To do this, go to Start &gt; Control Panel &gt; Windows Update</p> <p>Now click \"Check for updates\" and start upgrading your system. You may have to do this several times and restart your computer several times if your system is quite out of date.</p> <p>At some point, you'll receive a blue notification in the bottom right saying that your download is complete and Windows is ready to upgrade.</p> <p>If you click on the little Windows icon then the \"Get Windows 10\" window. Click \"OK, let's continue\".</p> <p>You'll then get a loading screen saying \"Working on it...\". This will be there for a minute or two. It will then disappear and a new window will appear called \"Windows Update\" saying \"Great, we'll get the upgrade started\". Click Accept.</p> <p>We'll then get a loading bar and then the window will disappear. A new window will appear asking whether you want to start the upgrade now or later. You can wait until later but I'm impatient so let's do it now! Click Start the upgrade now.</p> <p>The screen will fade to a purple screen saying \"Restarting...\". It will then fade to saying \"Configuring update for Windows 10\" with a percentage counter. The configuration process will take a little time so go make yourself a cup of tea. It'll then change to say \"Restarting\" and then your PC will restart.</p> <p>We'll then get a big black screen with a circular loading bar titled \"Upgrading Windows\". This bit is going to take a while and as it states on the screen, your PC will restart several times so go make another cup of tea, even a bit of cake. This is going to take a little over an hour. You may notice, and be interested in, that at the bottom of the screen, your PC is telling you what its doing. I dunno why this is important for you to know (I guess in case of errors) but I thought it was kind of cool and dorky...</p> <p>Once it's done, we'll get a screen saying \"Hello there, welcome back!\". Click next.</p> <p>IMPORTANT, this next screen you want to go to the bottom left and click \"Customise settings\". DO NOT click \"Use Express Settings\".</p> <p>In the \"Customise settings\" section, TURN OFF EVERYTHING. If you require something in the future, you can turn it on if you wish but for security's sake, turn everything off. Click next. Again, TURN OFF EVERYTHING. Especially the stuff in \"Connectivity and error reporting\". Click next. This page tells you about the new apps for Windows 10. You can read it, if you want. Click next. It'll state that it'll finalise your settings, and it'll boot into Windows 10.</p> <p>Log in and Windows will proceed to set up your apps (This won't happen again, this is a one time thing).</p> <p>Congratulations! Enjoy Windows 10! Remember to read my Configuring and Cleaning Blog to make sure your computer is safe and secure from the security risks that Windows 10 causes!</p>"},{"location":"blog/windows-10/6-configuring-and-cleaning/","title":"Configuring and Cleaning (Technical)","text":"<p>Originally published: 28 May 2016</p> <p>Windows 10, like all versions of Windows, contains features that you may not want or need. However, rather than taking the approach of: \"I might need it one day, so I'll just leave it\", we're going to be taking my approach of: \"Turn everything off and if you need it, you can turn it back on...maybe...unless there is a better way\".</p> <p>This post basically takes my approach to features and tells you what to do to configure Windows 10 so that you can have a secure operating system. Let's get started.</p> <p>The first thing to do is to go: Start &gt; Settings which should open a window titled \"Settings\" showing you 9 buttons. We're going to go through these buttons one at a time.</p> <ol> <li> <p>System</p> <ol> <li> <p>Display    To maximise your resolution, enter the Advanced display settings and edit there.</p> </li> <li> <p>Notifications &amp; actions    Quick actions are useful so we'll leave them on. But you can edit them how you wish.    Turn OFF \"Show me tips about Windows\". It'll get annoying.    Turn ON \"Show app notifications\", that'll be useful for antivirus reports and what not.    Turn OFF \"Show notifications on the lock screen\", you don't want people walking by your unattended computer    seeing lots of messages and notifications that may be private to you.    Turn OFF \"Show alarms, reminders and incoming VOIP calls on the lock screen\" as you may have private reminders or    alarms, and it's not hard to unlock your computer to see your schedule and if you've got an incoming VOIP call,    you should be on your computer, otherwise your away. Understandable right?    Turn ON \"Hide notifications while presenting\", this will keep your notifications hidden when in a PowerPoint    presentation, you don't want to be in a meeting showing a cool presentation for a notification to come up    showing \"You need to take Molly swimming in an hour\". Looks unprofessional.    Then below that are your specific app notifications. I'd turn them all ON as I like to know what my apps are    doing/want from me.</p> </li> <li> <p>Apps &amp; features    This is kind of like the \"Programs and features\" menu in the old Windows 7 Control panel.    This is basically where you uninstall all your programs (or \"Apps\"). New features involve moving your apps to    other Windows drives, which is kind of neat.    These are the programs that come with Windows that I'd uninstall (alphabetically):    I'm guessing some other versions of Windows may have different apps, but I'm running Windows 10 Pro so it has    most of it.    *Sadly there is no current way to uninstall these, that I know of. When/If I find a way to get rid of them, I'll    make a blog post.</p> <ul> <li>3D Builder (Don't need and if you want a 3D design program; use Blender)</li> <li>Get Office (Useless piece of marketing crap, if you want an office suite; use Office Online, OpenOffice,   LibreOffice or just BUY Microsoft Office)</li> <li>Get Skype (Useless piece of marketing crap, if your want Skype; Google it.)</li> <li>Get Started (Junk that's taking up your hard drive. No one reads the instructions. If you're new to Windows   then use this application, read through it, then uninstall it)</li> <li>*Mail and Calendar (If you want a mail client, use Mozilla Thunderbird)</li> <li>Microsoft Solitaire Collection (Full of adverts and microtransactions, just play online or buy a pack of cards   IRL)</li> <li>Money (Junk taking up space on your hard drive)</li> <li>News (Use BBC News, it's much better)</li> <li>*OneNote (Requires you to sign-up/sign-in to take notes. Just use a piece of paper or make .txt files on your   Desktop)</li> <li>*People (You're computer isn't your Phone, most people you'll be contacting will be through Skype or social   media. Plus People is a minor security risk due to its integration with Wi-Fi Sense)</li> <li>Phone Companion (Use the File Explorer for memory management, you don't need this to charge your phone either   so it's just useless)</li> <li>Sport (More useless crap, just use BBC News or Google stuff)</li> <li>*Voice Recorder (If you want simple recording software; use Audacity)</li> <li>*Weather (Use BBC News or Google)</li> <li>*Xbox (Unless you actually HAVE an Xbox in which this MIGHT be useful, but probably not)</li> </ul> </li> <li> <p>Multitasking    Turn ON everything in the Snap section, it's a useful feature.    Virtual desktops is a cool thing, but you probably won't be using it so just make sure that the boxes are \"Only    the desktop I'm using\"</p> </li> <li> <p>Tablet Mode    Unless you're using a tablet, turn it OFF.    Under \"When I sign in\", choose \"Remember what I used last\".    Under \"When this device automatically switches tablet mode on or off\", choose \"Always ask me before switching\".    Turn ON \"Hide app icons on the taskbar in tablet mode\"</p> </li> <li> <p>Power &amp; sleep    Choose whatever power settings you want, I'd just leave them on the default settings</p> </li> <li> <p>Storage    This is only useful if you've got multiple drives, configure it how you want.</p> </li> <li> <p>Offline maps    Because I've already recommended Google Maps:    Click \"Delete all maps\", click \"Delete all\"    Turn OFF \"Metered Connections\".    Turn OFF \"Automatically update maps\"</p> </li> <li> <p>Default Apps    Configure it how you want.</p> </li> <li> <p>About     You don't need to do anything here.</p> </li> </ol> </li> <li> <p>Devices</p> <ol> <li> <p>Printers &amp; scanners    If you've got a printer you can set it up here later.    Turn OFF metered connections.</p> </li> <li> <p>Connected devices    If you've got more devices, add them here later.    Turn OFF metered connections.</p> </li> <li> <p>Mouse &amp; touchpad    \"Select your primary button\" should be \"Left\" unless required otherwise.    \"Roll the mouse wheel to scroll\" should be \"Multiple lines at a time\" unless required otherwise.    Unless you want a very fast scrolling mouse, just leave how many lines your scroll by each time at 3.    Turn ON \"Scroll inactive windows when I hover over them\".</p> </li> <li> <p>Typing    Turn OFF Autocorrect misspelt words.    Turn OFF Highlight misspelt words.    Programs/Apps have built in spell checkers. These settings work with a piece of software that logs everything you    type and sends it to Microsoft where they can read anything/everything you've typed. They claim to use this to    improve their spell-checking software however, it can be used maliciously.</p> </li> <li> <p>AutoPlay    Turn ON \"Use AutoPlay for all media and devices\".    Choose \"Ask me every time\" for Removable drive.    Choose \"Ask me every time\" for Memory card.</p> </li> </ol> </li> <li> <p>Network &amp; Internet</p> <ol> <li> <p>Wi-Fi (Laptops/Wi-Fi users only)    Click \"Manage Wi-Fi Settings\".    Turn OFF everything under \"Wi-Fi Sense\" &lt;-- IMPORTANT; This is a massive security risk.</p> </li> <li> <p>Data usage    Don't need to do anything here.</p> </li> <li> <p>VPN    Unless you're going to be using a VPN, don't need to do anything here.</p> </li> <li> <p>Dial-up    Shut down your computer, go to a library, go on a computer there, buy a broadband connection for your home. GET    OFF DIAL-UP. IT'S FUCKING SHIT.</p> </li> <li> <p>Ethernet    Don't need to do anything here.</p> </li> <li> <p>Proxy    Turn ON \"Automatically detect settings\"    TURN OFF everything else.</p> </li> </ol> </li> <li> <p>Personalisation</p> <ol> <li> <p>Background    Configure how you like.</p> </li> <li> <p>Colours    Configure how you like.</p> </li> <li> <p>Lock Screen    Configure how you like.</p> </li> <li> <p>Themes    Configure how you like.</p> </li> <li> <p>Start    Turn OFF \"Show most used apps\".    Turn OFF \"Show recently added apps\".    Turn OFF \"Use Start full screen\".    Turn OFF \"Show recently opened items in Jump Lists on Start or the task bar\".</p> </li> </ol> </li> <li> <p>Accounts</p> <ol> <li> <p>Your Account    Configure how you like.</p> </li> <li> <p>Sign-in options    Select \"When PC wakes up from sleep\" under \"Require sign-in\"    DO NOT use a PIN or Picture Password.</p> </li> <li> <p>Work access    Don't touch anything here unless you know what you're doing.</p> </li> <li> <p>Family &amp; other users    Configure how you like.</p> </li> <li> <p>Sync your settings    Personally, I'd turn OFF everything here and personalise stuff myself especially \"Passwords\" and \"Other Windows    settings\". But configure it how you like.</p> </li> </ol> </li> <li> <p>Time &amp; language</p> <ol> <li> <p>Date &amp; time    Turn ON \"Set time automatically\".    Choose your time zone.    Turn ON \"Adjust for daylight saving time automatically\"    Choose your Formats. I use hh:mm:ss and DD/MM/YYYY or YYYY/MM/DD as they make logical sense to me.</p> </li> <li> <p>Region &amp; language    Configure how you like.</p> </li> <li> <p>Speech    Configure how you like.</p> </li> </ol> </li> <li> <p>Ease of Access</p> <ol> <li> <p>Narrator    Turn OFF \"Narrator\" unless required otherwise.</p> </li> <li> <p>Magnifier    Turn OFF \"Magnifier\" unless required otherwise.</p> </li> <li> <p>High Contrast    Choose \"None\" unless required otherwise.</p> </li> <li> <p>Closed captioning    Leave everything on Default unless required otherwise.</p> </li> <li> <p>Keyboard    Turn OFF \"On-screen Keyboard\" unless required otherwise.    Turn OFF \"Sticky Keys\" unless required otherwise.    Turn OFF \"Toggle Keys\" unless required otherwise.    Turn OFF \"Filter Keys\" unless required otherwise.    Turn OFF everything under \"Other Settings\" unless required otherwise.</p> </li> <li> <p>Mouse    Configure how you like but:    Turn OFF \"Use numeric keypad to move mouse around the screen\". Buy a mouse.</p> </li> <li> <p>Other options    Turn ON \"Play animations in Windows\".    Turn ON \"Show Windows background\".    Configure the rest how you like.</p> </li> </ol> </li> <li> <p>Privacy</p> <ol> <li> <p>General    Turn OFF \"Let apps use my advertising ID for experiences across apps\".    Turn OFF \"Turn on SmartScreen Filter to check web content (URLs) that Windows Store apps use\".    Turn OFF \"Send Microsoft info about how I write to help improve typing and writing in the future\".    Turn ON \"Let websites provide locally relevant content by accessing my language list\".</p> </li> <li> <p>Location    Turn OFF \"Location\".    Make sure all apps listed are turned OFF.</p> </li> <li> <p>Camera    Turn OFF \"Let apps use my camera\".    If you want this on, then turn it ON however, select the SPECIFIC apps that you want to use your camera with.</p> </li> <li> <p>Microphone    Turn OFF \"Let apps use my microphone\".    If you want this on, then turn it ON however, select the SPECIFIC apps that you want to use your microphone with.</p> </li> <li> <p>Speech, inking &amp; typing    Ooh this is a tricky one.    In my opinion, you should have this OFF. Inking is basically Key logging and is a BIG security risk.    On the other hand, a LOT of people want to use Cortana.    If you want the increase in Security, make sure the grey box says: \"Get to know me\". This means you've turned it    OFF.    If you want Cortana and a security risk, make sure the grey box says: \"Stop getting to know me\". This means    you've turned it ON.</p> </li> <li> <p>Account info    Turn OFF \"Let apps access my name, picture and other account info\", unless otherwise specifically required.</p> </li> <li> <p>Contacts    Turn OFF EVERYTHING. Except MAYBE Mail and Calendar if you use Mail and Calendar.</p> </li> <li> <p>Calendar    Turn OFF \"Let apps access my calendar\", unless otherwise specifically required.</p> </li> <li> <p>Messaging    Turn OFF \"Let apps read or send messages\".</p> </li> <li> <p>Radios     Turn OFF \"Let apps control radios\".</p> </li> <li> <p>Other devices     Turn OFF \"Sync with devices\"</p> </li> <li> <p>Feedback &amp; diagnostics     Under \"Windows should ask for my feedback\", choose \"Never\".     Under \"Send your device data to Microsoft\", choose \"Basic\".</p> </li> <li> <p>Background apps     Turn OFF everything except \"Alarms &amp; Clock\" and \"Calendar\" if you use those apps.     Make sure that Voice Recorder is OFF. NEVER turn it on.</p> </li> </ol> </li> <li> <p>Update &amp; security</p> <ol> <li> <p>Windows Update    Don't need to do anything unless Windows has updates. In which case, do what you need to do to update Windows.</p> </li> <li> <p>Windows Defender    Turn ON \"Real-time protection\".    Turn OFF \"Cloud-based Protection\".    Turn OFF \"Sample submission\".    Configure your Exclusions how you want. (I'm not adding any Exclusions).</p> </li> <li> <p>Backup    Backups are really useful things. If you can afford to buy a large external hard drive, then backup your stuff    onto that using this section, otherwise just leave it.</p> </li> <li> <p>Recovery    Don't touch anything here.</p> </li> <li> <p>Activation    If you still haven't activated Windows, activate it here. Otherwise, leave it.</p> </li> <li> <p>For developers    Don't touch this unless you know what you're doing.</p> </li> </ol> </li> </ol> <p>That concludes basic configuration of Windows 10. For anything more advanced do: Start &gt; Settings &gt; Search for and click on \"Control Panel\" &gt; View by \"Small Icons\" or Start &gt; Settings &gt; Search for and click on \"View local services\"</p> <p>~Harm</p>"},{"location":"lists/films-seen/","title":"Films seen","text":"<p>In order of most to least favourite:</p>"},{"location":"lists/tv-shows-seen/","title":"TV shows seen","text":"<p>In order of most to least favourite:</p>"}]}